<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Buffer-Cache-区别.md]]></title>
    <url>%2F2018%2F03%2F15%2FBuffer-Cache-%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[buffer 是磁盘缓存, 缓存的是连续的blockcache 文件缓存, 缓存的是组成文件的非连续的block, 示例:某个磁盘中的 block 编号 1 - 10, buffer 缓存: 依据磁盘的局部性原理, 其缓存的是 从 编号5-10 的block; cache 缓存: 某个文件存放该磁盘上, 并且占据所有编号为奇数的block.]]></content>
      <categories>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>buffer</tag>
        <tag>cache</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis 汇总]]></title>
    <url>%2F2018%2F03%2F15%2Fredis-%E6%B1%87%E6%80%BB%2F</url>
    <content type="text"><![CDATA[1. Redis 介绍1.1 使用场景Redis是远程字典服务器 的缩写, 适用于如下场景: 数据库 缓存 队列系统 1.2 Redis所支持的数据类型 字符串类型 当存储的字符串类型是整数形式时, 可以对其进行递增操作, 比如：set age 1, incr age, 返回递增后的值, 递增操作是原子操作. 散列类型 列表类型 集合类型 有序集合类型 2. 安装配置与优化2.1 源码编译安装# 安装依赖, 编译工具 $ yum groupinstall &quot;Development Tools&quot; -y # 编译安装 $ wget http://usl/redis-stable.tgz $ tar xf redis-stable.tgz $ cd redis-stable $ make PREFIX=/usr/loca/redis $ make install PREFIX=/usr/local/redis # 配置启动 $ mkdir /usr/local/redis/{logs,data,conf} $ cp *.conf /usr/local/redis/conf/ $ sed -i &apos;s/^daemonize no/daemonize yes/g&apos; /usr/local/redis/conf/redis.conf $ cat &gt;/etc/profile.d/redis.sh &lt;&lt;EOF #!/bin/bash # export PATH=$PATH:/usr/local/redis/bin EOF $ source /etc/profile.d/redis.sh # 启动测试 $ redis-server /usr/local/redis/conf/redis.conf $ redis-cli -p REDIS_PORT &gt; info make install命令执行完成后, 会在/usr/local/redis 目录下生成 6 个可执行文件, 分别是redis-server、redis-cli、redis-benchmark、redis-check-aof 、redis-check-dump, 它们的作用如下： redis-server : Redis服务器的daemon启动程序 redis-cli ：Redis命令行操作工具. 也可以用telnet根据其纯文本协议来操作 redis-benchmark : Redis性能测试工具, 测试Redis在当前系统下的读写性能 redis-check-aof : 数据修复,AOF文件修复工具 redis-check-dump : 检查导出工具,RDB文件检查工具 redis-sentinel : sentinel 服务器 ,哨兵 2.2 redis 配置参数2.2.1 redis.conf 配置文件参数daemonize ：是否以后台daemon方式运行 pidfile ：pid文件位置 port ：监听的端口号 timeout ：请求超时时间 loglevel ：log信息级别 logfile ：log文件位置 databases ：开启数据库的数量 save * * ：保存快照的频率, 第一个*表示多长时间, 第二个*表示执行多少次写操作. 在一定时间内执行一定数量的写操作时, 自动保存快照. 可设置多个条件. rdbcompression ：是否使用压缩 dbfilename ：数据快照文件名(只是文件名, 不包括目录) dir：数据快照的保存目录(这个是目录) -- 持久化存储保存位置. appendonly ：是否开启appendonlylog, 开启的话每次写操作会记一条log, 这会提高数据抗风险能力, 但影响效率. appendfsync ：appendonlylog如何同步到磁盘(三个选项, 分别是每次写都强制调用fsync、每秒启用一次fsync、不调用fsync等待系统自己同步) 2.2.2 命令行 config 配置还可以在 redis 运行中通过 CONFIG SET 命令在不重新启动redis的情况下,动态修改部分 redis 配置. redis&gt; CONFIG SET loglevel warning redis&gt; CONFIG GET loglevel 支持的配置修改项: save rdbcompression rdbchekcsum dbfilename masterauth slave-server-stale-data sleva-read-only maxmemory maxmemory-policy maxmemory-samples appendonly appendfsync auto-aof-rewrite-percentage auto-aof-rewrite-min-size lua-time-limit slowlog-log-slower-than slowlog-max-len hash-max-ziplist-entries hash-max-ziplist-value list-max-ziplist-entries list-max-ziplist-value set-max-intset-entries zset-max-ziplist-entries zset-max-ziplist-value 使用 config 命令配置 redis server 时, 应当同时手动修改配置文件, 保证 redis-server 的配置与配置文件中一致, 否则容易采坑. 2.2.3 系统优化参数修改系统配置文件, 执行命令 $ echo vm.overcommit_memory=1 &gt;&gt; /etc/sysctl.conf $ sysctl vm.overcommit_memory=1 或执行echo vm.overcommit_memory=1 &gt;&gt;/proc/sys/vm/overcommit_memory 使用数字含义： 0, 表示内核将检查是否有足够的可用内存供应用进程使用；如果有足够的可用内存, 内存申请允许；否则, 内存申请失败, 并把错误返回给应用进程. 1, 表示内核允许分配所有的物理内存, 而不管当前的内存状态如何. 2, 表示内核允许分配超过所有物理内存和交换空间总和的内存 2.3 启动与关闭2.3.1 启动 前台启动 # 以开发模式启动redis-server,并指定端口,默认为6379 $ redis-server [--port=6380] 后台启动 $ vim redis.conf daemonize yes $ redis-server /etc/redis.conf 当配置文件后跟参数时, 参数会覆盖配置文件中的该选项. 此种启动方式应当仅限于测试, 生产勿用. $ redis-server /etc/redis.conf --loglevel warning 检测是否启动成功 $ redis-cli info 2.3.2 关闭# redis 会先断开所有客户端连接,然后根据配置执行持久化,最后完成退出. $ redis-cli shutdown # redis 可以妥善处理 SIGTERM 信号, 效果与发送 shutdown 命令一样. $ kill redis_PID 3. redis 命令3.1 redis 执行命令方式方式一 : 将命令作为 redis-cli 的参数执行 $ redis-cli -h HOST -p PORT CMD 或 $ echo &quot;REDIS_CMD&quot; | redis-cli -h HOST -p PORT 方式二 : 交互式模式 $ redis-cli -h HOST -p PORT &gt; CMD 3.2 命令返回值 状态回复 : 直接显示状态信息 redis&gt; PING PONG 错误回复 : 当出现命令不存在或命令格式有错误等情况时 Redis 会返回错误回复(error reply). 错误回复以error开头, 并在后面跟上错误信息. 整数回复 : 整数回复(integer reply)以(integer)开头, 并在后面跟上整数数据 # 递增键值返回以整数形式返回递增后的键值. redis&gt; INCR foo # 返回数据库中键的数量 redis&gt; DBSIZE 字符串回复 : 当请求一个字符串类型键的键值或一个其他类型键中的某个元素时就会得到一个字符串回复. 字符串回复以双引号包裹 redis&gt; GET foo 多行字符回复 : 多行字符串回复中的每行字符串都以一个序号开头. # 返回符合指定规则的键名. redis&gt; KEYS * 4. redis 多数据库redis 是一个字典结构的存储服务器,实际上一个redis实例提供了多个用来存储数据的字典,可以加将每个字典理解成一个独立的数据库. 每个数据库对外都是以一个从0开始的递增数字命名,redis 默认支持16个数据库. 客户端与redis建立链接之后,自动连接 0号 数据库,不过可以随时使用 SELECT 命令更换数据库, 但这种数据库更像是一种命名空间,而不适宜存储不同应用程序的数据 ,可以启动多个redis实例,存储数据. 但是可以用 数据库0 存储应用A的线上数据, 数据库1 存储应用A的测试数据. redis&gt; SELECT 1 OK redis[1]&gt; GET foo 注意: 这种数据库更像是一种命名空间,而不适宜存储不同应用程序的数据. redis 不支持自定义数据库的名字,每个数据库都以编号命名. 开发者必须自己记录那些数据库存储了那些数据 redis 不支持为每个数据库设置不同的访问密码, redis 多个数据库之间并不是完全隔离的. 如 FLASHALL 会清空一个redis实例中所有数据库中的数据. 5. redis 数据类型:所有 redis 命令都是原子操作. Redis中每个键都属于一个明确的数据类型,如通过HSET建立的键是散列类型,通过SET建立的键是字符串类型. 使用一种数据类型的命令操作另一种数据类型的键会提示错误. 5.0 基础命令5.0.1. 获取符合规则的键名列表 : KEYS patternredis&gt; KEYS glob_pattern ? : 匹配一个字符 * : 匹配任意个字符,包括0个 [] : 匹配括号内的任一字符, &quot;-&quot; 表示范围,[0-9] \x : 转义 5.0.2. 判断一个键是否存在# 有返回1, 没有返回0 redis&gt; EXISTS key 5.0.3. 删除键# 可以删除多个keys, 返回删除的键的个数. redis&gt; DEL key [key ...] # 一次删除多个键 : 删除所有以 &quot;user:*&quot; 开头的键. $ redis-cli KEYS &apos;user:*&apos; | xargs redis-cli DEL 或 # 性能更好. $ redis-cli DEL `redis-cli KEYS &quot;user:*&quot;` 5.0.4. 获得键值的数据类型:# 返回 string(字符串),hash(散列),list(列表),set(集合),zset(有序集合) redis&gt; TYPE key # 向指定的列表类型键中增加一个元素,如果键不存在则创建它. &gt; LPUSH key value 5.0.5. 键命名规范:对象类型:对象ID:对象属性 如 : post:articleID:page.view -- 文章访问量 5.0.6. 过期时间 设置过期时间 返回值 : 1 表示甚至成功 ; 0 表示键不存在或设置失败. 对键多次使用 EXPIRE 会重新设置键的过期时间. # seconds 表示过期时间,必须为整数. &gt; EXPIRE key seconds # 设置过期时间,时间单位是毫秒 &gt; PEXPIRE key milliseconds PEXPIRE key 10000 == EXPIRE key 1 # 第二个参数表示键的过期时间.使用UNIX时间戳. 表示到 UNIX_TIME 时过期. &gt; EXPIREAT key UNIX_TIME # 同上,但单位为毫秒. &gt; PEXPIREAT key UNIX_TIME_MS 查看过期时间 返回值 : 剩余时间,单位秒 ; 当键不存在时,返回 -2 ; 没有为键设置过期时间 ,返回 -1 . # 查看一个键还有多久时间会被删除, &gt; TTL key # 以毫秒为单位返回键的剩余时间. &gt; PTTL key 取消过期时间设置 注意 : 使用 SET 或 GETSET 命令为键赋值也会同时清除键的过期时间. 其他只对键值进行操作的命令(如 INCR,LPUSH,HSET,ZREN) 均不会影响键的过期时间. # 取消键的过期时间设置,即将键恢复成永不过期. &gt; PERSIST key 实现缓存. 实际开发中会发现很难为缓存键设置合理的过期时间, 为此可以限制 Redis 能够使用的最大内存,并让Redis按照一定的规则淘汰不需要的缓存键,这种方式在只将Redis用作缓存系统时非常实用. $ vim redis.conf # 限制Redis最大可用内存大小,单位字节. maxmemory = 12345 # 在超出maxmemory大小时,指定策略删除不需要的键,直到Redis占用的内存小于指定内存. maxmemory-policy = volatile-lru maxmemory-policy 策略 : LRU (Least Recently Used),最近最少使用 volatile-lru : 使用LRU算法删除一个键(只对设置了过期时间的键) allkeys-lru : 使用LRU算法删除一个键 volatile-random : 随机删除一个键(只对设置了过期时间的键) allkeys-random : 随机删除一个键 volatile-ttl : 删除过期时间最近的一个键 noeviction : 不删除键,只返回错误. volatile : 易变的,不稳定的, 5.1 字符串类型 :最基本的数据类型, 能存储任何形式的字符串,包括二进制数据. 可以存储 用户邮箱/JSON化的对象/图片等. 字符串类型是其他4种数据类型的基础,其他数据类型和字符串的差别从某种角度来说只是组织字符串的形式不同. 如 列表类型是以列表的形式组织字符产,集合是以集合的形式组织字符串. 单个字符串类型键允许存储的数据的最大容量是512MB. 5.1.1 赋值与取值 :&gt; SET key value &gt; GET key # 为程序设置分布式锁时, 非常有用. &gt; set(key, value, ex=None, px=None, nx=False, xx=False) ex : 设置过期时间, 单位 秒 px : 设置过期时间, 单位 毫秒 nx : nx=True, 当 key 不存在时, 设置其值为 value. xx : xx=True, 当 key 存在时, 设置其值为 value 5.1.2 递增数字:当存储的字符串类型是整数形式时, 可以对其进行递增操作, # 让当前键值递增,并返回递增后的值 &gt; INCR key # 对 key 进行 100 次递增操作 $ redis-cli -r 100 incr key 用途 : 文章访问量统计 生成自增ID 存储文章数据 5.1.3 增加指定的整数 :# 指定当前 key 增加 increment 个数 &gt; INCRBY key increment # 指定 bar 增加 2 redis&gt; INCRBY bar 2 5.1.4 减少指定的整数 :# 让键值递减 &gt; DECR key # 指定减少个数 &gt; DECRBY key decrement 5.1.5 增加指定浮点数:# 指定增加一个双精度浮点数. &gt; INCRBYFLOAT key increment 5.1.6 向尾部追加值 :# 向键值的末尾追加value, 如果键不存在则将该键的值设置为 value, 返回值是追加后字符串的总长度. &gt; APPEND key value 5.1.7 获取字符串长度 :# 返回键值的长度,如果键不存在则返回0 . &gt; STRLEN key 5.1.8 同时获取/设置多个键值&gt; MGET key [key ...] &gt; MSET key value [key value ...] 5.1.9 位操作 :一个字节由8个二进制位组成, Redis提供了 4 个命令可以直接对二进制位进行操作. 位操作符可以非常紧凑的存储布尔值,并且 GETBIT 和 SETBIT 的时间复杂度是O(1)的,性能很高. GETBIT# 获取一个字符串类型键指定位置的二进制位的值. # 如果需要获取的二进制位的索引超出了键值的二进制位的长度,则默认位值是 0 . &gt; GETBIT key offset # bar 的3个字母,的ASCII码为 98,97,114 ,转换为二进制后分别为 : 1100010,1100001,1110010 存储结构为 011000100110000101110010 . redis&gt; SET foo bar OK &gt; GETBIT foo 0 (integer) 0 &gt; GETBIT foo 6 (integet) 1 SETBIT# 设置字符串类型键指定位置的二进制位的值, 返回值是该位置的旧值. # 如果设置的位值超过了键值的二进制位的长度,SETBIT命令会自动将中间的二进制位设置为0 # 如果设置一个不存在的键的指定二进制位的值会自动将其前面的位赋值为 0 . &gt; SETBIT key offset value BITCOUNT# 获得字符串类型键中值是 1 的二进制位个数. start 和 end 限制统计的字节范围. &gt; BITCOUNT key [start] [end] BITOP# 可以对多个字符串类型键进行位运算,并将结果存储在 destkey 参数指定的键中, BITOP的支持的运算操作有 AND,OR,XOR,NOT . &gt; BITOP operation destkey key [key ...] #示例 : 对 bar 和 aar 进行OR运算; &gt; SET foo1 bar &gt; SET foo2 aar &gt; BITOP OR res foo1 foo2 (integer)3 &gt; GET res &quot;car&quot; BITPOS# 可以获取指定键的第一位值是0 或 1 的位置. start end 指定起始字节 , 但返回的偏移量任然从开头算起. # redis_version &gt; 2.8.7 &gt; BITPOS key [0|1] [start] [end] &gt; SET foo bar &gt; BITPOS foo 1 (integer)1 BITFIELD# Perform arbitrary bitfield integer operations on strings. VER &gt; 3.2.0 &gt; BITFIELD key [GET type offset] [SET type offset value] [INCRBY type offset increment] [OVERFLOW WRAP|SAT|FAIL] 5.2 hash 散列类型 :1 个散列类型最多能容纳 2**32 - 1 个元素. hash 的键值也是一种字典结构,其存储了字段(field)和字段值得映射,但字段值只能是字符串,不支持其他类型.散列值类型不能嵌套其他的数据类型. ( Redis 的其他数据类型同样不支持数据类型嵌套. ) 适合存储对象 : 使用对象类别和ID构成键名,使用字段表示对象的属性,而字段值则存储属性值. 对象类别:ID:字段属性 – 字段值 例如 : 存储ID为2的汽车对象, CAR:2:color -- 白色 CAR:2:name -- 奥迪 CAr:2:price -- 900k 5.2.1 赋值与取值 : HSET, HMSET, HGET, HMGET, HGETALL# 设置 key 中指定 filed 的值 &gt; HSET key field value HSET 不区分插入和更新操作. 即修改数据时不用事先判断字段是否存在来决定是插入还是更新操作. 1. 当执行的是 插入 操作时,HSET返回 1 ; 2. 当执行的是 更新 操作时,HSET返回 0 ; 3. 当键本身 不存在 时,HSET会自动创建它. # 获取 key 中指定 field 的值. &gt; HGET key field # 设置一个 key 中的多个 field &gt; HMSET key field value [field value ...] &gt; HMSET car:3 name baoma color black price 900k # 获取一个 key 中的多个 field &gt; HMGET key field [field ...] &gt; HMGET car:3 name color price # 获取键中所有字段和字段值. &gt; HGETALL key &gt; HGETALL car:3 5.2.2 判断字段是否存在# 存在返回 1 ,不存在返回 0 ; &gt; HEXISTS key field &gt; HEXISTS car:3 name 5.2.3 当字段不存在时赋值 :# 如果字段已存在,则不执行任何操作. 且HSETNX是原子操作,不必担心竞态条件. &gt; HSETNX key field value 5.2.4 增加数字 :# 使字段增加指定的整数 . # 如果键不存在,则 HINCRBY 命令会自动建立该键,并默认新建字段的值为&apos;0&apos; . # 命令返回值为增值后的字段值. &gt; HINCRBY key field increment 5.2.5 删除字段 :# 可以删除一个或多个字段,返回值为被删除的字段的个数. &gt; HDEL key field [field ...] 5.2.6 获取所有 字段名(field) 或 字段值 :# 获取字段名字 &gt; HKEYS key # 获取字段值. &gt; HVALS key 5.2.7 获得字段数量&gt; HLEN key 5.3 列表类型 : – 队列 , 1 个列表类型最多能容纳 2**32 - 1 个元素.存储一个有序的字符串列表, 常用操作时向列表两端添加元素,或者后的列表的某一个片段. 列表类型内部是使用双向链表(double linked list)实现的 ,所以向列表两端增加元素的时间复杂度是 O(1) ,获取越接近两端的元素速度越快. 但代价是通过索引访问元素比较慢. 使用场景: 记录新鲜事儿, 最新记录 top 10 记录日志等 5.3.1 向列表两端增加元素.# 向列表左端增加元素,返回增加元素后列表的长度, &gt; LPUSH key value [value ...] # 向列表右端增加元素,返回增加元素后列表的长度. &gt; RPUSH key value [value ...] 5.3.2 从列表两端弹出数据# 从列表左边弹出一个元素,返回被移除的元素值. &gt; LPOP key # 从列表右边弹出一个元素,返回被移除的元素值. &gt; RPOP key ** 模拟 栈 : 后进先出 LPUSH -- LPOP ROUSH -- RPOP ** 模拟 队列 : 先进先出 LPUSH -- RPOP RPUSH -- LPOP 5.3.3 获取列表中元素的个数# LLEN时间复杂度为 O(1) ,使用redis会直接读取现成的值,而不需要遍历一遍数据表来完成统计. &gt; LLEN key 5.3.4 获得列表片段# 起始索引值为 0 .并且 LRANGE 返回的值包含最右边的元素. 支持负索引. &gt; LRANGE key start stop 1. 如果 strat &gt; stop ,则返回空列表 2. 如果 start &gt; LLEN(key) ,返回列表最右边的元素. 5.3.5 删除列表中指定的值# 删除列表中前 count 个值为 value 的元素, 返回值为实际删除的元素个数. &gt; LREM key count value 1. 当 count &gt; 0 ,LREM 从列表 左边 开始删除前 count 个值为 value 的元素 2. 当 count &lt; 0 ,LREM 从列表 右边 开始删除前 |count| 个值为 value 的元素. 3. 当 count = 0 ,LREM 删除 所有值 为 value 的元素. 5.3.6 获取/设置指定索引的元素值# 返回指定索引的元素. 索引从 0 开始. # 当index 为负数时,从右边开始计算索引,最右边的元素索引为 -1 . &gt; LINDEX key index # 将索引为 index 的元素, 赋值为 value. &gt; LSET key index value 5.3.7 只保留列表指定片段# 删除指定范围之外的所有元素. &gt; LTRIM key start end 5.3.8 向列表中插入元素# LINSERT首先会在列表中从左向右查找职位 pivot 的元素,然后 BEFORE|AFTER 插入 value . # LINSERT 返回值为插入后列表的元素个数. &gt; LINSERT key [ BEFORE | AFTER ] pivot value 5.3.9 将元素弓一个列表转到另一个列表.把列表类型作为队列使用时,RPOPLPUSH命令可以很直观的在多个队列中传递数据. 当 source 和 destination 相同时, RPOPLPUSH 会不断将对尾的元素转移到队首,借助这个特性,可以实现一个网站监控系统: 使用一个队列存储需要监控的网址,然后监控程序不断的使用 RPOPLPUSH 命令循环去除一个网址来检测可用性. 另外,在检测过程中,可以不断的向队列中增加新的元素, 而且整个系统容易扩展,允许多个客户端同时处理队列. # 从 source RPOP 一个元素, LPUSH 到 destination ,并返回改变的元素值. # 整个操作过程是原子的. &gt; RPOPLPUSH source destination 5.4 集合类型集合中的每个元素都是不同的, 且没有顺序 . 一个集合类型最多可以存储 2**32-1 个字符串. 集合类型在Redis内部是通过使用值为空的 散列表(hash table)实现的, 故操作的时间复杂度是O(1)的. 多个集合类型键之间可以进行并集/交集/差集运算. 5.4.1 增加/删除元素# 向集合中增加一个或多个元素,如果键不存在则会自动创建. 如果元素已经存在,则会忽略. # 命令返回值为加入的元素的个数. &gt; SADD key member [member ...] # 从集合中删除一个或多个元素,并返回删除成功的元素的个数. &gt; SREM key member [member ...] 5.4.2 获取集合中所有的元素# 返回集合中的所有元素. &gt; SMEMBERS key 5.4.3 判断元素是否在集合中# 判断一个元素是否在集合中是一个事件复杂度为O(1)的操作,无论集合中有多少元素. # 当值存在是返回 1 ,当值不存在是返回 0 . &gt; SISMEMBER key member 5.4.4 集合间运算集合运算 # 差集运算. # 集合A 与 集合B 的差集表示 A - B, 代表所有属于A 且 不属于B的元素构成的集合. &gt; SDIFF key [key ...] # 先计算 setA - setB ,再计算结果与 setC 的差集. &gt; SDIFF setA setB setC # 交集运算. # 集合A 和 集合B 的交集表示 A ∩ B,代表所有属于A 且 属于B的元素构成的集合. &gt; SINTER key [key ...] # 并集计算. # 集合A 和 集合B 的并集表示 A ∪ B, 代表所有属于A 或 属于B的元素构成的集合. &gt; SUNION key [key ...] 进行集合运算,并将结果存储. 常用于需要多次运算的场合. # SDIFF操作之后,将结果存储在 destination 中 &gt; SDIFFSTORE destination key [key ...] # SINTER操作之后,将结果存储在 destination 中 &gt; SINTERSTORE destination key [key ...] # SUNION操作之后,将结果存储在 destination 中 &gt; SUNIONSTORE destination key [key ...] 5.4.5 获取元素的个数&gt; SCARD key 5.4.6 随机获取集合中的元素# 用来随机集合中获取 count 个元素. &gt; SRANDMEMBER key [count] 当 count &gt; 0 , SRANDMEMBER 会随机从集合里获得count个不重复的元素. 当 count 的值大于集合中全部的元素的个数时,返回集合中的全部元素. 当 count &lt; 0 , SRANDMEMBER 随机从集合中获取 |count| 个元素.这些元素有可能相同. 5.4.7 从集合中弹出一个元素# 从集合中随机弹出一个元素. &gt; SPOP key 5.5 有序集合类型 : sorted set有序集合是在集合的基础上,为集合中的每个元素都关联了一个分数,这使得我们不仅可以完成插入/删除和判断元素是否存在等集合类型操作 ,还可以获得分数最高(或最低)的前N个元素、获得指定分数范围内的元素等与分数有关的操作. 虽然集合中每个元素都是不同的, 但是它们的分数却可以相同. 有序集合使用散列表和跳跃表实现,所以即使读取列表中间部分的数据也很快(时间发咋读O(log(N)))有序集合可以调整集合中某个元素的位置(通过修改这个元素的分数).有序集合比列表类型更耗费内存. 有序集合算得上是 Redis的5种数据类型中最高级的类型了,可以与列表类型和集合类型对照理解. 5.5.1 增加元素# 向有序集合中添加一个元素和该元素的分数,如果该元素已经存在,则会用新的分数替换原有的分数. # 命令返回值 : 新加入到集合中的元素的个数. # 分数可以为双精度浮点数. &gt; ZADD key score member [score member] # +inf 正无穷 &gt; ZADD testboard +inf haha # -inf 负无穷 &gt; ZADD testboard -inf hehe 5.5.2 获得元素的分数&gt; ZSCORE key member 5.5.3 获得排名在某个范围的元素列表# 按元素分数从小到大顺序返回 索引 从start 到 stop 之间的所有元素(包含两端的元素). # 索引从 0 开始, 负数代表从后向前查找. # [WITHSCORES] 表示在获得 元素 值的同时,展示元素的分数. &gt; ZRANGE key start stop [WITHSCORES] 时间复杂度 : O(log n+m) , n 为有序集合的基数, m为返回的元素的个数. 当两个元素分数相同时,Redis按照字典书序来进行排序. &gt; ZREVRANGE key start stop [WITHSCORES] # 元素从大到小排序. 5.5.4 获得指定分数范围的元素按照元素分数从小到大的顺序返回分数在min 和 max之间(包含min,max)的元素. 如果希望分数范围不包含端点值,可以在分数前加上 “(“ 符号,可以单独加. min ,max 支持 +inf 和 -inf . &gt; ZRANGEBYSCORE key min max [WITHSCORES] [LIMIT offset count] LIMIT offset count : 在获得的元素列表的基础上,向后偏移 offset 个元素,并只获取钱 count 个元素. &gt; ZREVRANGEBYSCORE key max min [WITHSCORES] [LIMIT offset count] # 元素从大到小. 5.5.5 增加某个元素的分数# 增加一个元素的分数,返回值是更改后的分数. increment 支持负数. &gt; ZINCRBY key increment member 5.5.6 获得集合中元素的数量&gt; ZCARD key 5.5.7 获得指定分数范围内的元素的个数&gt; ZCOUNT key min max 5.5.8 删除一个或多个元素&gt; ZREM key member [member ...] 5.5.9 按照排名范围删除元素.# 按照元素分数从小到大的顺序(即索引0表示最小的值)删除处在指定排名范围内的所有元素, 并返回删除的元素数量. &gt; ZREMRANGEBYRANK key start stop 5.5.10 按照分数范围删除元素# 删除指定分数范围内的所有元素,min max 的特定和 ZRANGEBUSCORE 命令中的一样. 返回值是删除的元素数量. &gt; ZREMRANGEBYSCORE key min max 5.5.11 获得元素的排名# 安装元素分数从小到大的顺序获得指定的元素的排名(从0开始,即分数最小的元素排名为0) &gt; ZRANK key member # 分数最大的元素排名为0 &gt; ZREVRANK key member 5.5.12 计算有序集合的交集计算多个有序集合的交集,并将结果存储在 destination 键中(同样以有序集合类型存储), 返回值为 destination 键中元素的个数. &gt; ZINTERSTORE destination numkeys key [key...] [WEIGHTS weight [weight ...]] [AGGREGATE SUM|MIN|MAX ] destination 键中元素的分数是由 AGGREGATE 参数决定的. - 当 AGGREGATE 是 SUM 时(默认值), destination键中元素的分数是每个参与计算的集合中该元素分数的和. - 当 AGGREGATE 是 MIN 时, destination键中元素的分数是每个参与计算的集合中该元素分数的最小值. - 当AGGREGATE是MAX时, destination键中元素的分数是每个参与计算的集合中该元素分数的最大值. ZINTERSTORE命令还能够通过WEIGHTS参数设置每个集合的权重, 每个集合在参与计算时元素的分数会被乘上该集合的权重 6. redis 事务 transction数据库原理中很重要的一个概念是事务, 简单来说就是把一系列动作看做一个整体, 如果其中一个出了问题, 应该把状态恢复到执行该整体之前的状态. 在 Redis 中, MULTI、EXEC、DISCARD、WATCH 这四个指令是事务处理的基础. - MULTI : 用来 组装 一个事务； - EXEC : 用来 执行 一个事务； - DISCARD : 用来 取消 一个事务； - WATCH : 用来 监视 一些key, 一旦这些key在事务执行之前被改变, 则取消事务的执行. 用 MULTI 组装事务时, 每一个命令都会进入到内存队列中缓存起来, 如果出现 QUEUED 则表示我们这个命令成功插入了缓存队列, 在将来执行 EXEC 时, 这些被 QUEUED 的命令都会被组装成一个事务来执行. 对于事务的执行来说, 如果 redis 开启了 AOF 持久化的话, 那么一旦事务被成功执行, 事务中的命令就会通过 write 命令一次性写到磁盘中去, 如果在向磁盘中写的过程中恰好出现断电、硬件故障等问题, 那么就可能出现只有部分命令进行了 AOF 持久化, 这时 AOF 文件就会出现不完整的情况, 这时, 我们可以使用 redis-check-aof 工具来修复这一问题, 这个工具会将 AOF 文件中不完整的信息移除, 确保 AOF 文件完整可用. 然后我们来说说 WATCH 这个指令, 它可以帮我们实现类似于“乐观锁”的效果, 即CAS(check and set). WATCH本身的作用是“监视key是否被改动过”, 而且支持同时监视多个key, 只要还没真正触发事务, WATCH都会尽职尽责的监视, 一旦发现某个key被修改了, 在执行EXEC时就会返回nil, 表示事务无法触发. 6.1 MULTI &amp; EXEC事务同命令一样是Redis的最小执行单位, 一个事务要么都执行,要么都不执行.事务的原理是先将属于一个事务的命令发送给Redis,然后再让Redis一次执行这些命令. &gt; MULTI &gt; CMD_1 # 返回值,QUEUED &gt; CMD_2 # 返回值,QUEUED &gt; EXEC # 返回这些命令的返回值组成的列表. 如果在发送 EXEC 命令前,客户端断开链接,则Redis会清空事务队列. Redis的事务还能保证一个事务内的命令一次执行而不被其他命令插入. 错误处理 : 语法错误 : 只要有一个命令有语法错误,执行EXEC命令之后Redis就会直接返回错误,连语法争取的命令也不会执行. redis 2.6.5 之前的版本会忽略有语法错误的命令,而执行事务中其他语法正确的命令. 运行错误 : 指在命令执行时出现的错误,比如使用散列类型的命令操作集合类型的键,这种错误在实际执行之前是无法发现的,所以事务里的这样的命令是会被执行的. 如果事务里的一条命令出现了运行错误,事务里的其他命令依然会继续执行(包括出错之后的命令). Redis的事务没有回滚功能,为此开发者必须自己将数据库复原回事务执行前的状态. Redis不支持回滚功能,也使得Redis 在事务上可以保值简洁和快速. 6.2 WATCH &amp; UNWATCHWatch 命令 : 用于监控一个或多个键, 一旦其中有一个键被修改(或删除), 之后的事务就不会执行. 监控将一直持续到 EXEC命令结束. 事务中的命令是在EXEC之后才执行的,所以在MULTI命令之后可以修改WATCH监控的键值. 示例 : redis&gt; SET key 1 redis&gt; WATCH key redis&gt; SET key 2 redis&gt; MULTI redis&gt; SET key 3 redis&gt; EXEC (nil) redis&gt; GET key &quot;2&quot; 该例子中,在执行WATCH命令后, 事务执行只玩修改了 key 的值(SET key 2),所以最后事务中的命令 SET key 3 没有执行,EXEC返回空结果. WATCH 命令的作用只是当被监控的键值被修改后阻止之后的一个事务执行,而不能保证其他客户端不修改这一键值,所以我们需要在EXEC执行失败后重新执行整个函数. 执行EXEC命令后会取消对所有键的监控,如果不想执行事务中的命令也可以使用 UNWATCH 命令来取消监控. 7. 排序 有序集合的集合操作 有序集合的常见的使用场景是大数据排序. ZINTERSTORE ZUNIONSTORE MULTI ZINTERSTORE tempKey ZRANGE tempKey DEL tempKey EXEC SORT命令 : 可以对列表类型,集合类型和有序集合类型键进行排序, 并且可以完成与关系数据库中的联结查询类似的任务. BY 参数 GTE 参数 STORE 参数 性能优化 : SORT 是Redis中最强大最复杂的命令之一,如果使用不好很容易成为性能瓶颈. SORT 的时间复杂度是 O(n+mlog(m)) , n 为排序的列表(或集合或有序集合)中元素的个数. 注意: 尽可能减少待排序键中元素的数量,使 n 尽可能小. 使用 LIMIT 参数,只获取需要的数据, 是 m 尽可能小. 如果要排序的数据数量很大,尽可能使用 STORE 参数将结果缓存. 8. 消息通知8.1 任务队列 : 传递任务的队列与任务队列进行交互的实体有两类 : 生产者 : producer, 生产者会将需要处理的任务放在任务队列中, 消费者 : comsumer, 消费这不断地从任务队列中读入任务信息并执行. 任务队列的优点 : 松耦合 : 生产者和消费者无需知道彼此的实现细节,只需要约定好任务的描述格式. 易于扩展 : 消费者可以有多个,而且可以分布在不同的服务器中,借此可以降低单台服务器的负载. 使用 Redis 实现任务队列 生产者 : LPUSH 消费者 : RPOP BRPOP : 当列表中没有元素时, BRPOP 命令会一直阻塞住链接, 直到有新元素加入. BRPOP 接受连个参数,第一个是键名, 第二个是超时时间; 当超过了超时时间任然没有获得新元素的话,就返回 nil. 0 表示不显示等待时间,即 如果没有新元素加入,就会一直阻塞下去. 当获得一个元素后, BRPOP 命令返回两个值, 第一个是键名,第二个是超时时间(单位s). BRPOP key [key ...] timeout BLPOOP : 从队列左边获取元素. 8.2 优先级队列8.3 ‘发布/订阅’ 模式 : pulish/subscribe ,可以实现进程间的消息传递.8.3.1 基础订阅模式 : 发布者 : 可以向指定的频道发布消息, 所有订阅此频道的订阅者都会收到此消息. 订阅者 : 可以订阅一个或若干个频道(channel). 发布者不会 : 发出的消息不会**持久化. &gt; PUBLISH channel message # 返回值表示接受到这条消息的订阅者数量. 订阅者 : 处于订阅状态下的客户端, 不能使用 SUBSCRIBE,UNSUBSCRIBE,PUNSUBSCRIBE 这四个属于’发布/订阅’模式的命令之外的命令, 否则会报错. &gt; SUBSCRIBE channel [channel ...] 进入订阅状态后客户端可能收到 3 种类型的回复,每种类型的回复都包含 3 个值, 第一个值是消息的类型, 依据消息了类型的不同,第二,第三个值的含义也不同. 消息类型 : subscribe : 表示订阅成功的返回消息, 第二个值是订阅成功的频道名称,第三个值是当前客户端订阅的频道数量. message : 表示收到的消息. 第二个值是产生消息的频道名称,第三个消息是消息的内容. unsubscribe : 表示成功取消订阅某个频道. 第二个值是对应的频道名称,第三个值是当前客户端订阅的频道数量, 当此值为 0 时,客户端会推出订阅状态. UNSUBSCRIBE : 取消订阅指定的频道. 没有指定频道的情况下,将退订所有频道. &gt; UNSUBSCRIBE [channel [channel ...]] 8.3.2 按照规则订阅.PSUBSCRIBE : 订阅指定的规则, 规则支持 glob 风格的通配符. 使用 PSUBSCRIBE 可以重复订阅一个频道. PSUBSCRIBE channel.? channel.?* 则 当 channel.2 发布消息后,该客户端会收到两条消息. &gt; PSUBSCRIBE channel.?* 1) &quot;pmessage&quot; # 表示通过 PSUBSCRIBE 命令订阅 2) &quot;channel.?*&quot; # 订阅时使用的通配符 3) &quot;channel.1&quot; # 实际收到消息的频道的名称 4) &quot;hi!&quot; # 消息的内容. PUNSBUSCRIBE [pattern [pattern ...]] : 退订指定的规则. 如果没有指定会退订所有规则. 使用 PUNSUBSCRIBE 命令只能退订通过 PSUBSCRIBE命令订阅的规则, 不会影响直接通过 SUBSCRIBE 命令订阅的频道； 同样 UNSUBSCRIBE 命令也不会影响通过PSUBSCRIBE命令订阅的规则. 使用 PUNSUBSCRIBE 命令退订某个规则时不会将其中的通配符展开, 而是进行严格的字符串匹配, 所以PUNSUBSCRIBE * 无法退订 channel.* 规则, 而是必须使用 PUNSUBSCRIBE channel.* 才能退订. 9. 管道往返时延 : 网络传输中,往返消息的总耗时. 在执行多条命令时,每条命令都需要等待上一条命令执行完(即受到Redis返回的结果)才能执行, 即使命令并不需要上一条命令的执行结果. Redis 的底层通信协议对管道(pipelining)提供了支持. 通过管道可以一次性发送多条命令并在执行完后一次性将结果返回, 当一组命令中每条命令都不依赖于之前命令的执行结果时就可以将这组命令一起通过管道发出. 管道通过减少客户端与 Redis 的通信次数来实现降低往返时延累计值的目的 Redis 通过监听一个 TCP 端口或者 Unix socket 的方式来接收来自客户端的连接, 当一个连接建立后, Redis 内部会进行以下一些操作： 客户端 socket 会被设置为非阻塞模式, 因为 Redis 在网络事件处理上采用的是非阻塞多路复用模型. 为这个 socket 设置 TCP_NODELAY 属性, 禁用 Nagle 算法. Nagle 算法实际就是当需要发送的数据攒到一定程度时才真正进行发包, 通过这种方式来减少 header 数据占比的问题. 不过在高互动的环境下是不必要的, 一般来说, 在客户端/服务器模型中会禁用. 创建一个可读的文件事件用于监听这个客户端 socket 的数据发送 Redis 管道技术可以在服务端未响应时, 客户端可以继续向服务端发送请求, 并最终一次性读取所有服务端的响应. 管道技术最显著的优势是提高了 redis 服务的性能. 10. 内存优化10.1. redisObject 对象redis 存储的所有值对象, 在内部定义为 redisObeject 结构体, 内部结构如下图: Redis 存储的数据都使用 redisObject 来封装, 包括 string, hash, list, set, zset 在内的所有数据类型. 理解 redisObject 对内存优化非常有帮助, 下面介绍每个字段 1. type 字段 表示当前对象使用的数据类型, redis 主要支持 5 种数据类型 : string,hash,list,set,zset . 可以使用 type {key} 命令查看对象所属类型, type 返回的是值对象类型, 键都是 string 类型. 2. encoding 字段 表示 redis 内部编码类型, encoding 在 redis 内存不使用, 代表当前对象内部使用哪种数据结构实现. 同一个对象采用不同的编码实现内存占用存在明显差异, 具体实现细节见 之后的编码部分. 3. lru 字段 记录对象最后一次被访问的时间, 当配置了 maxmemory 和 maxmemory-policy=volatile-lru | allkeys-lru 时, 用于辅助 LRU 算法删除键数据. 使用 objectidletime {key} 命令在不更新 lru 字段情况下查看当前键的空闲时间. **开发提示：可以使用scan + object idletime 命令批量查询哪些键长时间未被访问, 找出长时间不访问的键进行清理降低内存占用** 4. refcount 字段 记录当前对象被引用的次数, 用于通过引用次数回收内存, 当 refcount=0 时, 可以安全回收当前对象空间. 使用 objectrefcount {key} 获取当前对象引用. 当对象为整数且范围在 0-9999 时, redis 可以使用共享对象的方式来节省内存. 5. *ptr 字段 与对象的数据内容有关, 如果是整数直接存储数据, 否则表示指向数据的指针. Redis 在 3.0 之后对值对象是字符串且长度 &lt;= 39 字节的数据, 内部编码为 embstr 类型, 字符串 sds 和 redisObject 一起分配, 从而只要一次内存操作. **高并发写入场景中, 在条件允许的情况下建议字符串长度控制在39字节以内, 减少创建redisObject内存分配次数从而提高性能. ** 10.2. 缩减键值对象降低 Redis 内存使用最直接的方式就是缩减键和值的长度. - key 长度 : 如在设计键时, 在完整描述业务情况下, 键值越短越好. - value 长度 : 值对象缩减比较复杂, 常见需求是把业务对象序列化成二进制数组放入 redis. - 首先, 应该在业务上精简业务对象, 去掉不必要的属性避免存储无效数据. - 其次, 在序列化工具选择上, 应该选择更高效的序列化工具来降低字节数组大小. ![常见 java 序列化工具空间压缩对比](http://i2.itc.cn/20170216/3084_8e427aa5_bf49_c714_45f8_692fccf2cd6b_1.png) - 值对象除了存储二进制数据之外, 通常还会使用通用格式存储数据 比如 JSON, xml 等作为字符串存储在 redis 中. 这种方式的有点是方便调试和跨语言, 但是同样的数据相比字节数组所需的空间更大, 在内存紧张的情况下, 可使用通用压缩算法压缩 json,xml 后再存入 redis, 从而降低内存使用率, 例如 使用 GZIP 压缩后的 json 可以降低 60% 的空间. **当频繁压缩解压json等文本数据时, 开发人员需要考虑压缩速度和计算开销成本, 这里推荐使用google的Snappy压缩工具, 在特定的压缩率情况下效率远远高于GZIP等传统压缩工具, 且支持所有主流语言环境. ** 10.3. 共享对象池对象共享池指 redis 内存维护 0-9999 的整数对象池, 创建大量的整数类型 redisObject 存在内存开销, 每个 redisObject 内部结构至少占 16字节, 甚至超过了证书自身空间开销. 除了整数值对象, 其他类型如 list,hash,set,zset 内部元素也可以使用整数对象池, 因此开发中, 在满足需求的前提下, 尽量使用整数对象以节省内存. 整数对象池在Redis中通过变量 REDIS_SHARED_INTEGERS 定义, 不能通过配置修改. 可以通过object refcount 命令查看对象引用数验证是否启用整数对象池技术, redis&gt; set foo 100 redis&gt; object refcount foo (integer) 2 redis&gt; set bar 100 redis&gt; object refcount bar (integer) 3 ![共享对象池图示](http://i0.itc.cn/20170216/3084_ffd40aaf_b79c_0c62_9751_650a6b0ccc53_1.png) **当设置maxmemory并启用LRU相关淘汰策略如:volatile-lru, allkeys-lru时, Redis禁止使用共享对象池, 即 共享对象池与maxmemory+LRU策略冲突 ** LRU算法需要获取对象最后被访问时间, 以便淘汰最长未访问数据, 每个对象最后访问时间存储在redisObject对象的lru字段. 对象共享意味着多个引用共享同一个redisObject, 这时lru字段也会被共享, 导致无法获取每个对象的最后访问时间. 如果没有设置maxmemory, 直到内存被用尽Redis也不会触发内存回收, 所以共享对象池可以正常工作. 10.4. 字符串优化0. 在 redis 内部, 所有的键都是字符串类型, 值对象数据除了整数之外都使用字符串存储. 1. 字符串结构 Redis 自己实现了字符串结构, 内部简单动态字符串(Simple dynamic string), SDS. ![SDS 结构图](http://i2.itc.cn/20170216/3084_54bb9bb0_a89d_8e7e_2edb_11fcfc6ee5f1_1.png) SDS 特点 - O(1) 的时间复杂度获取 : 字符串长度,已用长度, 未用长度 - 可用于保存字节数组, 支持安全的二进制数据存储 - 内部实现空间预分配机制, 降低内存再分配次数. - 惰性删除机制, 字符串缩减后的空间不释放, 作为预分配空间保留. 2. 预分配机制. 因为 字符串SDS 存在预分配jizhi,日常开发中要小心预分配带来的内存浪费. 空间预分配规则 : - 第一次创建len属性等于数据实际大小, free等于0, 不做预分配. - 修改后如果已有free空间不够且数据小于1M, 每次预分配一倍容量. 如原有len=60byte, free=0, 再追加60byte, 预分配120byte, 总占用空间:60byte+60byte+120byte+1byte. - 修改后如果已有free空间不够且数据大于1MB, 每次预分配1MB数据. 如原有len=30MB, free=0, 当再追加100byte ,预分配1MB, 总占用空间:1MB+100byte+1MB+1byte. ** 尽量减少字符串频繁修改操作如append, setrange, 改为直接使用set修改字符串, 降低预分配带来的内存浪费和内存碎片化 ** 3. 字符串重构 : 不一定吧每份数据作为字符串整体存储, 像 json 这样的数据可以使用 hash 结构, 使用二级结构存储也能帮助我们节省内存. 同时可以使用hmget,hmset命令支持字段的部分读取修改, 而不用每次整体存取. 10.5. 编码优化10.5.1. 编码Redis对外提供了string,list,hash,set,zet等类型, 但是Redis内部针对不同类型存在编码的概念, 所谓编码就是具体使用哪种底层数据结构来实现. 编码不同将直接影响数据的内存占用和读写效率. 使用object encoding {key}命令获取编码类型. Redis 为每种数据类型都提供了两种内部编码方式, 查看key 的内部编码方式: &gt; OBJECT ENCODING key 数据类型与编码方式 数据类型 内部编码方式 OBJECT_ENCODING命令结果 数据结构 字符串类型 REDIS_ENCODING_RAM &apos;raw&apos; 动态字符串编码 REDIS_ENCODING_INT &apos;int&apos; 整数编码 REDIS_ENCODING_EMBSTR &apos;embstr&apos; 优化内存分配的字符串编码 散列类型 REDIS_ENCODING_HT &apos;hashtable&apos; 散列表编码 REDIS_ENCODING_ZIPLIST &apos;ziplist&apos; 压缩列表编码 列表类型 REDIS_ENCODING_LINKEDLIST &apos;linkedlist&apos; 双向链表编码 REDIS_ENCODING_ZIPLIST &apos;ziplist&apos; 压缩列表编码 REDIS_ENCODING_QUICKLIST &apos;quicklist&apos; 3.2 版本新的列表编码 集合类型 REDIS_ENCODING_HT &apos;hashtable&apos; 散列表编码 REDIS_ENCODING_INTSET &apos;intset&apos; 整数集合编码 有序集合类型 REDIS_ENCODING_SKIPLIST &apos;skiplist&apos; 跳跃表编码 REDIS_ENCODING_ZIPLIST &apos;ziplist&apos; 压缩列表编码 10.5.2. type 和 encoding 对应关系 类型 编码方式 数据结构 转换条件 string raw 动态字符串编码 string embstr 优化内存分配的字符串编码 string int 整数编码 hash hashtable 散列表编码 满足任意条件 : ① value 最大空间(字节) &gt; hash-max-ziplist-value ; ② field 个数 &gt; hash-max-ziplist-entries hash ziplist 压缩列表编码 满足所有条件 : ① value 最大空间(字节) &lt;= hash-max-ziplist-value ; ② field 个数 &lt;= hash-max-ziplist-entries list linkedlist 双向链表编码 满足任意条件 : ① value 最大空间(字节) &gt; list-max-ziplist-value ; ② 链表长度 &gt; list-max-ziplist-entries list ziplist 压缩列表编码 满足所有条件 : ① value 最大空间(字节) &lt;= list-max-ziplist-value ; ② 链表长度 &lt;= list-max-ziplist-entries list quicklist 3.2 版本新的列表编码 废弃 list-max-ziplist-entries 配置, 新配置 : ① list-max-ziplist-size 表示最大压缩空间或长度, 最大空间使用 [-5-1]范围配置, 默认 -2 表示 8KB , 正整数表示最大压缩长度;② list-compress-depth 表示最大压缩深度, 默认 0 不压缩 ; set hashtable 散列表编码 满足任意条件 : ① 元素必须为非整数类型 ; ② 集合长度 &gt; hash-max-ziplist-entries set intset 整数集合编码 满足所有条件 : ① 元素必须为整数 ; ② 集合长度 &lt;= hash-max-ziplist-entries zset skiplist 跳跃列表编码 满足任意条件 : ① value最大空间(字节) &gt; zset-max-ziplist-value ; ② 有序集合长度 &gt; zset-max-ziplist-entries zset ziplist 压缩列表编码 满足所有条件 : ① value最大空间(字节) &lt;= zset-max-ziplist-value ; ② 有序集合长度 &lt;= zset-max-ziplist-entries 10.5.3. 控制编码类型编码类型转换在 redis 写入数据时 自动完成, 这个转换过程不可逆.转换规则只能从小内存编码向大内存编码转换. 转换示例 : redis&gt; lpush list:1 a b c d (integer) 4 //存储4个元素 redis&gt; object encoding list:1 &quot;ziplist&quot; //采用ziplist压缩列表编码 redis&gt; config set list-max-ziplist-entries 4 OK //设置列表类型ziplist编码最大允许4个元素 redis&gt; lpush list:1 e (integer) 5 //写入第5个元素e redis&gt; object encoding list:1 &quot;linkedlist&quot; //编码类型转换为链表 redis&gt; rpop list:1 &quot;a&quot; //弹出元素a redis&gt; llen list:1 (integer) 4 // 列表此时有4个元素 redis&gt; object encoding list:1 &quot;linkedlist&quot; //编码类型依然为链表, 未做编码回退, 因为回退非常消耗 cpu, 得不偿失. 转换规则 见 10.5.2 表 : 可以使用 config set 命令设置编码相关参数来满足使用压缩编码的条件. 对于采用非压缩编码类型的数据, 如 hashtable, linkedlist 等, 设置参数后即使数据满足压缩编码的条件, redis 也不会做转换, 需要重启 redis 重新加载数据才能完成转换. 10.5.4. ziplist 编码主要为节约内存, 因此所有数据都采用线性连续的内存结构. 应用最广泛, 可以作为 hash, list, zset 类型的底层数据结构实现. 字段含义 : zlbytes : 记录整个压缩列表所占字节长度, 方便重新调整 ziplist 空间, 类型是 int-32, 长度为 4 字节. zltail : 记录距离尾节点的偏移量, 方便尾节点弹出操作, 类型 int-32, 长度 4 字节. zllen : 记录压缩链表节点数量, 当长度超过 216-2 时需要遍历整个列表获取长度, 一般很少见, 类型 int-16 , 长度为 2 字节. entry-1 : 记录具体的节点, 长度根据实际存储的数据而定. entry prev_entry_butes_length : 记录前一个节点所占空间, 用于快速定位上一个节点, 可实现列表反向迭代. encoding : 表示当前节点的编码和长度, 前两位表示编码类型: 字符串/整数, 其余位表示数据长度. contents : 保存节点值, 针对实际数据长度做内存占用优化. entry-2 : entry prev_entry_butes_length : encoding : contents : zlend : ziplist 数据结构特点 : 内部表现为数据紧凑排列的一块连续内存数组. 可以模拟双向链表结构, 以 O(1) 时间复杂度入队和出队, 新增删除操作设计内存重新分配或释放, 加大了操作的复杂性. 读写操作设计复杂的指针移动, 最坏的时间复杂度为 O(n2) 适合存储小对象和长度有限的数据. 注意: ziplist压缩编码的性能表现跟值长度和元素个数密切相关. 这对性能要求较高的场景使用 ziplist , 建议 长度不要超过 1000, 每个元素大小控制在 512 字节以内. 命令平均消耗时间使用 info Commandstats 命令获取, 包含每个命令调用次数, 总耗时, 平均耗时, 单位 微秒 redis&gt; info Commandstats 10.5.5. intset 编码intset 是集合(set)类型编码的一种, 内部表现为存储有序, 不重复的整数集.当集合只包含 整数, 且 长度不超过 set-max-intset-entries 配置时, 被启用. 127.0.0.1:6379&gt; sadd set:test 3 4 2 6 8 9 2 (integer) 6 127.0.0.1:6379&gt; OBJECT encoding set:test &quot;intset&quot; 127.0.0.1:6379&gt; SMEMBERS set:test 1) &quot;2&quot; 2) &quot;3&quot; 3) &quot;4&quot; 4) &quot;6&quot; 5) &quot;8&quot; 6) &quot;9&quot; 127.0.0.1:6379&gt; CONFIG SET set-max-intset-entries 6 OK 127.0.0.1:6379&gt; sadd set:test 5 (integer) 1 127.0.0.1:6379&gt; SMEMBERS set:test 1) &quot;6&quot; 2) &quot;3&quot; 3) &quot;8&quot; 4) &quot;4&quot; 5) &quot;9&quot; 6) &quot;2&quot; 7) &quot;5&quot; 127.0.0.1:6379&gt; OBJECT encoding set:test &quot;hashtable&quot; 127.0.0.1:6379&gt; SMEMBERS set:test 1) &quot;6&quot; 2) &quot;3&quot; 3) &quot;8&quot; 4) &quot;4&quot; 5) &quot;9&quot; 6) &quot;2&quot; 7) &quot;5&quot; intset 对写入证书进行排序, 通过 O(log(n)) 时间复杂度实现查找和去重操作 字段含义 : encoding : 整数表示类型, 格局集合内最长整数值确定类型, 证书类型划分三种 : int-16, int-32, int-64 . length : 表示集合元素个数. contents : 整数数组, 按从小到大顺序保存. intset 保存的整数类型根据长度划分, 当保存的证书超出当前类型时, 将会触发自动升级操作, 且升级后不能做回退. 升级操作会导致重新申请内存空间, 把原有数据安装转换类型后拷贝到新数组. 使用intset编码的集合时, 尽量保持整数范围一致, 如都在int-16范围内. 防止个别大整数触发集合升级操作, 产生内存浪费. 10.6. 控制 key 的数量当使用 redis 存储大量数据时, 通常会存在大量键, 过多的键同样会消耗大量内存. Redis本质是一个数据结构服务器, 它为我们提供多种数据结构, 如hash, list, set, zset 等结构. 10.7 系统优化10.7.1. 内存10.7.1.1 vm.overcommit_memoryLinux 操作系统对大部分申请内存的请求都恢复 yes, 以便能运行更多的程序, 应为申请内存之后, 并不会马上使用内存, 这种技术叫做overcommit. vm.overcommit_memory 用来设置内存分配策略, 他有三个可选值: 0 : 表示内核将检查是否有足够的可用内存. 如果有足够的可用内存, 内存申请通过, 否则内存申请失败, 并把错误返回给应用进程. 1 : 表示内核允许超量使用内存直到用完为止. 2 : 表示内存绝不过量的使用内存(“never overcommit”), 即系统整个内存地址空间不能超过 swap + 50% RAM 的值, 50% 是 overcommit_ratio 的默认值, 此参数同样支持修改. 报错信息: # WARNING overcommit_memory is set to 0! Background save may fail under low memory condition. To fix this issue add &apos;vm.overcommit_memory = 1&apos; to /etc/sysctl.conf and then reboot or run the command &apos;sysctl vm.overcommit_memory=1&apos; for this to take effect. 报错中的 Background save 代表的是 bgsave 和 bgrewritaof, 如果当前内存不足, 操作系统应该如何处理 fork. 如果vm.overcommit_memory=0, 代表如果没有可用内存, 就申请内存失败, 对应到 redis 就是 fork 执行失败, Redis 报错 Cannot allocate memory. 推荐配置: vm.overcommit_memory = 1 配置方法: # 查看当前配置 $ cat /proc/sys/vm/overcommit_memory # 设置 $ echo &quot;vm.overcommit_memory&quot; &gt;&gt; /etc/sysctl.conf $ sysctl vm.overcommit_memory=1 最佳实践: Redis 设置合理的 maxmemory, 保证机器有 20% ~ 30% 的闲置内存. 集中化管理 aof 重写 和 rdb 的 bgsave. 设置 vm.overcommit_memory=1, 防止极端情况下, 会造成 forl 失败. 10.7.1.2 swappinessswap 对于操作系统来说比较重要, 当物理内存不足时, 可以 swap out 一部分内存页, 以解燃眉之急. 但是, swap 空间由硬盘提供, 对于需要高并发, 高吞吐的应用来说, 磁盘 IO 通常会成为系统瓶颈. 在 Linux 中, 并不是要等到所有物理内存都使用完才会使用 swap, 系统参数 swappiness 会决定操作系统使用 swap 的倾向程度. swappiness 的取值范围是 0 ~ 100, swappiness 的值越大, 说明操作系统可能使用 swap 的概率越高. swappiness 值越低, 表示操作系统更加倾向于使用武力内存. swap 的默认值为 60. swappiness 策略 0 Linux 3.5 及以上: 宁愿 OOM , 也不用 swap; Linux 3.4 及以下 : 宁愿 swap 也不要 OOM 1 Linux 3.5 及以上 : 宁愿 swap 也不要 OOM. 60 默认值 100 操作系统主动使用 swap 设置方法: echo VALUE &gt; /proc/sys/vm/swappiness echo &quot;vm.swappiness=VALUE&quot; &gt;&gt; /etc/sysctl.conf 查看 redis 进程 swap 使用情况: cat /proc/REDIS_ID/smaps | grep Swap 最佳实践: 如果 Linux &gt; 3.5 , vm.swappiness=1 ; 否则 vm.swappiness=0, 从而实现如下两个目标: - 物理内存充足时, redis 足够快 - 物理内存不足时, 避免 redis 死掉. - 如果 redis 为高可用, 死掉比阻塞好. 10.7.1.3 Transparent Huge PagesLinux kernel 在 2.6.38 内核增加了 Transparent Huge Pages(THP) 特性, 支持大内存页(2M)分配, 默认开启. 当开启时可以降低 fork 子进程的速度, 但 fork 之后, 每个内存页从原来 4KB 变为 2MB, 会大幅着呢个件重写期间父进程内存消耗. 同时每次写命令引起的复制内存页单位放大了 512倍, 会拖慢写操作的执行时间, 导致大量写操作慢查询. 因此, redis 建议禁止THP 特性: $ echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled $ echo &quot;echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled&quot; &gt;&gt; /etc/rc.local 注意有些发行版将 THP 放在 /sys/kernel/mm/redhat_transparent_hugepage/enabled 中: $ echo never &gt; /sys/kernel/mm/redhat_transparent_hugepage/enabled 10.7.1.4 OOM killerOOM killer 会在可用内存不足时, 选择性的杀掉用户进程. OOM killer 进程会为每个用户进程设置一个权值, 这个权值越高, 被杀掉的概率越高, 反之, 越低. 每个进程的权值存放在 /proc/PID/oom_score 中, 这个值收到 /proc/PID/oom_adj 的控制, oom_adj 在不同的 Linux 版本中最小值不同, 可以参考 Linux源码中 oom.h (从 -15 到 -17). 当 oom_adj 设置为最小时, 该进程将不会被 OOM killer 杀掉. 对于 redis 所在的服务器来说, 可以将所有 Redis 的 oom_adj 设置为最低值或者稍小的值, 降低被 OOM killer 杀掉的概率. $ echo VALUE &gt; /proc/POD/oom_adj 提示: oom_adj 参数只起到辅助作用, 合理的规划内存更为重要; 在高可用的情况下, 被杀掉比僵死更好, 因此不要过多依赖 oom_adj 配置. 10.7.2. 系统优化10.7.2.1 使用 NTP我们知道像Redis Sentinel和Redis Cluster这两种需要多个Redis实例的类型, 可能会涉及多台服务器. 虽然Redis并没有对多个服务器的时钟有严格的要求, 但是假如多个Redis实例所在的服务器时钟不一致, 对于一些异常情况的日志排查是非常困难的, 10.7.2.2 ulimit在 Linux 中, 可以通过 ulimit 查看和设置系统的当前用户进程的资源数. 其中 open files 参数, 是单个用户同时打开的最大文件个数. Redis 允许同时有多个客户端通过网络进行连接, 可以通过设置 maxclients 来限制最大客户端连接数. 对 Linux 操作系统来说这些网络连接都是文件句柄. ulimit open files 的限制优先级比 maxclients 大. redis 建议把 open files 至少设置成 10032 , 因为 maxclients 的默认值是 10000, 这些用来处理客户端连接, 除此之外, Redis 内部会使用最多 32 个文件描述符, 所以 10032 = 10000 + 32. 10.7.3. 网络10.7.3.1 TCP backlogRedis 默认的 tcp-backlog 为 511, 可以通过修改配置 tcp-backlog 进行调整. 如果 Linux 的 tcp-backlog 小于 redis 设置的 tcp-backlog, 那么启动时会报错: # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128. 修改 TCP backlog : $ cat /proc/sys/net/core/somaxconn $ echo 511 &gt; /proc/sys/net/core/somaxconn 11. python与redis$ pip install redis # 创建链接: import redis r = redis.StrictRedis(host=&apos;127.0.0.1&apos;,port=&apos;6379&apos;,db=0) r.set(&apos;foo&apos;,&apos;bar&apos;) r.get(&apos;foo&apos;) # 字典 HMSET/HGETALL HMSET : 将字典作为参数存储 HGETALL : 返回值为字典. # 管道和事务 * 事务 pipe = r.pipeline() pipe.set(&apos;foo&apos;, &apos;bar&apos;) pipe.get(&apos;foo&apos;) result = pipe.execute() print result # [True, &apos;bar&apos;] * 管道 : 管道的使用方式和事务相同, 只不过需要在创建时加上参数transaction=False： pipe = r.pipeline(transaction=False) * 链式调用 : 事务和管道还支持链式调用： result = r.pipeline().set(&apos;foo&apos;, &apos;bar&apos;).get(&apos;foo&apos;).execute() # [True, &apos;bar&apos;] 12. redis 与 lua : Lua 语法简介12.1. 数据类型 :lua 是一个动态类型语言,一个变量可以存储类型的值.Lua 常用数据类型: 空(nil) 空类型只包含一个值,即nil . nil表示空, 没有赋值的变量或标的字段都是 nil.布尔(boolean) 布尔类型包含 True 和 False 两个值.数字(number) 整数合浮点数是都是使用数字类型存储.字符串(string) 字符串类型可以存储字符串,且与Redis的键值一样都是二进制安全的.字符串可以使用单引号或双引号表示,两个符号是相同的. 字符串可以包含转义字符,如 ‘\n’,’\r’ 等.表(table) 表类型是Lua 语言中唯一的数据结构,既可以当数组,又可以当字典,十分灵活.函数(function) 函数是Lua中的一等值(first-class value),可以存储在变量中,作为函数的参数或返回结果. 12.2. 变量:Lua 变量分为全局变量和局部变量. 全局变量无需声明就可以直接使用,默认值是 nil . &gt; print(b) a = 1 –为全局变量a赋值a = nil – 删除全局变量的方法是将其复制为 nil . 全局变量没有声明与未声明之分,只有非 nil 和 nil 的区别.print(b) –无需声明即可使用, 默认值是nil 在Redis脚本中不能使用全局变量,只允许使用局部变量,以防止脚本之间相互影响. 声明局部变量的方式为 “local 变量名” : local c –声明一个局部变量c, 默认值是nil local d = 1 –声明一个局部变量d并赋值为1 local e, f –可以同时声明多个局部变量 * 局部变量的作用域为从声明开始到所在层的语句块的结尾. 声明一个存储函数的局部变量的方法为 : local say_hi = function () print ‘hi’ end 变量名必须是非数字开头,只能包含字母,数字和下划线,区分大小写. 变量名不能与Lua的保留关键字相同, 保留关键字如下: and break do else elseif end false for function if in local nil not or repeat return then true until while 12.3. 注释 :单行 : 以 – 开始,到行尾结束.多行 : 以 –[[ 开始 ,到 ]] 结束. 12.4. 赋值 :多重赋值 : local a, b = 1, 2 – a的值是1, b的值是2 local c, d = 1, 2, 3 – c的值是1, d的值是2, 3被舍弃了 local e, f = 1 – e的值是1, f的值是nil 在执行多重赋值时,Lua会先计算所有表达式的值,比如: local a = {1, 2, 3} local i = 1 i, a[i] = i + 1, 5 – i = 2 ; a = {5,2,3} , lua 索引从 1 开始. lua 中的函数也可以返回多个值 12.5. 操作符12.5.1. 数学操作符 :常见的+、-、*、/、%(取模)、-(一元操作符, 取负)和幂运算符号^. 数学操作符的操作数如果是字符串,则会自动转换为数字. print(&apos;1&apos; + 1) -- 2 print(&apos;10&apos; * 2) -- 20 12.5.2. 比较操作符 :比较操作符的结果一定是布尔类型 ;比较操作符,不会对两边的操作数进行自动类型转换. == : 比较两个操作数的类型和值是否相等 ~= : 与 == 结果相反 &lt;,&gt;,&lt;=,&gt;= : 大于,小于,小于等于,大于等于. 12.5.3. 逻辑操作符 :只要操作数不是 nil 或 false ,逻辑操作符都认为操作数是真. 特别注意 0 或 空字符串也被当做真. Lua 逻辑操作符支持短路, 也就是说对于 false and foo() , lua 不会调用foo函数, or类似. not : 根据操作数的真和假返回false 和 true and : a and b, 如果a 是真,则返回 b , 否则返回 a . or : a or b , 如果a 是假,则返回 a , 否则返回 b . 12.5.4. 连接操作符. “…” 用来连接两个字符串.连接操作符会自动把数字类型的转换成字符串类型. 12.5.5. 取长度操作符. 是lua5.1 新增的操作符, “#” ,用来获取字符串或表的长度.&gt; print(#&apos;hello&apos;) -- 5 12.5.6. 运算符的优先级:^ not # -(一元) * / % + - .. &lt; &gt; &lt;= &gt;= ~= == and or 12.6. if 语句语法 : if 条件表达式 then 语句块 elseif 条件表达式 then 语句块 else 语句块 end 注意 : Lua 中只有 nil 和 false 才是假, 其余值,包括0 和空字符串,都被认为是真值. 在 Redis 的EXISTS命令返回值 1 和 0 分别表示存在或不存在,但无论如何,两个值在Lua 中都是真值. Lua 每个语句都可以 ; 结尾 ,但是一般来说编写 Lua 是会省略 ; , Lua 并不强制要求缩进,所有语句也可以写在一行中, 但为了增强可读性,建议在注意缩进. &gt; a = 1 b = 2 if a then b = 3 else b = 4 end 12.7. 循环语句 while 循环 while 条件表达式 do 语句块 end repeat 循环 repeat 语句块 until 条件表达式 for 循环 形式一 : for 变量=初值,终值,步长 do -- 步长可省略,默认为 1 语句块 end # 计算 1 ~ 100 之和 local sum = 0 for i = 1 ,100 do sum = sum + 1 end // for 循环中的 i 是**局部变量**, 作用域为 for 循环体内. 虽然没有使用 local 声明,但它不是全局变量. 形式二 : for 变量1 ,变量2, ... , 变量N in 迭代器 do 语句块 end 在编写Redis脚本时,我们常用通用形式的for 语句遍历表的值. 12.8. 表类型表是Lua中唯一的数据结构,可以理解为关联数组,任何类型的值(除了空类型)都可以作为表的索引. a = {} --将变量a赋值为一个空表 a[&apos;field&apos;] = &apos;value&apos; --将field字段赋值value print(a.field) --打印内容为&apos;value&apos;, a.field是a[&apos;field&apos;]的语法糖. people = { --也可以这样定义 name = &apos;Bob&apos;, age = 29 } 当索引为整数的时候表和传统的数组一样, 例如： a = {} a[1] = ‘Bob’ a[2] = ‘Jeff’ 可以写成下面这样： a = {‘Bob’, ‘Jeff’} print(a[1]) –打印的内容为’Bob’ 可以使用通用形式的for语句遍历数组,例如: for index,value in ipairs(a) do -- index 迭代数组a 的索引 ; value 迭代数组a 的值. print(index) print(value) end ** ipairs 是Lua 内置的函数,实现类似迭代器的功能. 数字形式的for语句 for i=1,#a do print(i) print(a[i]) end pair : 迭代器,用来遍历非数组的表值 person = { name = &apos;bob&apos;, age = 29 } for index,value in pairs(person) do print(index) print(value) end pairs 与 ipairs 的区别在于前者会办理所有值不为 nil 的索引, 而后者只会从索引 1 开始递增遍历到最后一个值不为 nil 的整数索引. 12.9. 函数函数的一般格式: function(参数列表) 函数体 end 示例: local function square(...) local argv = {...} for i = 1,#argv do argv[i] = argv[i] * argv[i] end return unpack(argv) -- unpack 函数用来返回 表 中的元素. 相当于return argv[1], argv[2], argv[3] end a,b,c = square(1,2,3) print(a) -- 1 print(b) -- 4 print(c) -- 9 可以将函数赋值给一个局部变量, 比如: local square = function(num) return num*num end // 因为在赋值前声明了局部变量 square, 所以可以在函数内部引用自身(实现递归). 函数参数 : 如果实参的个数小于形参的个数,则没有匹配到的形参的值为 nil . 相对应的,如果实参的个数大于形参的个数,则多出的实参会被忽略. 如果希望捕获多出的参数(即实现可变参数个数),可以让最后一个形参为 ‘…’ . 12.10. return &amp; break在 Lua 中 return 和 break (用于跳出循环) 语句必须是语句块中的最后一条语句, 简单的说在这两条语句之后只能是 end,else 或 until 三者之一. 如果希望在语句块中间使用这两条语句,可以人为的使用 do 和 end 将其包围 . 12.11. 标准库Lua 的标准库中提供了很多使用的函数, 比如 ipairs,pairs,tonumber,tostring,unpack 都属于标准库中的Base库. Redis 支持大部分Lua标准库,如下所示: 库名 说明 Base 一些基础函数 String 用于字符串操作的函数 Table 用于表操作的函数 Math 数学计算函数 Debug 调试函数 12.11.1 String库 : 可以通过字符串类型的变量以面向对象的形式访问, 如 string.len(string_var) 可以写成 string_var:len()获取字符串长度 : string.len(string) 作用与操作符 “#” 类似 &gt; print(string.len(&apos;hello&apos;)) -- 5 &gt; print(#&apos;hello&apos;) -- 5 转换大小写 string.upper(string) string.lower(string) 获取子字符串 string.sub(string start[,end ]) -- end 默认为 -1. string.sub() 可以获取一个字符串从索引 start 开始到 end 结束的子字符串,索引从1 开始. 索引也可以是负数, -1 代表最后一个元素 . &gt; print(string.sub(&apos;hello&apos;,1)) -- hello &gt; print(string.sub(&apos;hello&apos;,2)) -- ello &gt; print(string.sub(&apos;hello&apos;,2,-2)) -- ell 12.11.2 Table库 : 其中大部分函数都需要表的形式是数组形式.将数组转换为字符串 table.concat(table [,sep [,i [,j]]]) - sep : 以 sep 指定的参数分割, 默认为空. - i , j : 用来限制要转换的表元素的索引范围. 默认分别为 1 和 表的长度. 不支持负索引. &gt; print(table.concat({1,2,3})) --123 &gt; print(table.concat({1,2,3},&apos;,&apos;,2)) --2,3 &gt; print(table.concat({1,2,3},&apos;,&apos;,2,2)) --2 向数组中插入元素 : # 在指定索引位置 pos 插入元素 value, 并将后面的元素顺序后移. 默认 pos 值是数组长度加 1 , 即在数组尾部插入. table.insert(table ,[pos,] value) &gt; a = {1,2,4} &gt; table.insert(a,3,3) # {1,2,3,4} &gt; table.insert(a,5) # {1,2,3,4,5} &gt; print(table.concat(a,&apos;,&apos;)) 1,2,3,4,5 从数组中弹出一个元素 # 从指定的索引删除一个元素,并将后面的元素前移,返回删除元素值. 默认 pos 的值是数组的长度,即从数组尾部弹出一个元素. table.remove(table,[,pos]) &gt; table.remove(a) --{1,2,3,4} &gt; table.remove(a,1) --{2,3,4} &gt; print(table.caoncat(a,&apos;,&apos;)) 2,3,4 12.11.3 Math库 : 提供常用的数学运算函数, 如果参数是字符串会自动尝试转换成数字.math.abs(x) # 绝对值 math.sin(x) # 三角函数sin math.cos(x) # 三角函数cos math.tan(x) # 三角函数tan math.ceil(x) # 进一取整, 1.2 取整后是 2 math.floor(x) # 向下取整, 1.8 取整后是 1 math.max(x,...) # 获得参数中的最大的值 math.min(x,...) # 获取参数中的最小的值 math.pow(x,y) # 获取 xy 的值 math.sqrt(x) # 获取 x 的平方根 math.random([m,[,n]]) # 生成随机数,没有参数 返回 [0,1]的实数, 参数 m 返回范围在 [1,m] 的整数, 同时提供 m n 返回范围在 [m,n] 的整数. math.randomseed(x) # 设置随机数种子, 同一种子生成的随机数相同. 12.12. 其他库Redis 还通过 cjson库 和 cmsgpack库 提供了对 JSON 和 MessagePack的支持. Redis自动加载了这两个库,在脚本中可以分别通过 cjson 和 cmsgpack 两个全局变量来访问对应的库. local people = { name = &apos;bob&apos;, age = 29 } -- 使用 cjson 序列化成字符串 local json_people_str = cjson.encode(people) -- 使用 cmsgpack 序列化成字符串 local msgpack_people_str = cmsgpack.pack(people) -- 使用 cjson 将序列化后的字符串还原成表 local json_people_obj = cjson.decode(people) print(json_people_obj.name) -- 使用 cmshpack 将序列化后的字符串还原成表 local msgpack_people_obj = cmsgpack.unpack(people) print(msgpack_people_obj.name) 12.13. Redis 与 Lua 在脚本中调用redis命令： redis.call(‘set’, ‘foo’, ‘bar’) local value = redis.call(‘get’, ‘foo’) – value的值为bar 脚本相关命令 redis&gt; EVAL 脚本内容 key参数数量 [key…] [arg…] 13. 持久化13.1. RDB方式 : 存数据. 快照式. 根据指定的规则“定时”将内存中的数据存储在硬盘上RDB 方式的持久化是通过快照(snapshotting)完成的, 当符合一定条件时Redis会自动将内存中的所有数据生成一份副本并存储在硬盘上, 这个过程即为快照. 13.1.1 实现方式及原理: Redis 在进行数据持久化的过程中, 会先将数据写入到一个临时文件中, 待持久化过程都结束了, 才会用这个临时文件替换上次持久化好的文件. 正是这种特性, 让我们可以随时来进行备份, 因为快照文件总是完整可用的. 对于 RDB 方式, redis 会单独创建 (fork) 一个子进程来进行持久化, 而主进程是不会进行任何IO操作的, 这样就确保了redis 极高的性能. 如果需要进行大规模数据的恢复, 且对于数据恢复的完整性不是非常敏感, 那 RDB 方式要比 AOF 方式更加的高效. 虽然 RDB 有不少优点, 但它的缺点也是不容忽视的. 如果你对数据的完整性非常敏感, 那么 RDB 方式就不太适合你, 因为即使你每 5 分钟都持久化一次, 当 redis 故障时, 仍然会有近 5 分钟的数据丢失. 在执行 fork 的时候操作系统(类 Unix 操作系统)会使用写时复制(copy-on-write)策略, 即fork函数发生的一刻父子进程共享同一内存数据, 当父进程要更改其中某片数据时(如执行一个写命令), 操作系统会将该片数据复制一份以保证子进程的数据不受影响, 所以新的RDB文件存储的是执行 fork 一刻的内存数据. 另外需要注意的是, 当进行快照的过程中, 如果写入操作较多, 造成 fork 前后数据差异较大, 是会使得内存使用量显著超过实际数据大小的, 因为内存中不仅保存了当前的数据库数据, 而且还保存着 fork 时刻的内存数据. 进行内存用量估算时很容易忽略这一问题, 造成内存用量超限. 任何时候 RDB 文件都是完整的. 这使得我们可以通过定时备份 RDB 文件来实现 Redis 数据库备份. RDB 文件是经过压缩(可以配置rdbcompression参数以禁用压缩节省CPU占用) 的二进制格式, 所以占用的空间会小于内存中的数据大小, 更加利于传输. Redis启动后会读取RDB快照文件, 将数据从硬盘载入到内存. 根据数据量大小与结构和服务器性能不同, 这个时间也不同. 通常将一个记录1000万个字符串类型键、大小为1 GB 的快照文件载入到内存中需要花费20～30秒. 通过RDB方式实现持久化, 一旦Redis异常退出, 就会丢失最后一次快照以后更改的所有数据. 这就需要开发者根据具体的应用场合, 通过组合设置自动快照条件的方式来将可能发生的数据损失控制在能够接受的范围. 快照过程: Redis使用fork函数复制一份当前进程(父进程)的副本(子进程)； 父进程继续接收并处理客户端发来的命令, 而子进程开始将内存中的数据写入硬盘中的临时文件； 当子进程写入完所有数据后会用该临时文件替换旧的 RDB 文件, 至此一次快照操作完成. 13.1.2 Redis对数据进行快照情况/场景: 根据配置规则进行自动快照 : redis.conf 时间窗口 M 和 改动键的个数 N : 每当时间 M 内被改动的键的个数大于 N 时,即自动快照. save 900 1 save 300 10 save 60 10000 同时可以存在多个条件, 条件之间是或的关系. 时间单位是 秒 . 用户指定 SAVA 或 BGSAVE 命令 当进行服务重启、手动迁移以及备份时我们也会需要手动执行快照操作. SAVE : 当执行SAVE命令时, Redis同步地进行快照操作, 在快照执行的过程中会阻塞所有来自客户端的请求. 当数据库中的数据比较多时, 这一过程会导致 Redis 较长时间不响应, 所以要尽量避免在生产环境中使用这一命令. BGSAVE : 需要手动执行快照时推荐使用 BGSAVE 命令. BGSAVE 命令可以在后台异步地进行快照操作, 快照的同时服务器还可以继续响应来自客户端的请求. 执行 BGSAVE后Redis会立即返回 OK 表示开始执行快照操作, 如果想知道快照是否完成, 可以通过 LASTSAVE命令获取最近一次成功执行快照的时间, 返回结果是一个Unix时间戳. 执行 FLUSHALL 命令 当执行 FLUSHALL 命令时, Redis 会清除数据库中的所有数据. 需要注意的是, 不论清空数据库的过程是否触发了自动快照条件, 只要自动快照条件不为空, Redis就会执行一次快照操作. 当没有定义自动快照条件时, 执行FLUSHALL则不会进行快照. 执行复制(replication) 时. 当设置了主从模式时, Redis 会在复制初始化时进行自动快照. 当使用复制操作时, 即使没有定义自动快照条件, 并且没有手动执行过快照操作, 也会生成RDB快照文件. 13.1.3 快照保存 :Redis默认会将快照文件存储在Redis 当前进程的工作目录中的dump.rdb文件中, 也可以通过修改如下 redis.conf 中的参数, 指定快照文件的保存位置: dir : 指定快照文件的 存放路径; dbfilename : 指定 rdb 文件名. 13.1.4 关闭 RDB :redis &gt; CONFIG SET save &quot;&quot; 13.2. AOF方式 : 存操作. 在每次执行命令后将命令本身记录下来, Append Only File.13.2.1. 原理只允许追加不允许改写的文件. AOF 方式是将执行过的写指令记录下来, 在数据恢复时按照从前到后的顺序再将指令都执行一遍, 就这么简单. AOF可以将Redis执行的每一条写命令追加到硬盘文件中, 这一过程显然会降低Redis 的性能, 但是大部分情况下这个影响是可以接受的, 另外使用较快的硬盘可以提高AOF的性能. 原理 : 以纯文本方式实现. AOF文件的内容正是 Redis 客户端向 Redis 发送的原始通信协议的内容. 在启动时Redis会逐个执行AOF文件中的命令来将硬盘中的数据载入到内存中, 载入的速度相较RDB会慢一些. AOF 重写的内部运行原理 : 在重写即将开始之际, redis 会创建(fork)一个重写子进程, 这个子进程会首先读取现有的 AOF 文件, 并将其包含的指令进行分析压缩并写入到一个临时文件中. 与此同时, 主工作进程会将新接收到的写指令一边累积到内存缓冲区中, 一边继续写入到原有的 AOF 文件中, 这样做是保证原有的 AOF 文件的可用性, 避免在重写过程中出现意外. 当重写子进程完成重写工作后, 它会给父进程发一个信号, 父进程收到信号后就会将内存中缓存的写指令追加到新 AOF 文件中. 当追加结束后, redis 就会用新 AOF 文件来代替旧 AOF 文件, 之后再有新的写指令, 就都会追加到新的 AOF 文件中了. 重写机制 因为采用了追加方式, 如果不做任何处理的话, AOF 文件会变得越来越大, 为此, redis 提供了 AOF 文件重写(rewrite)机制, 即当 AOF 文件的大小超过所设定的阈值时, redis 就会启动 AOF 文件的内容压缩, 只保留可以恢复数据的最小指令集. 举个例子或许更形象, 假如我们调用了 100 次INCR指令, 在 AOF 文件中就要存储 100 条指令, 但这明显是很低效的, 完全可以把这 100 条指令合并成一条 SET 指令, 这就是重写机制的原理. 在进行 AOF 重写时, 仍然是采用先写临时文件, 全部完成后再替换的流程, 所以断电、磁盘满等问题都不会影响 AOF 文件的可用性. 在同样数据规模的情况下, AOF 文件要比 RDB 文件的体积大. 而且, AOF 方式的恢复速度也要慢于 RDB 方式. 重写机制相关配置: 自动重写 AOF 文件 : 取消redis命令执行中的冗余. # 当目前的AOF文件大小超过上一次重写时的AOF文件大小的百分之多少时会再次进行重写, 如果之前没有重写过, 则以启动时的AOF文件大小为依据 auto-aof-rewrite-percentage 100 # 限制了允许重写的最小AOF文件大小, 通常在AOF文件很小的情况下即使其中有很多冗余的命令我们也并不太关心 auto-aof-rewrite-min-size 64mb 重写的过程只和内存中的数据有关, 和之前的 AOF文件无关, 手动执行AOF文件重写 : # redis 会生成一个全新的 AOF 文件, 其中便包括了可以恢复现有数据的最少的命令集. &gt; BGREWRITEAOF 同步磁盘缓存到硬盘频率 : # 每秒执行一次同步操作. appendfsync everysec # 表示每次执行写入都会执行同步, 这是最安全也是最慢的方式. appendfsync always # 表示不主动进行同步操作, 而是完全交由操作系统来做(即每30秒一次), 这是最快但最不安全的方式. appendfsync no 13.2.2. 配置开启AOF : 默认 AOF 没有开启. 编辑 redis.conf appendonly yes 文件保存位置 : dir : 设置路径 appendfilename appendonly.aof : 设置文件名称. 默认的 AOF 持久化策略是每秒钟 fsync 一次(fsync 是指把缓存中的写指令记录到磁盘中), 因为在这种情况下, redis 仍然可以保持很好的处理性能, 即使 redis 故障, 也只会丢失最近 1 秒钟的数据. 如果在追加日志时, 恰好遇到磁盘空间满、inode 满或断电等情况导致日志写入不完整, 也没有关系, redis 提供了 redis-check-aof 工具, 可以用来进行日志修复. 13.2.3. 修复 AOF 文件.如果运气比较差, AOF 文件出现了被写坏的情况, 也不必过分担忧, redis 并不会贸然加载这个有问题的 AOF 文件, 而是报错退出. 可以通过以下步骤来修复出错的文件: 备份被写坏的 AOF 文件; 运行 redis-check-aof –fix 进行修复; 用 diff -u 来看下两个文件的差异, 确认问题点; 重启 redis, 加载修复后的 AOF 文件. 13.3. AOF + RDBRedis 允许同时开启 AOF 和 RDB, 既保证了数据安全又使得进行备份等操作十分容易. 此时重新启动Redis后Redis会使用AOF文件来恢复数据, 因为AOF方式的持久化可能丢失的数据更少. 13.4. 备份与恢复13.4.1. 备份 :对于RDB和AOF, 都是直接拷贝文件即可, 可以设定crontab进行定时备份 . 13.4.2. 恢复 :13.4.2.1. RDB将备份文件拷到 data 目录,并给 redis-server 访问权限, 并重启 redis-server 13.4.2.2. AOF重启时加载AOF文件恢复数据. 13.4.2.3. RDB + AOF 只需要将aof文件放入data目录, 启动redis-server, 查看是否恢复, 如无法恢复则应该将aof关闭后重启, redis就会从rdb进行恢复了, 随后调用命令BGREWRITEAOF进行AOF文件写入, 在info的aof_rewrite_in_progress为0后一个新的aof文件就生成了, 此时再将配置文件的 aof 打开, 再次重启redis-server就可以恢复了. 注意: 先不要将dump.rdb放入data目录, 否则会因为aof文件万一不可用, 则 rdb 也不会被恢复进内存, 此时如果有新的请求进来后则原先的 rdb 文件被重写. 14. 集群14.1. 复制复制场景下数据库分为两中角色, 主数据库(master), 可以进行读写操作, 当写操作导致数据变化时会自动将数据同步给从数据库. 从数据库(slave), 一般是只读的, 并接受主数据库同步过来的数据. 一个主数据库可以拥有多个从数据库, 而一个从数据库只能拥有一个主数据库, redis 是支持主从同步的, 而且也支持一主多从以及多级从结构. 主从结构, 一是为了纯粹的冗余备份, 二是为了提升读性能, 比如很消耗性能的 SORT 就可以由从服务器来承担. 在具体的实践中, 可能还需要考虑到具体的法律法规原因, 单纯的主从结构没有办法应对多机房跨国可能带来的数据存储问题, 这里需要特别注意一下 redis 的主从同步是异步进行的, 这意味着主从同步不会影响主逻辑, 也不会降低 redis 的处理性能. 主从架构中, 可以考虑关闭主服务器的数据持久化功能, 只让从服务器进行持久化, 这样可以提高主服务器的处理性能. 在主从架构中, 从服务器通常被设置为只读模式, 这样可以避免从服务器的数据被误修改. 但是从服务器仍然可以接受 CONFIG 等指令, 所以还是不应该将从服务器直接暴露到不安全的网络环境中. 如果必须如此, 那可以考虑给重要指令进行重命名, 来避免命令被外人误执行. 14.1.1 配置 : 配置文件配置方式 –&gt; 推荐生产环境 从数据库的配置文件中添加 : slaveof master_ip master_port 主数据库无需配置 . 命令行形式 : $ redis-server --port 6380 --slaveof 127.0.0.1 6379 在运行时设置 : redis&gt; SLAVEOF 127.0.0.1 6379 SLAVEOF NO ONE : # 使当前数据库停止接收其他数据库的同步并转换成为主数据库. redis&gt; SLAVEOF NO ONE 查看主从状态 redis SALV&gt; INFO replication role:master connected_slaves:1 slave0:ip=127.0.0.1,port=6380,state=online,offset=1,lag=1 master_repl_offset:1 redis MAST&gt; INFO replication role:slave master_host:127.0.0.1 master_port:6379 14.1.2 原理及过程 复制初始化 当一个从数据库启动后, 会向主数据库发送 SYNC 命令. 同时主数据库接收到SYNC命令后会开始在后台保存快照(即RDB持久化的过程), 并将保存快照期间接收到的命令缓存起来. 即使有多个从服务器同时发来 SYNC 指令, 主服务器也只会执行一次BGSAVE, 然后把持久化好的 RDB 文件发给多个下游. 当快照完成后, Redis会将快照文件和所有缓存的命令发送给从数据库. 从数据库收到后, 会载入快照文件并执行收到的缓存的命令. 复制同步阶段. 异步. 主数据库每当收到写命令时就会将命令同步给从数据库, 从而保证主从数据库数据一致. 在同步的过程中从数据库并不会阻塞, 而是可以继续处理客户端发来的命令. 默认情况下, 从数据库会用同步前的数据对命令进行响应. 可以配置 slave-serve-stale-data no 来使从数据库在同步完成前对所有命令(除了INFO和SLAVEOF)都回复错误：”SYNC with master in progress. “ 断线重连 : redis_version &lt; 2.6 重新进行 复制初始化(即主数据库重新保存快照并传送给从数据库), 即使从数据库可能仅有几条命令没有收到, 主数据库也必须要将数据库里的所有数据重新传送给从数据库. redis_version &gt; 2.8 断线重连能够支持 有条件的增量数据传输 , 当从数据库重新连接上主数据库后, 主数据库只需要将断线期间执行的命令传送给从数据库, 从而大大提高Redis复制的实用性. 14.1.3 乐观复制策略Redis采用了乐观复制(optimistic replication)的复制策略, 容忍在一定时间内主从数据库的内容是不同的, 但是两者的数据会最终同步. 限制只有当数据至少同步给指定数量的从数据库时, 主数据库才是可写的： # 只有当3个或3个以上的从数据库连接到主数据库时, 主数据库才是可写的, 否则会返回错误. min-slaves-to-write 3 # 允许从数据库最长失去连接的时间, 如果从数据库最后与主数据库联系(即发送 REPLCONF ACK命令)的时间小于这个值, 则认为从数据库还在保持与主数据库的连接. min-slaves-max-lag 10 14.1.4 图结构(级联结构)从数据库也可以有从数据库. 14.1.5 从数据库持久化 :持久化是相对耗时的. 为了提高性能, 可以通过复制功能建立一个(或若干个)从数据库, 并在从数据库中启用持久化, 同时在主数据库禁用持久化. 当主数据库崩溃时,恢复步骤 : 在从数据库中使用 SLAVEOF NO ONE 命令将从数据库提升成主数据库继续服务 ; 启动之前崩溃的主数据库, 然后使用SLAVEOF命令将其设置成新的主数据库的从数据库, 即可将数据同步回来 . 当开启复制且主数据库关闭持久化功能时, 一定不要使用Supervisor 以及类似的进程管理工具令主数据库崩溃后自动重启. 同样当主数据库所在的服务器因故关闭时, 也要避免直接重新启动. 这是因为当主数据库重新启动后, 因为没有开启持久化功能, 所以数据库中所有数据都被清空, 这时从数据库依然会从主数据库中接收数据, 使得所有从数据库也被清空, 导致从数据库的持久化失去意义. 14.1.6 无硬盘复制 : redis &gt; 2.8.18Redis引入了无硬盘复制选项, 开启该选项时, Redis在与从数据库进行复制初始化时将不会将快照内容存储到硬盘上, 而是直接通过网络发送给从数据库, 避免了硬盘的性能瓶颈. 目前无硬盘复制的功能还在试验阶段, 可以在配置文件中使用如下配置来开启该功能： repl-diskless-sync yes 14.1.7 增量复制 基础 : 从数据库会存储主数据库的运行ID(run id). 每个Redis 运行实例均会拥有一个唯一的运行ID, 每当实例重启后, 就会自动生成一个新的运行ID. 在复制同步阶段, 主数据库每将一个命令传送给从数据库时, 都会同时把该命令存放到一个积压队列(backlog)中, 并记录下当前积压队列中存放的命令的偏移量范围. 同时, 从数据库接收到主数据库传来的命令时, 会记录下该命令的偏移量. 过程 : 当主从连接准备就绪后, 从数据库会发送一条 SYNC 命令来告诉主数据库可以开始把所有数据同步过来了. 而 2.8 版之后, 不再发送 SYNC命令, 取而代之的是发送 PSYNC, 格式为PSYNC主数据库的运行ID 断开前最新的命令偏移量. 主数据库收到 PSYNC命令后, 会执行以下判断来决定此次重连是否可以执行增量复制. a. 首先主数据库会判断从数据库传送来的运行ID是否和自己的运行ID相同. 这一步骤的意义在于确保从数据库之前确实是和自己同步的, 以免从数据库拿到错误的数据(比如主数据库在断线期间重启过, 会造成数据的不一致). b. 然后判断从数据库最后同步成功的命令偏移量是否在积压队列中, 如果在则可以执行增量复制, 并将积压队列中相应的命令发送给从数据库. c. 如果此次重连不满足增量复制的条件, 主数据库会进行一次全部同步 大部分情况下, 增量复制的过程对开发者来说是完全透明的, 开发者不需要关心增量复制的具体细节. 2.8 版本的主数据库也可以正常地和旧版本的从数据库同步(通过接收SYNC 命令), 同样 2.8 版本的从数据库也可以与旧版本的主数据库同步(通过发送 SYNC命令). 唯一需要开发者设置的就是积压队列的大小了. 积压队列 1 .repl-backlog-size : 积压队列的大小,积压队列越大,允许主从断线的时间越长. 积压队列在本质上是一个固定长度的循环队列, **默认情况下积压队列的大小为 1 MB**, 可以通过配置文件的repl-backlog-size选项来调整. 很容易理解的是, 积压队列越大, 其允许的主从数据库断线的时间就越长. 根据主从数据库之间的网络状态, 设置一个合理的积压队列很重要. 因为**积压队列存储的内容是命令本身**, 如 `SET foo bar`, 所以估算积压队列的大小只需要**估计主从数据库断线的时间中主数据库可能执行的命令的大小即可**. repl-backlog-ttl : 当主从连接断开之后,经过多久可以释放积压队列的内存空间,默认1h. 与积压队列相关的另一个配置选项是repl-backlog-ttl, 即当所有从数据库与主数据库断开连接后, 经过多久时间可以释放积压队列的内存空间. 默认时间是1小时. 14.2. 哨兵 :14.2.0. 作用 监控主数据库和从数据库是否正常运行 主数据库出现故障时,自动主从切换. 需要客户端也能实现自动的主从切换, 或者在 redis 集群前端配置负载均衡或者自动切换. 14.2.1. 配置过程一个哨兵可以监控多个集群, 一个集群也可以配置多个哨兵进行监控. 配置哨兵监控一个系统时,只需要配置其监控主数据库即可,哨兵会自动发现所有复制该主数据库的从数据库. quorum : 表示最低通过票数. 执行故障恢复(切换)前,至少需要几个哨兵点同意. # 基本配置 $ vim /etc/redis/sentinel.conf # sentinel monitor master_name ip redis-port quorum + sentinel monitor mymaster 127.0.0.1 6379 1 # mymaster : 表示要监控的主数据库的名字 # 127.0.0.1 6379 : 表示主数据库的地址和端口号, # 启动 $ redis-server /etc/redis/sentinel.conf --sentinel 14641:X 28 Sep 14:59:14.771 # Sentinel runid is 22c9d5b569313d6140806a12ccdc9792df3299c7 14641:X 28 Sep 14:59:14.771 # +monitor master mymaster 127.0.0.1 6379 quorum 1 14641:X 28 Sep 14:59:14.772 * +slave slave 127.0.0.1:6380 127.0.0.1 6380 @ mymaster 127.0.0.1 6379 14641:X 28 Sep 14:59:14.774 * +slave slave 127.0.0.1:6381 127.0.0.1 6381 @ mymaster 127.0.0.1 6379 14.2.2. 其他配置 一个哨兵节点可以同时监控多个Redis主从系统, 只需要提供多个sentinel monitor配置即可. 同时,多个哨兵节点也可以同时监控同一个 Redis 主从系统, 从而形成网状结构. # 一个哨兵监控多个集群 sentinel monitor mymaster 127.0.0.1 6379 2 sentinel monitor othermaster 192.168.1.3 6380 4 配置文件中还可以定义其他监控相关的参数, 每个配置选项都包含主数据库的名字使得监控不同主数据库时可以使用不同的配置参数. 例如 sentinel down-after-milliseconds mymaster 60000 sentinel down-after-milliseconds othermaster 10000 其他配置. sentinel failover-timeout mymaster 180000 sentinel parallel-syncs mymaster 1 14.3. 集群14.4. 分区分区是分割数据到多个 Redis 实例的处理过程, 因此每个实例只保存 key 的一个子集. 分区主要用于扩充容量, 而不是提高redis 的可用性. 通过利用多台计算机内存的和值, 允许我们构造更大的数据库 通过多核和多台计算机, 允许我们扩展计算能力 通过多台计算机和网络适配器, 允许我们扩展网络带宽 分区实际上把数据进行了隔离, 如果原本应该在同一分区的数据被放在了不同分区, 或者原本没有太多关系的数据因为新的业务产生了关系, 就会遇到一些问题： 涉及多个 key 的操作通常是不被支持的. 举例来说, 当两个 set 映射到不同的 redis 实例上时, 你就不能对这两个 set 执行交集操作 涉及多个 key 的 redis 事务不能使用 当使用分区时, 数据处理较为复杂, 比如你需要处理多个 rdb/aof 文件, 并且从多个实例和主机备份持久化文件 增加或删除容量也比较复杂. redis 集群大多数支持在运行时增加, 删除节点的透明数据平衡的能力, 但是类似于客户端分区、代理等其他系统则不支持这项特性. 然而, 一种叫做 presharding 的技术对此是有帮助的. Redis 有两种类型分区. 假设有 4 个 Redis实例 R0, R1, R2, R3, 和类似 user:1, user:2 这样的表示用户的多个 key, 对既定的 key 有多种不同方式来选择这个 key 存放在哪个实例中. 也就是说, 有不同的系统来映射某个 key 到某个 Redis 服务. 范围分区 最简单的分区方式是按范围分区, 就是映射一定范围的对象到特定的 Redis 实例. 比如, ID 从 0 到 10000 的用户会保存到实例 R0, ID 从 10001 到 20000 的用户会保存到 R1, 以此类推. 这种方式的不足之处是要有一个区间范围到实例的映射表, 同时还需要各种对象的映射表, 通常对 Redis 来说并非是好的方法. 哈希分区 另外一种分区方法是 hash 分区. 这对任何 key 都适用, 也无需是 object_name: 这种形式, 只需要确定统一的哈希函数, 然后通过取模确定应该保存在哪个分区即可. 15. 管理15.1. 安全Redis 安全问题 : redis 是一个弱安全的组件,只有一个简单的明文密码. 使用redis单独用户和组进行安全部署, 并且在OS层面禁止此用户ssh登陆, 这就从根本上防止了root用户启停redis带来的风险. 修改默认端口, 降低网络简单扫描危害. 修改绑定地址, 如果是本地访问要求绑定本地回环. 要求设置密码, 并对配置文件访问权限进行控制, 因为密码在其中是明文. 设置密码: $ config set requirepass [PASSWD] 认证密码 : redis-cli &gt; auth PASSWD HA环境下主从均要求设置密码. 建议在网络防火墙层面进行保护, 杜绝任何部署在外网直接可以访问的redis的出现. 15.2. 通信协议15.3. 管理工具16. 监控16.1 redis-cli info 内存使用 info memory ** 如果 Redis 使用的内存超出了可用的物理内存大小, 那么 Redis 很可能系统会被 OOM Killer 杀掉. 为使用内存量设定阈值, 并设定相应的报警机制. used_memory used_memory_peak 持久化 info Persistence rdb_last_save_time : 进行监控, 了解你最近一次 dump 数据操作的时间, rdb_changes_since_last_save : 进行监控来知道如果这时候出现故障, 你会丢失多少数据. 主从复制 info replication master_link_status : 进行监控, 如果这个值是 up, 那么说明同步正常, 如果是 down, fork 性能 info stats 当 Redis 持久化数据到磁盘上时, 它会进行一次 fork 操作, 通过 fork 对内存的 copy on write 机制最廉价的实现内存镜像. 但是虽然内存是 copy on write 的, 但是虚拟内存表是在 fork 的瞬间就需要分配, 所以 fork 会造成主线程短时间的卡顿(停止所有读写操作), 这个卡顿时间和当前 Redis 的内存使用量有关. 通常 GB 量级的 Redis 进行 fork 操作的时间在毫秒级. latest_fork_usec : 监控最近一次fork操作使用时间. 16.2 慢日志Redis 的慢查询日志功能用于记录执行时间超过给定时长的命令请求, 用户可以通过这个功能产生的日志来监视和优化查询速度. Redis 提供了 SLOWLOG 指令来获取最近的慢日志, Redis 的慢日志是直接存在内存中的, 所以它的慢日志开销并不大, 在实际应用中, 我们通过 crontab 任务执行 SLOWLOG 命令来获取慢日志, 然后将慢日志存到文件中, 并用 Kibana 生成实时的性能图表来实现性能监控. Redis 的慢日志记录的时间, 仅仅包括 Redis 自身对一条命令的执行时间, 不包括 IO 的时间, 比如接收客户端数据和发送客户端数据这些时间. Redis 的慢日志和其它数据库的慢日志有一点不同, 其它数据库偶尔出现 100ms 的慢日志可能都比较正常, 因为一般数据库都是多线程并发执行, 某个线程执行某个命令的性能可能并不能代表整体性能, 但是对 Redis 来说, 它是单线程的, 一旦出现慢日志, 可能就需要马上得到重视, 最好去查一下具体是什么原因了. 配置 # 选项指定执行时间超过多少微秒(1 秒等于 1,000,000 微秒)的命令请求会被记录到日志上. 设置的单位是微妙, 默认是10000微妙, 也就是10ms slowlog-log-slower-than # 选项指定服务器最多保存多少条慢查询日志. 服务器使用先进先出的方式保存多条慢查询日志： 当服务器储存的慢查询日志数量等于 slowlog-max-len 选项的值时, 服务器在添加一条新的慢查询日志之前, 会先将最旧的一条慢查询日志删除. slowlog-max-len slowlog 格式详解 &gt; SLOWLOG GET 10 1) 1) (integer) 4 # 日志的唯一标识符(uid) 2) (integer) 1378781447 # 命令执行时的 UNIX 时间戳 3) (integer) 13 # 命令执行的时长, 以微秒计算 4) 1) &quot;SET&quot; # 命令以及命令参数 2) &quot;database&quot; 3) &quot;Redis&quot; 结果为查询ID、发生时间、运行时长 和 原命令, 默认10毫秒, 默认只保留最后的128条. 单线程的模型下, 一个请求占掉10毫秒是件大事情, 注意设置和显示的单位为 微秒, 注意这个时间是不包含网络延迟的. 获取慢查询日志 &gt; slowlog get &gt; slowlog get 10 # 获取前10条 &gt; slowlog get -10 # 获取后10条 获取慢查询日志条数 &gt; slowlog len 清空慢查询 &gt; slowlog reset 16.3 监控服务 sentinel : 哨兵 是 Redis 自带的工具, 它可以对 Redis 主从复制进行监控, 并实现主挂掉之后的自动故障转移. 在转移的过程中, 它还可以被配置去执行一个用户自定义的脚本, 在脚本中我们就能够实现报警通知等功能. Redis Live Redis Live 是一个更通用的 Redis 监控方案, 它的原理是定时在 Redis 上执行 MONITOR 命令, 来获取当前 Redis 当前正在执行的命令, 并通过统计分析, 生成web页面的可视化分析报表 Redis Faina 其原理和 Redis Live 类似, 都是对通过 MONITOR 来做的. 16.4 数据分布 : redis 数据集分析 Redis-sampler Redis-sampler 是 Redis 作者开发的工具, 它通过采样的方法, 能够让你了解到当前 Redis 中的数据的大致类型, 数据及分布状况. redis-audit Redis-audit 是一个脚本, 通过它, 我们可以知道每一类 key 对内存的使用量. 它可以提供的数据有：某一类 key 值的访问频率如何, 有多少值设置了过期时间, 某一类 key 值使用内存的大小, 这很方便让我们能排查哪些 key 不常用或者压根不用. redis-rdb-tools 跟 Redis-audit 功能类似, 不同的是它是通过对 rdb 文件进行分析来取得统计数据的. 17. 其他管理类遍历数据库中的键 : &gt; keys * # 生产环境已禁止,当数据库很大时,会阻塞数据库. &gt; SCAN cursor [MATCH pattern] [COUNT count] # 以渐进的方式,分多次遍历啊整个数据库,并返回匹配给定模式的键. &gt; SSCAN key cursor [MATCH pattern] [COUNT count] # 代替可能会阻塞服务器的 SMEMBERS 命令,遍历集合包含的各个元素. &gt; HSCAN key cursor [MATCH pattern] [COUNT count] # 代替肯能会阻塞服务器的 HGETALL 命令,遍历散列包含的各个键值对. &gt; ZSCAN key cursor [MATCH pattern] [COUNT count] # 代替可能会则色服务器的 ZRANGE 命令,遍历有序集合包含的各个元素. redis-cli 扫描 $ redis-cli --scan --pattern &apos;PATTERN&apos; 管理类 &gt; exists key &gt; del key1 &gt; type key1 &gt; randomkey # 随机返回一个 key &gt; rename OLDKEY NEWKEY &gt; renamenx OLDKEY NEWKEY # 超时时间 &gt; expire key second &gt; persist key # 消除设置的超时时间 &gt; expireat # 采用绝对超时. &gt; ttl key # 返回key 的剩余过期时间. &gt; pexpire key ms # 毫秒为时间单位 &gt; pttl key # 以毫秒返回生命周期. &gt; setnx key value # 仅当 key 不存在时,才set ,存在返回 0 ; nx , not exist . # 用来选举 master 或 做分布式锁, 所有 client 不断尝试使用 setnx key value 抢注 master, 成功的那位不断使用 expire 刷新他的过期时间. &gt; set key value nx|xx # nx : 仅在不存在 key 时 ,进行设置操作. # xx : 尽在存在 key 时, 进行设置操作. &gt; setget key value # 原子的设置key的值,并返回 key 的旧值. 配合 setnx 可以实现分布式锁. 开发设计规范 : key 设计 object-type:id:field.conn # 用 &quot;:&quot; 分割域, 用 &quot;.&quot; 做单词间的链接. ① 把表名转换为 key 前缀, ② 第二段放置用于区分 key 的字段, ③ 第三段放置主键 ④ 第四段写要存储的列名. 性能测试: $ redis-benchmark -q -r 100000 -n 100000 -c 50 $ redis-benchmark -t SET -c 100 -n 10000000 -r 10000000 -d 256 # 开100条线程(默认50), SET 1千万次(key在0-1千万间随机), key长21字节, value长256字节的数据. -r指的是使用随机key的范围. 数据库 : &gt; select db_index # 选择数据库 &gt; flushdb # 删除当前数据库中的所有 key, &gt; flushall # 删除所有的数据库. 执行 lua 脚本 $ redis-cli --eval name.lua PARAMETER 数据迁移 : 1. 将 key 从当前数据库移动到指定数据库 &gt; move key db_index 探测服务延迟 : $ redis-cli --latency # 显示的单位是milliseconds, 作为参考, 千兆网一跳一般延迟为0.16ms左右 查看统计信息 : redis:6379&gt; info 在cli下执行info. redis:6379&gt; info Replication 只看其中一部分. redis:6379&gt; config resetstat 重新统计 查看客户端 : &gt; client list # 列出所有连接 &gt; client kill 127.0.0.1:43501 # 杀死某个连接 查看日志 : 默认位于 redis/log 下 redis.log # redis 主日志 sentinel.log # sentinel 监控日志. 多实例配置 : taskset $ taskset -p REDIS_PID # 显示进行运行的 cpu, 结果 为 f $ taskset -p REDIS_PID -c 3 # 指定进程运行在某个特定 cpu 上, REDIS_PID 只会运行在 第4个 CPU 上. $ taskset -c 1 ./redis-server ./redis-6379.conf # 进程启动时,指定 cpu. 配置文件参数设置技巧 : Include 如果是多实例的话可以将公共的设置放在一个conf文件中, 然后引用即可： include /redis/conf/redis-common.conf 并发延迟检查 1. 检查 cpu 情况 $ mpstat -P ALL 1 2. 检查网络情况 : 可以在系统不繁忙或者临时下线前检测客户端和server或者proxy 的带宽： 1) 在 10.230.48.65 上使用 iperf -s 命令将 Iperf 启动为 server 模式: $ iperf –s 2) 启动客户端, 向IP为10.230.48.65的主机发出TCP测试, 并每2秒返回一次测试结果, 以Mbytes/sec为单位显示测试结果： $ iperf -c 10.230.48.65 -f M -i 2 3. 检查系统情况 : 探测服务延迟 监控正在请求执行的命令 获取慢查询 4. 检查连接数 $ redis-cli info Stats | grep total_connections_received # 如果该值不断升高, 则需要升级应用,改用连接池方式进行,因为频繁的关闭和创建连接,对redis 开销很大. 5. 检查持久化 RDB的时间： latest_fork_usec:936 上次导出rdb快照,持久化花费, 微秒. 检查是否有人使用了SAVE. 6. 检查命令执行情况 &gt; INFO commandstats 查看命令执行了多少次, 执行命令所耗费的毫秒数(每个命令的总时间和平均时间) 内存检查 1. 系统内存查看 2. 系统swap内存查看 3. info 查看内存 &gt; info memory used_memory:859192 # 数据结构的空间 used_memory_rss:7634944 # 实占空间 mem_fragmentation_ratio:8.89 # 前2者的比例, 1.N为佳,如果此值过大,说明redis的内存的碎片化严重,可以导出再导入一次. 4. dump.rdb 文件生成内存报告(rdb-tools) $ rdb -c memory ./dump.rdb &gt; redis_memory_report.csv $ sort -t, -k4nr redis_memory_report.csv 5. query 在线分析: redis-faina , redis 版本 &gt; 2.4 $ cd /opt/test $ git clone https://github.com/Instagram/redis-faina.git $ cd redis-faina/ $ redis-cli -p 6379 MONITOR | head -n 100 | ./redis-faina.py --redis-version=2.4 $ redis-cli MONITOR | head -n 5000 | ./redis-faina.py 6. 内存抽样分析 $ /redis/script/redis-sampler.rb 127.0.0.1 6379 0 10000 $ /redis/script/redis-audit.rb 127.0.0.1 6379 0 10000 7. 统计生产上比较大的 key $ redis-cli --bigkeys # 对redis中的key进行采样, 寻找较大的keys. 是用的是scan方式, 不用担心会阻塞redis很长时间不能处理其他的请求. 执行的结果可以用于分析redis的内存的只用状态, 每种类型key的平均大小. 8. rss 增加,内存碎片增加 可以选择时间进行redis服务器的重新启动, 并且注意在rss突然降低观察是否swap被使用, 以确定并非是因为swap而导致的rss降低. 测试方法 : 1. 模拟 oom $ redis-cli debug oom # redis 直接退出 2. 模拟宕机 $ redis-cli debug segfault 3. 模拟 hang $ redis-cli debug sleep 30 4. 快速产生测试数据 &gt; debug populate 1000 &gt; dbsize 5. 模拟 RDB load 情形 &gt; debug reload # save当前的rdb文件, 并清空当前数据库, 重新加载rdb, 加载与启动时加载类似, 加载过程中只能服务部分只读请求(比如info、ping等)： rdbSave(); emptyDb(); rdbLoad(); 6. 模拟 AOF 加载情形 &gt; debug loadaof # 清空当前数据库,重新从aof文件里加载数据库 emptyDb(); loadAppendOnlyFile();]]></content>
      <categories>
        <category>Middleware</category>
      </categories>
      <tags>
        <tag>redis</tag>
        <tag>缓存</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法复杂度速查表]]></title>
    <url>%2F2018%2F03%2F15%2F%E7%AE%97%E6%B3%95%E5%A4%8D%E6%9D%82%E5%BA%A6%E9%80%9F%E6%9F%A5%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[数据结构, 排序算法, 图操作, 堆操作, Big-O 算法复杂度速查表. 参考链接 图例 数据结构操作 数组排序算法 图操作复杂度 堆操作复杂度 Big-O 复杂度图表]]></content>
  </entry>
  <entry>
    <title><![CDATA[创业的本质]]></title>
    <url>%2F2018%2F03%2F15%2F%E6%9D%82%E8%AE%B0-%E5%88%9B%E4%B8%9A%E7%9A%84%E6%9C%AC%E8%B4%A8%2F</url>
    <content type="text"><![CDATA[我从没有听到过他们的一丝抱怨，我看到的永远是他们冲在一线解决客户问题的身影。这种乐观的心态，这种对荣辱毫不计较的精神，才是真正的创业者精神。 对创业初心的坚持，就是对使命的坚持。使命一定是解决了他人的问题，而不是解决了自己的问题。解决他人的问题就是「利他」，商业的本质不是交换，而是「利他」。一定是在利他的基础上，才能产生交换的需求。这就是为什么多数成功公司的企业文化里，都会强调「利他」的原因。需要有对应的企业文化，来催生对应的组织机制，以完成相应的商业目标，这是一环扣一环的。很多管理者从书上借鉴了一些做法，但并没有理解这其中的深层次联系。如果理解了企业的根本是客户价值，客户价值的本质是利他，从而鼓励所有员工成为「利他」的人，企业文化才不会流于形式和口号，才能真正成为基业长青的基石。 解决他人的问题就是「利他」，商业的本质不是交换，而是「利他」。一定是在利他的基础上，才能产生交换的需求。 我会狂妄的宣称要去颠覆世界，现在看来，世界根本不需要被颠覆，也很难在短时间内颠覆一个行业或一个市场。世界需要的是变得更美好，这是创业公司应该追求的。 Make the world a batter place. 不要用战术上的勤奋，掩盖战略上的懒惰。 创业不是一将功成万骨枯，创业是一个团队的成功。 来源]]></content>
      <categories>
        <category>杂记</category>
      </categories>
      <tags>
        <tag>创业</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[fpm 制作 rpm 包]]></title>
    <url>%2F2018%2F03%2F15%2FOS-Linux-fpm-%E6%89%93%E5%8C%85%2F</url>
    <content type="text"><![CDATA[使用 fpm 打包 rpm 包. 一. 使用 fpm 打包 rpm 包. 支持的 源类型包 ① dir : 将目录打包成所需要的类型, 可用于源码编译安装软件包 ② rpm : 对 rpm 包进行转换 ③ gem : 对 rubygem 包进行转换 ④ python : 将 python 模块打包成响应的类型 支持的 目标类型包 ① rpm : 转换为 rpm 包 ② deb : 转换为 deb 包 ③ solaris : 转换为 solaris 包 ④ puppet : 转换为 puppet 模块 FPM 安装 及 使用帮助 : FPM 基于 ruby , 需要首先安装 ruby 环境. ruby &gt; 1.8.5 $ yum install ruby rubygems ruby-devel gcc make libffi-devel -y $ yum install rpm-build -y # fpm 依赖 rpmbuild $ gem sources list $ gem sources --remove https://rubygems.org/ $ gem sources --add https://ruby.taobao.org $ gem install fpm # for centos7 $ gem install json -v 1.8.3 # for centos6 $ gem install fpm -v 1.3.3 # for centos6 $ fpm --help -s 指定源类型 -t 指定目标类型，即想要制作为什么包 -n 指定包的名字 -v 指定包的版本号 -C 指定打包的相对路径 Change directory to here before searching forfiles -d 指定依赖于哪些包 -f 第二次打包时目录下如果有同名安装包存在，则覆盖它 -p 输出的安装包的目录，不想放在当前目录下就需要指定 --post-install 软件包安装完成之后所要运行的脚本；同--after-install --pre-install 软件包安装完成之前所要运行的脚本；同--before-install --post-uninstall 软件包卸载完成之后所要运行的脚本；同--after-remove --pre-uninstall 软件包卸载完成之前所要运行的脚本；同--before-remove --description 示例: 定制 nginx rpm 包 $ yum -y install pcre-devel openssl-devel libzip $ useradd nginx -M -s /sbin/nologin $ tar xf nginx_1.10.tar.gz $ cd nginx_1.10 $ ./configure --prefix=/opt/nginx --user=nginx --group=nginx --with-http_ssl_module --with-http_stub_status_module $ make &amp;&amp; make install $ echo &quot;nginx-1.10.2&quot; &gt; /opt/nginx/version $ vim /tmp/nginx_rpm.sh #!/bin/bash useradd nginx -M -s /sbin/nologin $ fpm -s dir -t rpm -n nginx -v 1.10.2 -d &apos;pcre-devel,openssl-devel,libzip&apos; --post-install /tmp/nginx_rpm.sh -f /opt/nginx # 注意此处的 绝对路径. $ rpm -qpl nginx-1.6.2-1.x86_64.rpm # 查看软件包内容. 二. 有用的命令.$ yum provides *bin/prove provides Find what package provides the given value resolvedep Determine which package provides the given dependency $ tar -tvf new.tgz # 查看包的内容, 不解压包. $ make install DESTDIR=/tmp/installdir/ $ getent passwd root # 查看手否存在用户root root:x:0:0:root:/root:/bin/bash $ rpm -qp --scripts tengine-2.1.0-1.el6.x86_64.rpm # 查看 rpm 保存的脚本信息]]></content>
      <categories>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>rpm</tag>
        <tag>fpm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[AWS-S3]]></title>
    <url>%2F2018%2F03%2F15%2FAWS-S3%2F</url>
    <content type="text"><![CDATA[AWS 的数据存储服务 S3使用 S3 创建公司内部的文件服务器, 保存员工私人/共享文件, 并以类似 Dropbox 的方式双向同步. S3 介绍S3 是 AWS 最早发布的诸多服务之一, 用作可信存储. 可信 : 在指定年度内, 为对象提供 99.999999999% 的持久性和 高达 99.99% 的可用性. S3 提供如下特性 : 跨区域复制 : 只需要简单的配置，存储于S3中的数据会自动复制到选定的不同区域中。当你的数据对象的收集分散在不同的区域，而处理集中在某些区域时非常有用。 事件通知 : 当数据对象上传到 Amazon S3 中或从中删除的时候会发送事件通知。事件通知可使用 SQS 或 SNS 进行传送，也可以直接发送到 AWS Lambda 进行处理。 版本控制 : 数据对象可以启用版本控制，这样你就可以很方便地进行回滚。对于应用开发者来说，这是个特别有用的特性。 加密 S3的访问本身是支持 SSL（HTTPS）的，保证传输的安全，对于数据本身，你可以通过Server side encryption（AES256）来加密存储在S3的数据。 访问管理 通过 IAM/VPC 可以控制S3的访问粒度，你甚至可以控制一个bucket（S3对数据的管理单元，一个bucket类似于一组数据的根目录）里面的每个folder，甚至每个文件的访问权限。 可编程 可以使用 AWS SDK 进行客户端或者服务端的开发。 成本监控和控制 S3 有几项管理和控制成本的功能，包括管理成本分配的添加存储桶标签和接收账单警报的 Amazon Cloud Watch 集成。 灵活的存储选项 S3 Standard， Standard–Infrequent Access 选项可用于非频繁访问数据，存储的价格大概是 Standard 的 2/5。 Glacier : 用于存储冷数据（如N年前的Log），价格在 Standard 的 1/4，缺点是需要几个小时来恢复数据。 S3 操作方式consoleAWS CLI创建 bucket aws s3api create-bucket --bucket &lt;name&gt; 删除 bucket aws s3api delete-bucket --bucket &lt;name&gt; 像使用一般文件系统一样操作 S3 aws s3 ls aws s3 cp aws s3 rm 本地文件 与 S3 上文件同步 : aws s3 sync ./local_dir s3://my_bucket/my_dir AWS SDK使用的一般流程 : ① 创建 AWS Connection (需要 access key) ② 使用 connection 创建 S3 对象 ③ 使用 S3 API 进行各种操作. 使用 S3 的典型场景存储用户上传的文件, 如照片,视频等静态内容单做一个 k-v 存储, 承担简单的数据库服务功能数据备份静态网站的托管 : 可以对一个 bucket 使能 Web Hosting.参考]]></content>
      <categories>
        <category>AWS</category>
      </categories>
      <tags>
        <tag>S3</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[AWS-IAM权限控制]]></title>
    <url>%2F2018%2F03%2F15%2FAWS-IAM%E6%9D%83%E9%99%90%E6%8E%A7%E5%88%B6%2F</url>
    <content type="text"><![CDATA[AWS 的认证与权限管理服务 IAM 与 权限访问控制机制IAM , Identity and Access Management 基本概念ARN, Amazon Resource Name :在 AWS 里, 创建的任何资源都有其全局唯一的 ARN, 是访问控制可以到达的最小粒度. users, 用户groups, 组将用户添加到一个群组中, 可以自动获得这个群组的所有权限. roles, 角色没有任何访问凭证(密码或密钥), 他一般被赋予某个资源(包括用户), 那时起临时具有某些权限. 角色的密钥是动态创建的, 更新和失效都无需特别处理. permissions, 权限,权限可以赋给 用户,组, roles, 权限通过 policy document 描述, policy, 是描述权限的一段 JSON 文本. 示例如下{ &quot;Version&quot;: &quot;2012-10-17&quot;, &quot;Statement&quot;: [ { &quot;Effect&quot;: &quot;Allow&quot;, &quot;Action&quot;: &quot;*&quot;, &quot;Resource&quot;: &quot;*&quot; } ] } 用户或者群组只有添加了相关的 policy才会有响应的权限. policy 是 IAM 的核心内容. 是 JSON 格式来描述, 主要包含 Statement, 也就是 policy 拥有的权限的陈诉. 一言以蔽之: 谁在什么条件下能对哪些资源的哪些操作进行处理。 policy 的 PARCE 原则 : Principal : 谁 – 单独创建的 policy 是不需要指明 Principal 的, 当该 policy 被赋给用户,组或者 roles 时, principal 自动创建. Action : 那些操作 Resource : 那些资源 Condition : 什么条件 Effect : 如何处理 (Allow/Deny) 示例 { &quot;Version&quot;: &quot;2012-10-17&quot;, &quot;Statement&quot;: [ { &quot;Effect&quot;: &quot;Allow&quot;, &quot;Action&quot;: [ &quot;s3:Get*&quot;, &quot;s3:List*&quot; ], &quot;Resource&quot;: &quot;*&quot; } ] } 在这个policy里，Principal和Condition都没有出现。如果对资源的访问没有任何附加条件，是不需要Condition的；而这条policy的使用者是用户相关的principal（users, groups, roles），当其被添加到某个用户身上时，自然获得了principal的属性，所以这里不必指明，也不能指明。 Resource policy，它们不能单独创建，只能依附在某个资源之上（所以也叫inline policy），这时候，需要指明Principal。 示例如下 : { &quot;Version&quot;: &quot;2012-10-17&quot;, &quot;Statement&quot;: [ { &quot;Effect&quot;: &quot;Allow&quot;, &quot;Principal&quot;: &quot;*&quot;, &quot;Action&quot;: &quot;s3:GetObject&quot;, &quot;Resource&quot;: &quot;arn:aws:s3:::corp-fs-web-bucket/*&quot; } ] } 当我希望对一个S3 bucket使能Web hosting时，这个bucket里面的对象自然是要允许外界访问的，所以需要如下的inline policy： Condition 示例 &quot;Condition&quot;: { &quot;IPAddress&quot;: {&quot;aws:SourceIP&quot;: [&quot;10.0.0.0/8&quot;, &quot;4.4.4.4/32&quot;]}, &quot;StringEquals&quot;: {&quot;ec2:ResourceTag/department&quot;: &quot;dev&quot;} } &quot;Condition&quot;: { &quot;StringLike&quot;: { &quot;s3:prefix&quot;: [ &quot;tyrchen/*&quot; ] } } ** 在一条Condition下并列的若干个条件间是and的关系，这里IPAddress和StringEquals这两个条件必须同时成立； ** 在某个条件内部则是or的关系，这里10.0.0.0/8和4.4.4.4/32任意一个源IP都可以访问。 Policy 执行规则 : 默认情况下，一切资源的一切行为的访问都是Deny 如果在policy里显式Deny，则最终的结果是Deny 否则，如果在policy里是Allow，则最终结果是Allow 否则，最终结果是Deny]]></content>
      <categories>
        <category>AWS</category>
      </categories>
      <tags>
        <tag>IAM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Lua学习笔记]]></title>
    <url>%2F2018%2F03%2F15%2FLua%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[Lua 是一种高性能, 解释型, 面向对象的语句, 广泛用于各种项目的内嵌语言, 如 redis, nginx, scrapy, 愤怒的小鸟, 魔兽世界等等. 本文主要介绍 Lua 的语法. 1. 数据类型lua 是一个动态类型语言,一个变量可以存储类型的值.Lua 常用数据类型: 空(nil): 空类型只包含一个值,即nil . nil表示空, 没有赋值的变量或标的字段都是 nil. 布尔(boolean): 布尔类型包含 True 和 False 两个值. 数字(number): 整数合浮点数是都是使用数字类型存储. 字符串(string): 字符串类型可以存储字符串,且与Redis的键值一样都是二进制安全的.字符串可以使用单引号或双引号表示,两个符号是相同的. 字符串可以包含转义字符,如 ‘\n’,’\r’ 等. 表(table): 表类型是Lua 语言中唯一的数据结构,既可以当数组,又可以当字典,十分灵活. 函数(function): 函数是Lua中的一等值(first-class value),可以存储在变量中,作为函数的参数或返回结果. 2. 变量Lua 变量分为全局变量和局部变量. 全局变量无需声明就可以直接使用,默认值是 nil . &gt; print(b) a = 1 -- 为全局变量a赋值 a = nil -- 删除全局变量的方法是将其复制为 nil . 全局变量没有声明与未声明之分,只有非 nil 和 nil 的区别. print(b) -- 无需声明即可使用，默认值是nil 声明局部变量的方式为 “local 变量名” : local c --声明一个局部变量c，默认值是nil local d = 1 --声明一个局部变量d并赋值为1 local e, f --可以同时声明多个局部变量 * 局部变量的作用域为从声明开始到所在层的语句块的结尾. 声明一个存储函数的局部变量的方法为 : local say_hi = function () print &apos;hi&apos; end 变量名必须是非数字开头,只能包含字母,数字和下划线,区分大小写. 变量名不能与Lua的保留关键字相同, 保留关键字如下: and break do else elseif end false for function if in local nil not or repeat return then true until while 3. 注释 单行: -- 开始, 到行尾结束. 多行: --[[ ... ]] . 4. 赋值多重赋值 : local a, b = 1, 2 -- a的值是1，b的值是2 local c, d = 1, 2, 3 -- c的值是1，d的值是2，3被舍弃了 local e, f = 1 -- e的值是1，f的值是nil 在执行多重赋值时,Lua会先计算所有表达式的值,比如: local a = {1, 2, 3} local i = 1 i, a[i] = i + 1, 5 -- i = 2 ; a = {5,2,3} , lua 索引从 1 开始. lua 中的函数也可以返回多个值 5. 操作符5.1 数学操作符 :常见的+、-、*、/、%（取模）、-（一元操作符，取负）和幂运算符号^。 数学操作符的操作数如果是字符串,则会自动转换为数字. print(&apos;1&apos; + 1) -- 2 print(&apos;10&apos; * 2) -- 20 5.2 比较操作符 : == : 比较两个操作数的类型和值是否相等 ~= : 与 == 结果相反 &lt;,&gt;,&lt;=,&gt;= : 大于,小于,小于等于,大于等于. 比较操作符的结果一定是布尔类型 ; 比较操作符,不会对两边的操作数进行自动类型转换. 5.3 逻辑操作符 : not : 根据操作数的真和假返回false 和 true and : a and b, 如果a 是真,则返回 b , 否则返回 a . or : a or b , 如果a 是假,则返回 a , 否则返回 b . 只要操作数不是 nil 或 false ,逻辑操作符都认为操作数是真. 特别注意 0 或 空字符串也被当做真. Lua 逻辑操作符支持短路，也就是说对于 false and foo() ，lua 不会调用foo函数，or 类似。 5.4 连接操作符.... 用来连接两个字符串. 连接操作符会自动把数字类型的抓换成字符串类型. 5.5 取长度操作符.是lua5.1 新增的操作符, # ,用来获取字符串或表的长度. &gt; print(#&apos;hello&apos;) -- 5 5.6 运算符的优先级:^ not # -(一元) * / % + - .. &lt; &gt; &lt;= &gt;= ~= == and or 6. if 语句语法 : if 条件表达式 then 语句块 elseif 条件表达式 then 语句块 else 语句块 end 注意 : Lua 中只有 nil 和 false 才是假, 其余值,包括0 和空字符串,都被认为是真值. Lua 每个语句都可以 ; 结尾 ,但是一般来说编写 Lua 是会省略 ; , Lua 并不强制要求缩进,所有语句也可以写在一行中, 但为了增强可读性,建议在注意缩进. &gt; a = 1 b = 2 if a then b = 3 else b = 4 end 7. 循环语句7.1 while 循环while 条件表达式 do 语句块 end 7.2 repeat 循环repeat 语句块 until 条件表达式 7.3 for 循环形式一 :for 循环中的 i 是局部变量, 作用域为 for 循环体内. 虽然没有使用 local 声明,但它不是全局变量. for 变量=初值,终值,步长 do -- 步长可省略,默认为 1 语句块 end 示例 # 计算 1 ~ 100 之和 local sum = 0 for i = 1 ,100 do sum = sum + 1 end 形式二 :for 变量1 ,变量2, ... , 变量N in 迭代器 do 语句块 end 8. 表类型表是Lua中唯一的数据结构,可以理解为关联数组, 任何类型的值(除了空类型)都可以作为表的索引. a = {} --将变量a赋值为一个空表 a[&apos;field&apos;] = &apos;value&apos; --将field字段赋值value print(a.field) --打印内容为&apos;value&apos;，a.field是a[&apos;field&apos;]的语法糖。 people = { --也可以这样定义 name = &apos;tom&apos;, age = 29 } 当索引为整数的时候表和传统的数组一样，例如： a = {} a[1] = &apos;Tom&apos; a[2] = &apos;Jeff&apos; 可以写成下面这样： a = {&apos;Tom&apos;, &apos;Jeff&apos;} print(a[1]) --打印的内容为&apos;Tom&apos; 可以使用通用形式的for语句遍历数组,例如: for index,value in ipairs(a) do -- index 迭代数组a 的索引 ; value 迭代数组a 的值. print(index) print(value) end -- ipairs 是Lua 内置的函数,实现类似迭代器的功能. 数字形式的for语句 for i=1,#a do print(i) print(a[i]) end pair : 迭代器,用来遍历非数组的表值. person = { name = &apos;Tom&apos;, age = 29 } for index,value in pairs(person) do print(index) print(value) end pairs 与 ipairs 的区别在于前者会遍历所有值不为 nil 的索引, 而后者只会从索引 1 开始递增遍历到最后一个值不为 nil 的整数索引. 9. 函数一般形式: function(参数列表) 函数体 end 可以将函数赋值给一个局部变量, 比如: local square = function(num) return num*num end ** 因为在赋值前声明了局部变量 square, 所以可以在函数内部引用自身(实现递归). 函数参数 : 如果实参的个数小于形参的个数,则没有匹配到的形参的值为 nil . 相对应的,如果实参的个数大于形参的个数,则多出的实参会被忽略. 如果希望捕获多出的参数(即实现可变参数个数),可以让最后一个形参为 ... . local function square(...) local argv = {...} for i = 1,#argv do argv[i] = argv[i] * argv[i] end return unpack(argv) -- unpack 函数用来返回 表 中的元素. 相当于return argv[1], argv[2], argv[3] end a,b,c = square(1,2,3) print(a) -- 1 print(b) -- 4 print(c) -- 9 在 Lua 中, return 和 break (用于跳出循环) 语句必须是语句块中的最后一条语句, 简单的说在这两条语句之后只能是 end,else 或 until 三者之一. 如果希望在语句块中间使用这两条语句,可以认为的使用 do 和 end 将其包围. 10. 标准库 http://www.lua.org/manual/5.1/manual.html#5Lua 的标准库中提供了很多使用的函数, 比如 ipairs,pairs,tonumber,tostring,unpack 都属于标准库中的Base库. Redis 支持大部分Lua标准库,如下所示: 库名 说明 Base 一些基础函数 String 用于字符串操作的函数 Table 用于表操作的函数 Math 数学计算函数 Debug 调试函数 10.1 String库 : 可以通过字符串类型的变量以面向对象的形式访问, 如 string.len(string_var) 可以写成 string_var:len() 获取字符串长度 : string.len(string) 作用与操作符 “#” 类似 &gt; print(string.len(&apos;hello&apos;)) -- 5 &gt; print(#&apos;hello&apos;) -- 5 转换大小写 string.upper(string) string.lower(string) 获取子字符串 string.sub() 可以获取一个字符串从索引 start 开始到 end 结束的子字符串,索引从1 开始. 索引也可以是负数, -1 代表最后一个元素 . string.sub(string start[,end ]) -- end 默认为 -1. &gt; print(string.sub(&apos;hello&apos;,1)) -- hello &gt; print(string.sub(&apos;hello&apos;,2)) -- ello &gt; print(string.sub(&apos;hello&apos;,2,-2)) -- ell 10.2 Table库 : 其中大部分函数都需要表的形式是数组形式. 将数组转换为字符串 table.concat(table [,sep [,i [,j]]]) # sep : 以 sep 指定的参数分割, 默认为空. # i , j : 用来限制要转换的表元素的索引范围. 默认分别为 1 和 表的长度. 不支持负索引. print(table.concat({1,2,3})) –123print(table.concat({1,2,3},’,’,2)) –2,3print(table.concat({1,2,3},’,’,2,2)) –2 向数组中插入元素 table.insert(table ,[pos,] value) # 在指定索引位置 pos 插入元素 value, 并将后面的元素顺序后移. 默认 pos 值是数组长度加 1 , 即在数组尾部插入. &gt; a = {1,2,4} &gt; table.insert(a,3,3) # {1,2,3,4} &gt; table.insert(a,5) # {1,2,3,4,5} &gt; print(table.concat(a,&apos;,&apos;)) 1,2,3,4,5 从数组中弹出一个元素 table.remove(table,[,pos]) # 从指定的索引删除一个元素,并将后面的元素前移,返回删除元素值. 默认 pos 的值是数组的长度,即从数组尾部弹出一个元素. &gt; table.remove(a) --{1,2,3,4} &gt; table.remove(a,1) --{2,3,4} &gt; print(table.caoncat(a,&apos;,&apos;)) 2,3,4 10.3 Math库 : 提供常用的数学运算函数, 如果参数是字符串会自动尝试转换成数字.math.abs(x) # 绝对值 math.sin(x) # 三角函数sin math.cos(x) # 三角函数cos math.tan(x) # 三角函数tan math.ceil(x) # 进一取整, 1.2 取整后是 2 math.floor(x) # 向下取整, 1.8 取整后是 1 math.max(x,...) # 获得参数中的最大的值 math.min(x,...) # 获取参数中的最小的值 math.pow(x,y) # 获取 xy 的值 math.sqrt(x) # 获取 x 的平方根 math.random([m,[,n]]) # 生成随机数,没有参数 返回 [0,1]的实数, 参数 m 返回范围在 [1,m] 的整数, 同时提供 m n 返回范围在 [m,n] 的整数. math.randomseed(x) # 设置随机数种子, 同一种子生成的随机数相同. 11. 其他库Redis 还通过 cjson库 和 cmsgpack库 提供了对 JSON 和 MessagePack的支持. Redis自动加载了这两个库,在脚本中可以分别通过 cjson 和 cmsgpack 两个全局变量来访问对应的库. local people = { name = &apos;Tom&apos;, age = 29 } -- 使用 cjson 序列化成字符串 local json_people_str = cjson.encode(people) -- 使用 cmsgpack 序列化成字符串 local msgpack_people_str = cmsgpack.pack(people) -- 使用 cjson 将序列化后的字符串还原成表 local json_people_obj = cjson.decode(people) print(json_people_obj.name) -- 使用 cmshpack 将序列化后的字符串还原成表 local msgpack_people_obj = cmsgpack.unpack(people) print(msgpack_people_obj.name)]]></content>
      <categories>
        <category>Lua</category>
      </categories>
      <tags>
        <tag>lua</tag>
        <tag>redis</tag>
        <tag>scrapy</tag>
        <tag>nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scrapy]]></title>
    <url>%2F2018%2F03%2F15%2FScrapy%2F</url>
    <content type="text"><![CDATA[摘要 一. Scrapy 架构组件 P13 组件 描述 类型 ENGINE 引擎, SCHEDULER 调度器, DOWNLOADER 下载器, SPIDER 爬虫, MIDDLEWARE 中间件, ITEM PIPELINE 数据管道, 框架中的数据流 P14 对象 描述 REQUEST Scrapy 中的 HTTP 请求对象 RESPONSE Scrapy 中的 HTTP 响应对象 ITEM 从页面中爬去的一项数据 Request(url[, callback, method=&#39;GET&#39;, headers, body, cookies, meta, encoding=&#39;utf-8&#39;, priority=0, dont_filter=False, errback]) Response 对象 HtmlResponse TextResponse XmlResponse 二. Spiderstart_urls.parse / start_requests.callback --&gt; REQUEST --&gt; DOWNLOADER --&gt; RESPONSE --&gt; Selector/SelectorList[xpath/css][extract/re/extract_first/re_first] 三. Selector 提取数据 构造 Selector html = &quot; &lt;html lang=&quot;en&quot;&gt; &lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;Title&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h1&gt;Hello world!&lt;/h1&gt; &lt;p&gt;This is a line.&lt;/p&gt; &lt;/body&gt; &lt;/html&gt;&quot; # 使用文本字符串构造 from scrapy.selector import Selector selector = Selector(text=html) # 使用 Response 对象构造 from scrapy.http import HtmlResponse response = HtmlResponse(url=&quot;http://www.example.com&quot;, body=body, encoding=&apos;utf-8&apos;) selector = Selector(response=response) RESPONSE 内置 selector : 在第一次访问一个 Response 对象的 selector 属性时, Response 对象内部会以自身为参数, 自动创建 Selector 对象, 并将该 Selector 对象缓存, 以便下次使用. class TextResponse(Response): def __init__(self, *args, **kwargs): ... self._cached_selector = None ... @property def selector(self): from scrapy.selector import Selector if self._cached_selector is None: self._cached_selector = Selector(self) return self._cached_selector ... def xpath(self, query, **kwargs): return self.selector.xpath(query, **kwargs) def css(self, query): return self.selector.css(query) 选中数据 XPATH/CSS 提取数据 extract()/re()/extract_first()/re_first() 四. Item 封装数据Scrapy 提供一下两个类, 用户可以使用它们自定义数据类(如书籍信息), 封装爬取到的数据. Item 基类 : 自定义数据类的基类, 支持字典接口(访问 自定义数据类 中的字段与访问字典类似, 支持 get() 方法). Field 类 : 用来描述自定义数据类包含哪些字段(如name, price 等). 1. 自定义数据类, 只需继承 Item, 并创建一系列 Field 对象的类属性即可, 类似于 ORM 创建的 Model.from scrapy import Item, Field class BookItem(Item): name = Field() price = Field() class ForeignBookItem(BookItem): &quot;&quot;&quot; 扩展 BookItem 类 &quot;&quot;&quot; translator = Field() 2. Field 元数据class BookItem(Item): name = Field(a=123, b=[1,2,3]) price = Field(a=lambda x: x+2) # Field 是 Python 字典的子类, 可以通过键获取 Field 对象中的元数据. b = BookItem(name=100, price=101) b[&apos;name&apos;] # 100 b[&apos;price&apos;] # 101 b.fields # {&apos;name&apos;: {&apos;a&apos;: 123, &apos;b&apos;: [1, 2, 3]}, &apos;price&apos;: {&apos;a&apos;: &lt;function __main__.&lt;lambda&gt;&gt;}} 代码示例class BookItem(Item): ... # 当 authors 是一个列表而不是一个字符串时, 串行化为一个字符串. authors = Field(serializer=lambda x: &quot;|&quot;.join(x)) ... 以上例子中, 元数据键 serializer 时 CSVItemExportr 规定好的, 他会用该键获取元数据, 即一个串行化函数对象, 并使用这个串行化函数将 authors 字符串行化成一个字符串, 具体代码参考 scrapy/exporters.py 文件. 五. Item Pipeline 处理数据在 Scrapy 中, Item Pipeline 是处理数据的组件, 一个 Item Pipeline 就是一个包含特定接口的类, 通常只负责一种功能的数据处理, 在一个项目中可以同时启动多个 Item Pipeline, 他们按指定次序级联起来, 形成一条数据处理流水线. Item Pipeline 典型应用 数据请求 验证数据有效性 过滤重复数据 将数据存入数据库. 1. 编写 pipelineclass PriceConverterPipeline(object): # 英镑兑换人民币汇率 exchange_rate = 8.5309 def process_item(self, item, spider): # 提取 item 的 price 字段(如 £ 53.74), 去掉 £ 符号, 转换为 float 类型, 乘以汇率 price = float(item[&quot;price&quot;][1:]) * self.exchange_rate # 保留两位小数赋值, item[&quot;price&quot;] = &quot;¥ %.2f&quot; % price return item 一个 Item Pipeline 不需要继承特定基类, 只需要实现某些特定的方法. process_item(self, item, spider) : 该方法必须实现. 该方法用来处理每一项由 Spider 爬取到的数据, 其中 item 为爬取到的一项数据, Spider 为爬取此项数据的 Spider 对象. process_item 是 Item Pipeline 的核心, process_item 返回的一项数据, 会传递给下一级 Item Pipeline(如果有) 继续处理. 如果 process_item 在处理某项 item 时抛出 DropItem(scrapy.exceptions.DropItem) 异常, 该项 item 便会被抛弃, 不会传递到下一级 Item Pipeline, 也不会导出到文件. 通常, 在检测到无效数据, 或者希望过滤的数据时, 抛出该异常. open_spider(self, spider) Spider 打开时(处理数据前), 回调该方法, 通常该方法用于在 开始处理数据之前完成某些初始化的工作, 如连接数据库. close_spider(self, spider) Spider 关闭时(处理数据后), 回调该方法, 通常该方法用于在 处理完所有数据之后完成某些清理工作, 如关闭数据库链接. from_crawler(cls, crawler) 创建 Item Pipeline 对象时回调该类方法. 通常, 在该方法中通过 crawler.settings 读取配置, 根据配置创建 Item Pipeline 对象. 如果一个 Item Pipeline 定义了 from_crawler 方法, Scrapy 就会调用该方法来创建 Item Pipeline 对象. 该方法的两个参数: cls : Item Pipeline 类的对象, crawler : crawler 是 Scrapy 中的一个核心对象, 可以通过 crawler.settings 属性访问配置文件. 2. 启用 Item Pipeline.$ cat settings.py ITEM_PIPELINES = { &apos;example.pipelines.PriceConverterPipeline&apos;: 300, } ITEM_PIPELINES 是一个字典, 其中每一项 Item Pipeline 类的导入路径, 值是一个 0~1000 的数字, 同时启用多个 Item Pipeline 时, Scrapy 根据这些数值决定各 Item Pipeline 处理数据的先后次序, 数值小的优先级高. 3. 代码示例3.1 Item Pipeline : 过滤重复数据class DuplicatesPipeline(object): def __init__(self): self.book_set = set() def process_item(self, item, spider): name = item[&quot;name&quot;] if name in self.book_set: raise DropItem(&quot;Duplicate book found: %s&quot; % item) self.book_set.add(name) return item 3.2 Item Pipeline : 将数据存储 MongoDB# pipelines.py class MongoDBPipeline(object): @classmethod def from_crawler(cls, crawler): &quot;&quot;&quot; 使用 settings.py 配置文件, 配置 MongoDB 数据库, 而不是硬编码 如果一个 Item Pipeline 定义了 from_crawler 方法, Scrapy 就会调用该方法来创建 Item Pipeline 对象. &quot;&quot;&quot; cls.DB_URI = crawler.settings.get(&quot;MONGO_DB_URI&quot;, &quot;mongodb://localhost:27017/&quot;) cls.DB_NAME = crawler.settings.get(&quot;MONGO_DB_NBAME&quot;, &quot;scrapy_data&quot;) return cls() def open_spider(self, spider): &quot;&quot;&quot;在开始处理数据之前, 链接数据库&quot;&quot;&quot; self.client = pymongo.MongoClient(self.DB_URI) self.db = self.client[self.DB_NAME] def close_spider(self, spider): &quot;&quot;&quot;数据处理完成之后, 关闭数据库链接&quot;&quot;&quot; self.client.close() def process_item(self, item, spider): &quot;&quot;&quot;将 item 数据 写入 MongoDB&quot;&quot;&quot; collection = self.db[spider.name] post = dict(item) if isinstance(item, Item) else item collection.insert_one(post) return item # settings.py MONGO_DB_URI = &quot;mongodb://192.168.1.1:27017/&quot; MONGO_DB_NBAME = &quot;my_scrapy_data&quot; ITEM_PIPELINES = { &quot;toscrapt_book.pipelines.MongoDBPipeline&quot;: 403, } 3.3 Item Pipeline : 将数据存储 Mysql版本一 : # pipelines.py class MySQLPipeline(object): def open_spider(self, spider): db = spider.settings.get(&quot;MYSQL_DB_NAME&quot;, &quot;scrapy_data&quot;) host = spider.settings.get(&quot;MYSQL_HOST&quot;, &quot;locahost&quot;) port = spider.settings.get(&quot;MYSQL_PORT&quot;, &quot;3306&quot;) user = spider.settings.get(&quot;MYSQL_USER&quot;, &quot;root&quot;) password = spider.settings.get(&quot;MYSQL_PASSWORD&quot;, &quot;123456&quot;) self.db_conn = MySQLdb.connect(host=host, port=port, db=db, user=user, passwd=password, charset=&quot;utf-8&quot;) self.db_cur = self.db_conn.cursor() def closer_spider(self, spider): self.db_conn.commit() self.db_conn.close() def procecss_item(self, item, spider): self.insert_db(item) return item def insert_db(self, item): values = ( item[&quot;name&quot;], item[&quot;price&quot;], item[&quot;rating&quot;], item[&quot;number&quot;] ) sql = &quot;INSERT INTO books VALUES (%s, %s, %s, %s, )&quot; self.db_cur.execute(sql, values) # settings.py MYSQL_DB_NAME = &quot;scrapy_data&quot; MYSQL_HOST = &quot;locahost&quot; MYSQL_PORT = &quot;3306&quot; MYSQL_USER = &quot;root&quot; MYSQL_PASSWORD = &quot;123456&quot; ITEM_PIPELINES = { &quot;toscrapt_book.pipelines.MySQLPipeline&quot;: 403, } 版本二: Scrapy 框架本身使用 Twisted 编写, Twisted 是一个事件驱动型的异步网络框架, 鼓励用户编写异步代码, Twisted 中提供了以异步方式多线程访问数据库的模块 adbapi, 使用该模块可以显著提高程序访问数据库的效率. # pipelines.py from twisted.enterprise import adbapi class MySQLAsyncPipeline(object): def open_spider(self, spider): db = spider.settings.get(&quot;MYSQL_DB_NAME&quot;, &quot;scrapy_data&quot;) host = spider.settings.get(&quot;MYSQL_HOST&quot;, &quot;locahost&quot;) port = spider.settings.get(&quot;MYSQL_PORT&quot;, &quot;3306&quot;) user = spider.settings.get(&quot;MYSQL_USER&quot;, &quot;root&quot;) password = spider.settings.get(&quot;MYSQL_PASSWORD&quot;, &quot;123456&quot;) # adbapi.ConnectionPool 可以创建一个数据库连接池对象, 其中包含多个链接对象, 每个链接对象在单独的线程中工作. # adbapi 只提供异步访问数据库的框架, 其内部依然使用 MySQLdb, sqlite3 这样的库访问数据库. self.dbpool = adbapi.ConnectionPool(&quot;MySQLdb&quot;, host=host, database=db, user=user, password=password, charset=&quot;utf-8&quot;) def close_spider(self, spider): self.dbpool.close() def process_item(self, item, spider): # 以异步方式调用 insert_db 方法, 执行完 insert_db 方法之后, 链接对象自动调用 commit 方法. self.dbpool.runInteraction(self.insert_db, item) return item def insert_db(self, tx, item): # tx 是一个 Transaction 对象, 其接口与 Cursor 对象类似, 可以调用 execute 执行 SQL 语句. values = ( item[&quot;name&quot;], item[&quot;price&quot;], item[&quot;rating&quot;], item[&quot;number&quot;] ) sql = &quot;INSERT INTO books VALUES (%s, %s, %s, %s, )&quot; tx.execute(sql, values) # settings.py MYSQL_DB_NAME = &quot;scrapy_data&quot; MYSQL_HOST = &quot;locahost&quot; MYSQL_PORT = &quot;3306&quot; MYSQL_USER = &quot;root&quot; MYSQL_PASSWORD = &quot;123456&quot; ITEM_PIPELINES = { &quot;toscrapt_book.pipelines.MySQLAsyncPipeline&quot;: 403, } 3.4 Item Pipeline : 将数据存储 Redis# pipelines.py import redis from scrapy import Item class RedisPipeline(object): def open_spider(self, spider): db_host = spider.settings.get(&quot;REDIS_HOST&quot;, &quot;localhost&quot;) db_port = spider.settings.get(&quot;REDIS_PORT&quot;, &quot;6379&quot;) db_index = spider.settings.get(&quot;REDIS_DB_INDEX&quot;, 0) self.db_conn = redis.StrictRedis(host=db_host, port=db_port, db=db_index) self.item_i = 0 def close_spider(self, spider): self.db_conn.connection_pool.disconnect() def process_item(self, item, spider): self.insert_db(item) return item def insert_db(self, item): if isinstance(item, Item): item = dict(item) self.item_i += 1 self.db_conn.hmset(&quot;book:%s&quot; % self.item_i, item) # settings.py &quot;REDIS_HOST&quot; = &quot;localhost&quot; &quot;REDIS_PORT&quot; = 6379 &quot;REDIS_DB_INDEX&quot; = 0 ITEM_PIPELINES = { &quot;toscrapt_book.pipelines.RedisPipeline&quot;: 403, } 六. LinkExtractor 提取链接提取页面中的链接, 有两种方法: selector : 提取少量链接时, 或提取规则比较简单. LinkExtractor : 专门用于提取链接的类 LinkExtractor. 1. 使用示例:# url from selector # next_url = response.css(&quot;ul.pager li.next a::attr(href)&quot;).extract_first() # if next_url: # next_url = response.urljoin(next_url) # yield scrapy.Request(next_url, callback=self.parse) # url from LinkeExtractor le = LinkExtractor(restrict_css=&quot;ul.pager li.next&quot;) links = le.extract_links(response) # 返回一个列表, 其中每一个元素都是一个 Link 对象, Link 对象的 url 属性便是链接页面的 绝对 url 地址. if links: next_url = links[0].url yield scrapy.Request(next_url, callback=self.parse) 2. 链接提取规则:LinkExtractor 构造器的所有参数都有默认值, 如果构造对象时, 不传递任何参数(使用默认值), 则提取页面中的所有链接. allow : 接受一个正则表达式或一个正则表达式列表, 提取绝对 url 与 正则表达式匹配的链接, 如果该参数为空(默认), 则提取全部链接. &gt;&gt;&gt; pattern = &apos;/intro/.+\.html$&apos; &gt;&gt;&gt; le = LinkExtractor(allow=pattern) &gt;&gt;&gt; links = le.extract_links(response) deny : 接受一个正则表达式或一个正则表达式列表, 与 allow 相反, 排除绝对 url 与正则表示匹配的链接. &gt;&gt;&gt; pattern = &apos;^http://example.com&apos; &gt;&gt;&gt; le = LinkExtractor(deny=pattern) &gt;&gt;&gt; links = le.extract_links(response) allow_domains : 接受一个域名或一个域名列表, 提取到指定域的链接. &gt;&gt;&gt; domains = [&quot;github.com&quot;, &quot;stackoverflow.com&quot;] &gt;&gt;&gt; le = LinkExtractor(allow_domains=domains) &gt;&gt;&gt; links = le.extract_links(response) deny_domains : 接受一个域名或一个域名列表, 与 allow_domains 相反, 排除到指定域的链接. &gt;&gt;&gt; domains = [&quot;github.com&quot;, &quot;stackoverflow.com&quot;] &gt;&gt;&gt; le = LinkExtractor(deny_domains=domains) &gt;&gt;&gt; links = le.extract_links(response) restrict_xpaths : 接受一个 Xpath 表达式或一个 Xpath 表达式列表, 提取 XPath 表达式选中区域下的的链接. &gt;&gt;&gt; le = LinkExtractor(restrict_xpaths=&quot;//div[@id=&apos;top&apos;]&quot;) &gt;&gt;&gt; links = le.extract_links(response) restrict_css : 接受一个 CSS 选择器或一个 CSS 选择器列表, 提取 CSS 选择器选中区域下的的链接. &gt;&gt;&gt; le = LinkExtractor(restrict_css=&quot;div#bottom&quot;) &gt;&gt;&gt; links = le.extract_links(response) tags : 接受一个标签(字符串)或一个标签列表, 提取指定标签内的链接, 默认为 [&quot;a&quot;, &quot;area&quot;] attrs : 接受一个属性(字符串)或一个属性列表, 提取指定属性内的链接, 默认为 [&quot;href&quot;] # &lt;script type=&quot;text/javascript&quot; src=&quot;/js/app.js&quot; /&gt; &gt;&gt;&gt; le = LinkExtractor(tags=&quot;script&quot;, attrs=&quot;src&quot;) &gt;&gt;&gt; links = le.extract_links(response) process_value : 接受一个形如 func(value) 的回调函数. 如果传递了该参数, LinkExtractor 将调用该回调函数对提取的每一个链接(如 a 的 href) 进行处理, 回调函数正常情况下应该返回一个字符串(处理结果), 想要抛弃所处理的连接时, 返回 None. # &lt;a href=&quot;javascript:goToPage(&apos;/doc.html&apos;); return false&quot;&gt;文档&lt;/a&gt; &gt;&gt;&gt; import re &gt;&gt;&gt; def process(value): m = re.search(&quot;javascript:goToPage\(&apos;(.*?)&apos;&quot;, value) # 如果匹配, 就提取其中 url 并返回, 不匹配则返回原值. if m: value = m.group() return value &gt;&gt;&gt; le = LinkExtractor(process_value=process) &gt;&gt;&gt; links = le.extract_links(response) 七. Exporter 导出数据在 Scrapy 中, 负责导出数据的组件被称为 Exporter(导出器), Scrapy 内部实现了多个 Exporter , 每个 Exporter 实现一种数据格式的导出. 支持的数据导出格式如下(括号内为对应的 Exporter): JSON (JsonItemExporter) JSON lines (JsonLinesItemExporter) CSV (CsvItemExporter) XML (XmlItemExporter) Pickle (PickleItemExporter) Marshal (MarshalItemExporter) Scrapy 爬虫会议 -t 参数中的数据格式字符串(如 csv, json, xml) 为键, 在配置字典 FEED_EXPORTERS 中搜索 Exporter, FEED_EXPORTERS 的内容由以下两个字典的内容合并而成: 默认配置文件中的 FEED_EXPORTERS_BASE, 为 Scrapy 内部支持的导出数据格式, 位于 scrapy.settings.default_settings 用户配置文件中的 FEED_EXPORTERS, 为用户自定义的导出数据格式, 在配置文件 settings.py 中. FEED_EXPORTERS = {&quot;excel&quot;: &quot;my_propject.my_exporters.ExcelItemExporter&quot;} 1. 导出数据方式: 命令行参数 $ scrapy crawl CRAWLER -t FORMAT -o /path/to/save.file $ scrapy crawl books -t csv -o books.data $ scrapy crawl books -t xml -o books.data $ scrapy crawl books -t json -o books.data $ scrapy craw books -o books.csv # Scrapy 可以通过文件后缀名, 推断出文件格式, 从而省去 -t 参数. 如 `-o books.json` -o /path/to/file 支持变量: 如 scrapy crawl books -o &#39;export_data/%(name)s/%(time)s.csv&#39; %(name)s : Spider 的名字 %(time)s : 文件创建时间 通过配置文件指定. 常用选项如下: FEED_URL : 导出文件路径 FEED_URL = &apos;export_data/%(name)s/%(csv)s.data&apos; FEED_FORMAT : 导出数据格式 FEED_FORMAT = &apos;csv&apos; FEED_EXPORT_ENCODING : 导出文件编码, 默认 json 使用数字编码, 其他使用 utf-8 编码 FEED_EXPORT_ENCODING = &quot;gbk&quot; FEED_EXPORT_FIELDS : 导出数据包含的字段(默认情况下, 导出所有字段), 并指定导出顺序. FEED_EXPORT_FIELDS = [&quot;name&quot;, &quot;author&quot;, &quot;price&quot;] FEED_EXPORTERS : 用户自定义的 Exporter 字典, 添加新的导出数据格式时使用. FEED_EXPORTERS = {&quot;excel&quot;: &quot;my_project.my_exporters.ExcelItemExporter&quot;} 2. 自定义数据导出格式import six from scrapy.utils.serialize import ScrapyJSONEncoder import xlwt class BaseItemExporter(object): def __init__(self, **kwargs): self._configure(kwargs) def _configure(self, options, dont_fail=False): self.encoding = options.pop(&quot;encoding&quot;, None) self.fields_to_export = options.pop(&quot;field_to_export&quot;, None) self.export_empty_fields = options.pop(&quot;export_empty_fields&quot;, False) if not dont_fail and options: raise TypeError(&quot;Unexpected options: %s&quot; % &apos;,&apos;.join(options.keys())) def export_item(self, item): &quot;&quot;&quot; 负责导出爬去到的每一项数据, 参数 item 为一项爬取到的数据, 每个子类必须实现该方法. :param item: :return: &quot;&quot;&quot; raise NotImplementedError def serialize_field(self, field, name, value): serializer = field.get(&quot;serializer&quot;, lambda x: x) return serializer(value) def start_exporting(self): &quot;&quot;&quot; 在导出开始时被调用, 可在该方法中执行某些初始化操作. :return: &quot;&quot;&quot; pass def finish_exporting(self): &quot;&quot;&quot; 在导出完成时被调用, 可在该方法中执行某些清理工作. :return: &quot;&quot;&quot; pass def _get_serialized_field(self, item, default_value=None, include_empty=None): &quot;&quot;&quot; Return the fields to export as an iterable of tuples (name, serialized_value) :param item: :param default_value: :param include_empty: :return: &quot;&quot;&quot; if include_empty is None: include_empty = self.export_empty_fields if self.fields_to_export is None: if include_empty and not isinstance(item, dict): field_iter = six.iterkeys(item.fields) else: field_iter = six.iterkeys(item) else: if include_empty: field_iter = self.fields_to_export else: field_iter = (x for x in self.fields_to_export if x in item) for field_name in field_iter: if field_name in item: field = {} if isinstance(item, dict) else item.fields[field_name] value = self.serialize_field(field, field_name, item[field_name]) else: value = default_value yield field_name, value # json class JsonItemExporter(BaseItemExporter): def __init__(self, file, **kwargs): self._configure(kwargs, dont_fail=True) self.file = file kwargs.setdefault(&quot;ensure_ascii&quot;, not self.encoding) self.encoder = ScrapyJSONEncoder(**kwargs) self.first_item = True def start_exporting(self): &quot;&quot;&quot; 保证最终导出结果是一个 json 列表. :return: &quot;&quot;&quot; self.file.write(b&quot;[\n&quot;) def finish_exporting(self): &quot;&quot;&quot; 保证最终导出结果是一个 json 列表. :return: &quot;&quot;&quot; self.file.write(b&quot;\n]&quot;) def export_item(self, item): &quot;&quot;&quot; 调用 self.encoder.encode 将每一项数据转换成 json 串. :param item: :return: &quot;&quot;&quot; if self.first_item: self.first_item = False else: self.file.write(b&quot;,\n&quot;) itemdict = dict(self._get_serialized_field(item)) data = self.encoder.encode(itemdict) self.file.write(to_bytes(data, self.encoding)) # 自定义 excel 导出格式. class ExcelItemExporter(BaseItemExporter): def __init__(self, file, **kwargs): self._configure(kwargs) self.file = file self.wbook = xlwt.Workbook() self.wsheet = self.wbook.add_sheet(&quot;scrapy&quot;) self.row = 0 def finish_exporting(self): self.wbook.save(self.file) def export_item(self, item): fields = self._get_serialized_field(item) # 获取所有字段的迭代器. for col, v in enumerate(x for _, x in fields): self.wsheet.write(self.row, col, v) self.row += 1 $ vim settings.py # my_exporters.py 与 settings.py 位于同级目录下. FEED_EXPORTERS = {&quot;excel&quot;: &quot;example.my_exporters.ExcelItemExporter&quot;} 八. 下载文件(FilesPipeline)和图片(ImagesPipeline)FilesPipeline 和 ImagesPipeline 可以看做两个特殊的下载器, 用户使用时, 只需要铜鼓 item 的一个特殊字段将要下载文件或图片的 URL 传递给他们, 他们会自动将文件或图片下载到本地, 并将下载结果信息存入 item 的另一个特殊字段, 以便用户在导出文件中查阅. 1. FilesPipeline 使用方法 在 settings.py 中启用 FilesPipeline, 通常将其置于其他 Item Pipelines 之前 ITEM_PIPELINES = {&quot;scrapy.pipelines.files.FilesPipeline&quot;: 1} 在 settings.py 中使用 FILES_STORE 指定文件下载目录. FILES_STORE = &quot;/path/to/my/download&quot; 下载文件 在 Spider 解析一个包含文件下载链接的也面试, 将所有需要下载文件的 url 地址收集到一个列表, 赋给 item 的 file_urls 字段(item[&quot;file_urls&quot;]). FilesPipeline 在处理每一项 item 时, 会读取 item[&#39;file_urls&#39;], 对其中每一个 url 进行下载. class DownloadBookSpider(scrapy.Spider): ... def parse(response): item = {} item[&quot;file_urls&quot;] = [] for url in response.xpath(&quot;//a/@href&quot;).extract(): download_url = response.urljoin(url) item[&quot;file_urls&quot;].append(download_url) yield item 当 FilesPipeline 下载完 item[&quot;file_urls&quot;] 中的所有文件后, 会将各文件的下载结果信息收集到另一个列表, 赋给 item[&quot;files&quot;] 字段, 下载信息结果包含以下内容: Path : 文件下载到本地的路径, 相对于 FILES_STORE 的相对路径. Checksum : 文件的校验和 url : 文件的 url 地址. 示例代码# items.py import scrapy class MatplotlibFileItem(scrapy.Item): file_urls = scrapy.Field() files = scrapy.Field() # spiders/matplotlib.py import scrapy from scrapy.linkextractors import LinkExtractor from ..items import MatplotlibFileItem class MatplotlibSpider(scrapy.Spider): name = &apos;matplotlib&apos; allowed_domains = [&apos;matplotlib.org&apos;] start_urls = [&apos;https://matplotlib.org/examples/index.html&apos;] def parse(self, response): # 爬取所有 二级 页面地址 le = LinkExtractor(restrict_css=&quot;div.toctree-wrapper.compound&quot;, deny=&quot;/index.html$&quot;) links = le.extract_links(response) for link in links: yield scrapy.Request(link.url, callback=self.parse_page) def parse_page(self, response): href = response.css(&quot;a.reference.external::attr(href)&quot;).extract_first() url = response.urljoin(href) example = MatplotlibFileItem() example[&quot;file_urls&quot;] = [url] yield example # pipelines.py : 重写 FilesPipeline 的 file_path 代码, 以自定义保存路径. from scrapy.pipelines.files import FilesPipeline from urlparse import urlparse from os.path import basename, dirname, join class MyFilesPipeline(FilesPipeline): def file_path(self, request, response=None, info=None): path = urlparse(request.url).path return join(basename(dirname(path)), basename(path)) # settings.py ITEM_PIPELINES = { # &quot;scrapy.pipelines.files.FilesPipeline&quot;: 1, &apos;matplotlib_file.pipelines.MyFilesPipeline&apos;: 1, } FILES_STORE = &quot;example_src&quot; # 文件下载路径 # 运行: $ scrapy crawl matplotlib -o examp.json 2. ImagesPipeline图片本身也是文件, ImagesPipeline 是 FilesPipeline 的子类, 使用上和 FilesPipeline 大同小异, 只是在所使用的 item 字段和配置选项上略有差别. Desc FilesPipeline ImagesPipeline 导入路径 scrapy.pipelines.files.FilesPipeline scrapy.pipelines.images.ImagesPipeline Item 字段 file_urls, files image_urls, images 下载目录 FILES_STORE IMAGES_STORE 2.1 生成缩略图在 settings.py 中设置 IMAGES_THUMBS, 他是一个字典, 每一项的值是缩略图的尺寸. IMAGES_THUMBS = { &quot;small&quot;: (50, 50), &quot;big&quot;: (270, 270) } 开启该功能后, 下载一张图片时, 会在本地出现 3 张图片, 其存储路径如下: [IMAGES_STORE]/full/name.jpg [IMAGES_STORE]/thumbs/small/name.jpg [IMAGES_STORE]/thumbs/big/name.jpg 2.2 过滤尺寸过小的图片在 settings.py 中设置 IMAGES_MIN_WIDTH 和 IMAGES_MIN_HEIGHT IMAGES_MIN_WIDTH = 110 IMAGES_MIN_HEIGHT = 110 示例代码# settings.py ITEM_PIPELINES = { &quot;scrapy.pipelines.images.ImagesPipeline&quot;: 1, # &apos;so_img.pipelines.SoImgPipeline&apos;: 300, } IMAGES_STORE = &apos;download_images&apos; # spider import json import scrapy class ImagesSpider(scrapy.Spider): IMG_TYPE = &quot;wallpaper&quot; IMG_START = 0 BASE_URL = &quot;http://image.so.com/zj?ch=%s&amp;sn=%s&amp;listtype=new&amp;temp=1&quot; name = &apos;wallpaper&apos; # allowed_domains = [&apos;image.so.com&apos;] start_urls = [BASE_URL % (IMG_TYPE, IMG_START)] MAX_DOWNLOAD_NUM = 100 start_index = 0 def parse(self, response): infos = json.loads(response.body.decode(&quot;utf-8&quot;)) yield {&quot;image_urls&quot;: [info[&quot;qhimg_url&quot;] for info in infos[&quot;list&quot;]]} # 如果 count 字段大于 0, 并且下载数量不足 MAX_DOWNLOAD_NUM, 继续获取下一页信息. self.start_index += infos[&quot;count&quot;] if infos[&quot;count&quot;] &gt; 0 and self.start_index &lt; self.MAX_DOWNLOAD_NUM: yield scrapy.Request(self.BASE_URL % (self.IMG_TYPE, self.start_index)) # 爬取图片 $ scrapy crawl wallpaper 九. 模拟登陆Scrapy 提供一个 FormRequest 类(Request 的子类), 专门用于构造含有表单数据的请求, FormRequest 的构造器方法有一个 formdata 参数, 接受字典形式的表单数据. 直接构造 FormRequest $ scrapy shell http://example.webscraping.com/places/default/user/login &gt;&gt;&gt; sel = response.xpath(&apos;//div[@style]/input&apos;) &gt;&gt;&gt; sel [&lt;Selector xpath=&apos;//div[@style]/input&apos; data=u&apos;&lt;input name=&quot;_next&quot; type=&quot;hidden&quot; value=&apos;&gt;, &lt;Selector xpath=&apos;//div[@style]/input&apos; data=u&apos;&lt;input name=&quot;_formkey&quot; type=&quot;hidden&quot; val&apos;&gt;, &lt;Selector xpath=&apos;//div[@style]/input&apos; data=u&apos;&lt;input name=&quot;_formname&quot; type=&quot;hidden&quot; va&apos;&gt;] &gt;&gt;&gt; fd = dict(zip(sel.xpath(&apos;./@name&apos;).extract(), sel.xpath(&apos;./@value&apos;).extract())) &gt;&gt;&gt; fd {u&apos;_formkey&apos;: u&apos;9c751a58-3dc2-489f-bf7b-93c31fa00c7f&apos;, u&apos;_formname&apos;: u&apos;login&apos;, u&apos;_next&apos;: u&apos;/places/default/index&apos;} &gt;&gt;&gt; fd[&apos;email&apos;] = &quot;liushuo@webscraping.com&quot; &gt;&gt;&gt; fd[&apos;password&apos;] = &quot;12345678&quot; &gt;&gt;&gt; fd {u&apos;_formkey&apos;: u&apos;9c751a58-3dc2-489f-bf7b-93c31fa00c7f&apos;, u&apos;_formname&apos;: u&apos;login&apos;, u&apos;_next&apos;: u&apos;/places/default/index&apos;, &apos;email&apos;: &apos;liushuo@webscraping.com&apos;, &apos;password&apos;: &apos;12345678&apos;} &gt;&gt;&gt; from scrapy.http import FormRequest &gt;&gt;&gt; request = FormRequest(&quot;http://example.webscraping.com/places/default/user/login&quot;, formdata=fd) &gt;&gt;&gt; fetch(request) &gt;&gt;&gt; response.url &apos;http://example.webscraping.com/places/default/index&apos; &gt;&gt;&gt; &quot;Welcome&quot; in response.text True 调用 FormRequest 的 from_response 方法 调用时, 只需传入一个 Response 对象作为第一个参数, 该方法会解析 Response 对象所包含的页面中的 元素, 帮助用户创建 FormRequest 对象, 并将隐藏 中的信息自动填入表单数据. $ scrapy shell http://example.webscraping.com/places/default/user/login &gt;&gt;&gt; fd = {&quot;email&quot;: &quot;liushuo@webscraping.com&quot;, &quot;password&quot;: &quot;12345678&quot;} &gt;&gt;&gt; from scrapy.http import FormRequest &gt;&gt;&gt; req = FormRequest.from_response(response, fd) &gt;&gt;&gt; fetch(req) &gt;&gt;&gt; response.url &apos;http://example.webscraping.com/places/default/index&apos; 1. 实现登录 Spiderclass LoginSpider(scrapy.Spider): name = &quot;login&quot; allowed_domains = [&quot;example.webscraping.com&quot;] start_urls = [&quot;http://example.webscraping.com/places/default/user/profile&quot;] login_url = &quot;http://example.webscraping.com/places/default/user/login&quot; def parse(self, response): keys = response.css(&quot;table label::text&quot;).re(&quot;(.+):&quot;) values = response.css(&quot;table td.w2p_fw::text&quot;).extract() yield dict(zip(keys, values)) def start_requests(self): yield scrapy.Request(self.login_url, callback=self.login) def login(self, response): fd = {&quot;email&quot;: &quot;liushuo@webscraping.com&quot;, &quot;password&quot;: &quot;12345678&quot;} yield scrapy.http.FormRequest.from_response(response, formdata=fd, callback=self.parse_login) def parse_login(self, response): # 如果 if 判断成功, 调用基类的 start_request() 方法, 继续爬取 start_urls 中的页面. if &quot;Welcome&quot; in response.text: yield from super().start_requests() # Python 3 语法 2. 识别验证码 OCR 识别: tesseract-ocr pytesseract 可以识别的验证码比较简单, 对于某些复杂的验证码, pytesseract 识别率很低, 或无法识别. 基本安装与使用 # 安装 $ yum install tesseract -y $ pip install pillow $ pip install pytesseract # 使用 &gt;&gt;&gt; from PIL import Image &gt;&gt;&gt; import pytesseract &gt;&gt;&gt; img = Image.open(&quot;code.png&quot;) &gt;&gt;&gt; img = img.convert(&quot;L&quot;) # 为提高图像识别率, 把图片转换成黑白图. &gt;&gt;&gt; pytesseract.image_to_string(img) 代码示例: import json from PIL import Image from io import BytesIO import pytesseract class CaptchaLoginSpider(scrapy.Spider): name = &quot;login_captcha&quot; start_urls = [&quot;http://xxx.com&quot;] login_url = &quot;http://xxx.com/login&quot; user = &quot;tom@example.com&quot; password = &quot;123456&quot; def parse(self, response): pass def start_requests(self): yield scrapy.Request(self.login_url, callback=self.login, dont_filter=True) def login(self, response): &quot;&quot;&quot; 该方法即是登录页面的解析方法, 又是下载验证码图片的响应处理函数. :param response: :return: &quot;&quot;&quot; # 如果 response.meta[&quot;login_response&quot;] 存在, 当前 response 为验证码图片的响应, # 否则, 当前 response 为登录页面的响应. login_response = response.meta.get(&quot;login_response&quot;) if not login_response: # 此时 response 为 登录页面的响应, 从中提取验证码图片的 url, 下载验证码图片 captchaUrl = response.css(&quot;label.field.prepend-icon img::attr(src)&quot;).extract_first() captchaUrl = response.urljoin(captchaUrl) yield scrapy.Request(captchaUrl, callback=self.login, meta={&quot;login_response&quot;: response}, dont_filter=True) else: # 此时, response 为验证码图片的响应, response.body 为图片二进制数据, # login_response 为登录页面的响应, 用其构造表单请求并发送. formdata = { &quot;email&quot;: self.user, &quot;password&quot;: self.password, &quot;code&quot;: self.get_captcha_by_ocr(response.body) } yield scrapy.http.FormRequest.from_response(login_response, callback=self.parse_login, formdata=formdata, dont_click=True) def parse_login(self, response): info = json.loads(response.text) if info[&quot;error&quot;] == &quot;0&quot;: scrapy.log.logger.info(&quot;登录成功!&quot;) return super().start_requests() scrapy.log.logger.info(&quot;登录失败!&quot;) return self.start_requests() def get_captha_by_ocr(self, data): img = Image.open(BytesIO(data)) img = img.convert(&quot;L&quot;) captcha = pytesseract.image_to_string(img) img.close() return captcha 网络平台识别 阿里云市场提供很多验证码识别平台, 他们提供了 HTTP 服务接口, 用户通过 HTTP 请求将验证码图片发送给平台, 平台识别后将结果通过 HTTP 响应返回. 人工识别 在 Scrapy 下载完验证码图片后, 调用 Image.show 方法将图片显示出来, 然后调用 Python 内置的 Input 函数, 等待用户肉眼识别后输入识别结果. def get_captha_by_user(self, data): img = Image.open(BytesIO(data)) img.show() captha = input(&quot;请输入验证码: &quot;) img.close() return captha 3. Cookie 登录 &amp;&amp; CookiesMiddleware在使用浏览器登录网站后, 包含用户身份信息的 Cookie 会被浏览器保存到本地, 如果 Scrapy 爬虫能直接使用浏览器的 Cookie 发送 HTTP 请求, 就可以绕过提交表单登录的步骤. 3.1 browsercookie第三方 Python 库 browsercookie 便可以获取 Chrome 和 Firefox 浏览器中的 Cookie. $ pip install browsercookie &gt;&gt;&gt; import browsercookie &gt;&gt;&gt; chrome_cookiejar = browsercookie.chrome() &gt;&gt;&gt; firefox_cookiejar = browsercookie.firefox() &gt;&gt;&gt; type(chrome_cookiejar) http.cookiejar.CookieJar &gt;&gt;&gt; for cookie in chrome_cookiejar: # 对 http.cookiejar.CookieJar 对象进行迭代, 可以访问其中的每个 Cookie 对象. print cookie 3.2 CookiesMiddlewareimport six import logging from collections import defaultdict from scrapy.exceptions import NotConfigured from scrapy.http import Response from scrapy.http.cookies import CookieJar from scrapy.utils.python import to_native_str logger = logging.getLogger(__name__) class CookieMiddleware(object): &quot;&quot;&quot; This middleware enables working with sites that need cookies. &quot;&quot;&quot; def __init__(self, debug=False): &quot;&quot;&quot; jars 中的每一项值, 都是一个 scrapy.http.cookies.CookisJar 对象, CookieMiddleware 可以让 Scrapy 爬虫同时使用多个不同的 CookieJar, 即多个不同的账号. Request(url, meta={&quot;cookiejar&quot;: &quot;account_1&quot;}} :param debug: &quot;&quot;&quot; self.jars = defaultdict(CookieJar) self.debug = debug @classmethod def from_crawler(cls, crawler): &quot;&quot;&quot; 从配置文件读取 COOKIES_ENABLED, 决定是否启用该中间件. :param crawler: :return: &quot;&quot;&quot; if not crawler.settings.getbool(&quot;COOKIES_ENABLED&quot;): raise NotConfigured return cls(crawler.settings.getbool(&quot;COOKIES_DEBUG&quot;)) def process_request(self, request, spider): &quot;&quot;&quot; 处理每一个待发送到额 Request 对象, 尝试从 request.meta[&quot;cookiejar&quot;] 获取用户指定使用的 Cookiejar, 如果用户未指定, 就是用默认的 CookieJar(self.jars[None]). 调用 self._get_request_cookies 方法获取发送请求 request 应携带的 Cookie 信息, 填写到 HTTP 请求头. :param request: :param spider: :return: &quot;&quot;&quot; if request.meta.get(&quot;dont_merge_cookies&quot;, False): request cookiejarkey = request.meta.get(&quot;cookiejar&quot;) jar = self.jar[cookiejarkey] cookies = self._get_request_cookies(jar, request) for cookie in cookies: jar.set_cookie_if_ok(cookie, request) # set Cookie header request.headers.pop(&quot;Cookie&quot;, None) jar.add_cookie_header(request) self._debug_cookie(request, spider) def process_response(self, request, response, spider): &quot;&quot;&quot; 处理每一个 response 对象, 依然通过 request.meta[&quot;cookiejar&quot;] 获取用户指定使用的 cookiejar, 调用 extract_cookies 方法将 HTTP 响应头部中的 Cookie 信息保存到 CookieJar 对象中. :param request: :param response: :param spider: :return: &quot;&quot;&quot; if request.meta.get(&quot;dont_merge_cookies&quot;, False): return response # extract cookies from Set-Cookie and drop invalid/expired cookies. cookiejarkey = request.meta.get(&quot;cookiejar&quot;) jar = self.jars[cookiejarkey] jar.extract_cookies(response, request) self._debug_set_cookie(response, request) return response def _debug_cookie(self, request, spider): if self.debug: cl = [to_native_str(c, errors=&quot;replace&quot;) for c in request.headers.getlist(&quot;Cookie&quot;)] if cl: cookies = &quot;\n&quot;.join(&quot;Cookie: {}\n&quot;.format(c) for c in cl) msg = &quot;Sending cookies to:{}\n&quot;.format(request, cookies) logger.debug(msg, extra={&quot;spider&quot;: spider}) def _debug_set_cookie(self, response, spider): if self.debug: cl = [to_native_str(c, errors=&quot;replace&quot;) for c in response.headers.getlist(&quot;Set-Cookie&quot;)] if cl: cookies = &quot;\n&quot;.join(&quot;Set-Cookie: {}\n&quot;.format(c) for c in cl) msg = &quot;Received cookies from:{}\n&quot;.format(response, cookies) logger.debug(msg, extra={&quot;spider&quot;: spider}) def _format_cookie(self, cookie): # build cookie string cookie_str = &quot;%s=%s&quot; % (cookie[&quot;name&quot;], cookie[&quot;value&quot;]) if cookie.get(&quot;path&quot;, None): cookie_str += &quot;;Path=%s&quot; % cookie[&quot;path&quot;] if cookie.get(&quot;domain&quot;, None): cookie_str += &quot;;Domain=%s&quot; % cookie[&quot;domain&quot;] return cookie_str def _get_request_cookies(self, jar, request): if isinstance(request.cookies, dict): cookie_list = [{&quot;name&quot;: k, &quot;value&quot;: v} for k, v in six.iteritems(request.cookies)] else: cookie_list = request.cookies cookies = [self._format_cookie(x) for x in cookie_list] headers = {&quot;Set-Cookie&quot;: cookies} response = Response(request.url, headers=headers) return jar.make_cookies(response, request) 3.3 实现 BrowserCookieMiddleware利用 browsercookie 对 CookieMiddleware 进行改良. import browsercookie from scrapy.downloadermiddlewares.cookies import CookiesMiddleware class BrowserCookiesMiddleware(CookiesMiddleware): def __init__(self, debug=False): super().__init__(debug) self.load_browser_cookies() def load_browser_cookies(self): # for chrome jar = self.jars[&quot;chrome&quot;] chrome_cookies = browsercookie.chrome() for cookie in chrome_cookies: jar.set_cookie(cookie) # for firefox jar = self.jars[&quot;firefox&quot;] firefox_cookies = browsercookie.firefox() for cookie in firefox_cookies: jar.set_cookie(cookie) 使用示例: # settings.py USER_AGENT = &quot;Mozilla/5.0 (X11; CrOS i686 3912.101.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/27.0.1453.116 Safari/537.36&quot; DOWNLAODER_MIDDLEWARES = { &quot;scrapy.downloademiddleware.cookies.CookiesMiddleware&quot;: None, &quot;browser_cookie.middlewares.BrowserCookiesMiddleware&quot;: 701 } $ scrapy shell &gt;&gt;&gt; from scrapy import Request &gt;&gt;&gt; url = &quot;https://www.zhihu.com/settings/profile&quot; &gt;&gt;&gt; fetch(Request(url, meta={&quot;cookiejar&quot;: &apos;chrome&apos;})) &gt;&gt;&gt; view(response) 十. 爬取动态页面: Splash 渲染引擎Splash 是 Scrapy 官方推荐的 javascript 渲染引擎, 他是用 Webkit 开发的轻量级无界面浏览器, 提供基于 http 接口的 javascript 渲染服务, 支持如下功能: 为用户返回经过渲染的 HTML 页面或页面截图 并发渲染多个页面 关闭图片加载, 加速渲染 在页面中执行用户自定义的 javascript 代码. 指定用户自定义的渲染脚本(lua), 功能类似于 PhantomJS. 1. 安装# 安装 docker $ yum remove docker docker-client docker-client-latest docker-common docker-latest docker-latest-logrotate docker-logrotate docker-selinux docker-engine-selinux docker-engine -y $ yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo $ yum install -y yum-utils device-mapper-persistent-data lvm2 $ yum install docker-ce $ systemctl start docker # 获取镜像 $ docker pull scrapinghub/splash # 运行 splash 服务 $ docker run -p 8051:8051 -p 8050:8050 scrapinghub/splash 2. render.html : 提供 javascript 渲染服务 服务端点 render.html 请求地址 http://localhost:8051/render.html 请求方式 GET/POST 返回类型 html 参数列表: 参数 是否必选 类型 描述 url 必选 string 需要渲染的页面 url timeout 可选 float 渲染页面超时时间 proxy 可选 string 代理服务器地址 wait 可选 float 等待页面渲染的时间 images 可选 integer 是否下载图片, 默认为 1 js_source 可选 string 用自定义的 javascript 代码, 在页面渲染前执行 示例: 使用 request 库调用 render.html 渲染页面. &gt;&gt;&gt; import requests &gt;&gt;&gt; from scrapy.selector import Selector &gt;&gt;&gt; splash_url = &quot;http://localhost:8050/render.html&quot; &gt;&gt;&gt; args = {&quot;url&quot;: &quot;http://quotes.toscrape.com/js&quot;, &quot;timeout&quot;:5,&quot;image&quot;:0} &gt;&gt;&gt; response = requests.get(splash_url, params=args) &gt;&gt;&gt; sel = Selector(response) &gt;&gt;&gt; sel.css(&quot;div.quote span.text::text&quot;).extract() 3. execute : 指定用户自定义的 lua 脚本, 利用该断点可在页面中执行 javascript 代码.在爬去某些页面时, 希望在页面中执行一些用户自定义的 javascript 代码, 例如, 用 javascript 模拟点击页面中的按钮, 或调用页面中的 javascript 函数与服务器交互, 利用 Splash 的 execute 端点可以实现这样的功能. 服务端点 execute 请求地址 http://localhost:8051/execute 请求方式 POST 返回类型 自定义 参数 : 参数 是否必选 类型 描述 lua_source 必选 string 用户自定义的 Lua 脚本 timeout 可选 float 渲染页面超时时间 proxy 可选 string 代理服务器地址 可以将 execute 端点的服务看做一个可用 lua 语言编程的浏览器, 功能类似于 PhantomJS. 使用时需传递一个用户自定义的 Lua 脚本给 Spalsh, 该 Lua 脚本中包含用户想要模拟的浏览器行为. 如 打开某 url 地址的页面 等待页面加载完成 执行 javascript 代码 获取 HTTP 响应头部 获取 Cookie 用户定义的 lua 脚本必须包含一个 main 函数作为程序入口, main 函数被调用时传入一个 splash 对象(lua中的对象), 用户可以调用该对象上的方法操作 Splash. main 函数的返回值可以使 字符串, 也可以是 lua 中的表(类似 python 字典), 表会被编码成 json 串. Splash 对象常用属性和方法 splash.args属性 用户传入参数的表, 通过该属性可以访问用户传入的参数, 如 splash.args.url splash.js_enabled 用于开启/禁止 javascript 渲染, 默认为 True splash.images_enabled 用于开启/禁止 图片加载, 默认为 True splash:go() splash:go(url, baseurl=nil, headers=nil, http_method=&quot;GET&quot;, body=nil, formdata=nil) 类似于在浏览器中打开某 url 地址的页面, 页面所需资源会被加载, 并进行 javascript 渲染, 可以通过参数指定 HTTP 请求头部, 请求方法, 表单数据等. splash:wait() splash:wait(time, cancel_on_redirect=false, cancel_on_error=true) 等待页面渲染, time 参数为等待的秒数. splash:evaljs() splash:evaljs(snippet) 在当前页面下, 执行一段 javascript 代码, 并返回最后一句表达式的值. splash:runjs() splash:runjs(snippet) 在当前页面下, 执行一段 javascript 代码, 与 evaljs 相比, 该函数只执行代码, 不返回值. splash:url() 获取当前页面的 url splash:html() 获取当前页面的 html 文本. splash:get_cookits() 获取全部 cookie 信息. 示例代码: requests 库调用 execute 端点服务 &gt;&gt;&gt; import json &gt;&gt;&gt; lua_script = &quot;&quot;&quot; ...: function main(splash) ...: splash:go(&apos;http://example.com&apos;) ...: splash:wait(0.5) ...: local title = splash:evaljs(&quot;document.title&quot;) ...: return {title=title} ...: end &quot;&quot;&quot; &gt;&gt;&gt; splash_url = &quot;http://localhost:8050/execute&quot; &gt;&gt;&gt; headers = {&quot;content-type&quot;: &quot;application/json&quot;} &gt;&gt;&gt; data = json.dumps({&quot;lua_source&quot;: lua_script}) &gt;&gt;&gt; response = requests.post(splash_url, headers=headers, data=data) &gt;&gt;&gt; response.content &gt;&gt;&gt; &apos;{&quot;title&quot;: &quot;Example Domain&quot;}&apos; &gt;&gt;&gt; response.json() &gt;&gt;&gt; {u&apos;title&apos;: u&apos;Example Domain&apos;} 4. scrapy-splash 安装 $ pip install scrapy-splash 配置 $ cat settings.py # Splash 服务器地址 SPLASH_URL = &quot;http://localhost:8050&quot; # 开启 Splash 的两个下载中间件并调整 HttpCompressionMiddleware 的次序 DOWNLOADER_MIDDLEWARES = { &quot;scrapy_splash.SplashCookiesMiddleware&quot;: 723, &quot;scrapy_splash.SplashMiddleware&quot;: 725, &quot;scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware&quot;: 810, } # 设置去重过滤器 DUPEFILTER_CLASS = &quot;scrapy_splash.SplashAwareDupeFilter&quot; # 用来支持 cache_args (可选) SPIDER_MIDDLEWARES = { &quot;scrapy_splash.SplashDeduplicateArgsMiddleware&quot;: 100, } 使用 Scrapy_splash 调用 Splash 服务非常简单. scrapy_Splash 中定义了一个 SplashRequest 类, 用户只需使用 scrapy_splash.SplashRequest (替代 scrapy.Request) 提交请求即可. SplashRequest 构造器方法参数 url : 待爬去页面的 url headers : 请求 headers, 同 scrapy.Request. cookies : 请求 cookie, 同 scrapy.Request. args : 传递给 Splash 的参数(除 url), 如 wait, timeout, images, js_source 等 cache_args : 如果 args 中的某些参数每次调用都重复传递, 并且数据量巨大, 此时可以把该参数名填入 cache_args 列表中, 让 Splash 服务器缓存该参数. 如 SplashRequest(url, args={&quot;js_source&quot;: js, &quot;wait&quot;: 0.5}, cache_args=[&quot;js_source&quot;]) endpoint : Splash 服务端点, 默认为 render.html, 即 javascript 渲染服务. 该参数可以设置为 render.json, render.har, render.png, render.jpeg, execute 等. 详细参考文档. splash_url : Splash 服务器地址, 默认为 None, 即使用配置文件中的 SPLASH_URL 地址. 5. 代码示例 quote 名人名言爬取 import scrapy from scrapy_splash import SplashRequest class QuotesSpider(scrapy.Spider): name = &apos;quotes&apos; allowed_domains = [&apos;quotes.toscrape.com&apos;] start_urls = [&apos;http://quotes.toscrape.com/js&apos;] splash_base_args = {&quot;images&quot;: 0, &quot;timeout&quot;: 3} def start_requests(self): for url in self.start_urls: yield SplashRequest(url, args=self.splash_base_args) def parse(self, response): for sel in response.css(&quot;div.quote&quot;): quote = sel.css(&quot;span.text::text&quot;).extract_first() author = sel.css(&quot;small.author::text&quot;).extract_first() yield {&quot;quote&quot;: quote, &quot;author&quot;: author} href = response.css(&quot;li.next &gt; a::attr(href)&quot;).extract_first() if href: url = response.urljoin(href) yield SplashRequest(url, args=self.splash_base_args) jd 图书 爬取 import scrapy from scrapy import Request from scrapy_splash import SplashRequest lua_script = &quot;&quot;&quot; function main(splash) splash:go(splash.args.url) splash:wait(2) splash:runjs(&quot;document.getElementsByClassName(&apos;page&apos;)[0].scrollIntoView(true)&quot;) splash:wait(2) return splash:html() end &quot;&quot;&quot; class JdBookSpider(scrapy.Spider): name = &apos;jd_book&apos; allowed_domains = [&apos;search.jd.com&apos;] base_url = &quot;https://search.jd.com/Search?keyword=python&amp;enc=utf-8&amp;book=y&amp;wq=python&quot; def start_requests(self): # 请求第一个页面, 无需渲染 js yield Request(self.base_url, callback=self.parse_url, dont_filter=True) def parse_url(self, response): # 获取商品总数, 计算出总页数. total = int(response.css(&quot;span#J_resCount::text&quot;).re_first(&quot;(\d+)\D?&quot;)) pageNum = total // 60 + (1 if total % 60 else 0) # 构造每一页的 url, 向 Splash 端点发送请求 for i in xrange(pageNum): url = &quot;%s&amp;page=%s&quot; % (self.base_url, 2*i + 1) headers = {&quot;refer&quot;: self.base_url} yield SplashRequest(url, endpoint=&quot;execute&quot;, headers=headers, args={&quot;lua_source&quot;: lua_script}, cache_args=[&quot;lua_source&quot;]) def parse(self, response): # 获取单个页面中每本书的名字和价格 for sel in response.css(&quot;ul.gl-warp.clearfix &gt; li.gl-item&quot;): yield { &quot;name&quot;: sel.css(&quot;div.p-name&quot;).xpath(&quot;string(.//em)&quot;).extract_first(), &quot;price&quot;: sel.css(&quot;div.p-price i::text&quot;).extract_first() } $ vim settings.py USER_AGENT = u&apos;Mozilla/5.0 (Windows NT 6.2; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1667.0 Safari/537.36&apos; 十一. HTTP 代理Scrapy 内部提供了一个下载中间件HttpProxyMiddleware, 专门用于给 Scrapy 设置代理, 他默认是启动的, 他会在系统环境变量中搜索当前系统代理(名称格式为xxx_proxy的环境变量), 作为 Scrapy 爬虫使用的带来. $ export http_proxy=&quot;http://192.168.1.1:8000&quot; $ export https_proxy=&quot;http://192.168.1.1:8001&quot; # 包含用户名和密码 $ export https_proxy=&quot;http://username:password@192.168.1.1:8001&quot; $ curl http(s)://httpbin.org/ip # 返回一个包含请求源 ip 地址信息的额 json 字符串. Scrapy 中为一个请求设置代理的本质就是将代理服务器的 url 填写到 request.meta[&quot;proxy&quot;]. class HttpProxyMiddleware(object): ... def _set_proxy(self, request, scheme): creds, proxy = self.proxies[scheme] request.meta[&quot;proxy&quot;] = proxy if creds: # 如果需要认证, 传递包含用户账号和密码的身份验证信息 request.headers[&quot;Proxy-Authorization&quot;] = b&quot;Basic&quot; + creds # 手动实现 $ scrapy shell &gt;&gt;&gt; from scrapy import Request &gt;&gt;&gt; import base64 &gt;&gt;&gt; req = Request(&quot;http://httpbin.org/ip&quot;, meta={&quot;proxy&quot;: &quot;http://192.168.1.1:8000&quot;}) &gt;&gt;&gt; user = &quot;tom&quot; &gt;&gt;&gt; password = &quot;tom123&quot; &gt;&gt;&gt; user_passwd = (&quot;%s:%s&quot; % (user, password)).encode(&quot;utf8&quot;) &gt;&gt;&gt; req.headers[&quot;Proxy-Authorization&quot;] = b&quot;Basic&quot; + base64.b64encode(user_passwd) &gt;&gt;&gt; fetch(req) 1. 抓取免费代理:代理网站: http://proxy-list.org https://free-proxy-list.net http://www.xicidaili.com http://www.proxy360.cn http://www.kuaidaili.com 获取西祠代理代码 # settings.py USER_AGENT = &quot;Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN) AppleWebKit/523.15 (KHTML, like Gecko, Safari/419.3) Arora/0.3 (Change: 287 c9dfb30)&quot; # spider.py import json import scrapy from scrapy import Request class XiciSpider(scrapy.Spider): name = &apos;xici&apos; allowed_domains = [&apos;www.xicidaili.com&apos;] # start_urls = [&apos;http://www.xicidaili.com/nn/&apos;] base_url = &quot;http://www.xicidaili.com/nn/%s&quot; check_url = &quot;%s://httpbin.org/ip&quot; def start_requests(self): for i in xrange(1, 5): yield Request(self.base_url % i) def parse(self, response): for sel in response.xpath(&quot;//table[@id=&apos;ip_list&apos;]/tr[position()&gt;1]&quot;): ip = sel.css(&apos;td:nth-child(2)::text&apos;).extract_first() port = sel.css(&apos;td:nth-child(3)::text&apos;).extract_first() scheme = sel.css(&apos;td:nth-child(6)::text&apos;).extract_first().lower() url = self.check_url % scheme proxy = &quot;%s://%s:%s&quot; % (scheme, ip, port) meta = { &quot;proxy&quot;: proxy, &quot;dont_retry&quot;: True, &quot;download_timeout&quot;: 10, &quot;_proxy_scheme&quot;: scheme, &quot;_proxy_ip&quot;: ip } yield Request(url, callback=self.check_available, meta=meta, dont_filter=True) def check_available(self, response): proxy_ip = response.meta[&quot;_proxy_ip&quot;] if proxy_ip == json.loads(response.text)[&quot;origin&quot;]: yield { &quot;proxy_scheme&quot;: response.meta[&quot;_proxy_scheme&quot;], &quot;proxy&quot;: response.meta[&quot;proxy&quot;] } 2. 基于 HttpProxyMiddleware 实现随机代理# middlewares.py class RandomHttpProxyMiddleware(HttpProxyMiddleware): def __init__(self, auth_encoding=&quot;latin-1&quot;, proxy_list_file=None): if not proxy_list_file: raise NotConfigured self.auth_encoding = auth_encoding # 用两个列表维护 HTTP 和 HTTPS 代理, {&quot;http&quot;: [...], &quot;https&quot;: [...]} self.proxies = defaultdict(list) with open(proxy_list_file) as f: proxy_list = json.load(f) for proxy in proxy_list: scheme = proxy[&quot;proxy_scheme&quot;] url = proxy[&quot;proxy&quot;] self.proxies[scheme].append(self._get_proxy(url, scheme)) @classmethod def from_crawler(cls, crawler): auth_encoding = crawler.settings.get(&quot;HTTPPROXY_AUTH_ENCODING&quot;, &quot;latin-1&quot;) proxy_list_file = crawler.settings.get(&quot;HTTPPROXY_PROXY_LIST_FILE&quot;) return cls(auth_encoding, proxy_list_file) def _set_proxy(self, request, scheme): creds, proxy = random.choice(self.proxies[scheme]) request.meta[&quot;proxy&quot;] = proxy if creds: request.headers[&quot;Proxy-Authorization&quot;] = b&quot;Basic&quot; + creds # spider.py : 测试随机 proxy 是否 work import json import scrapy from scrapy import Request class TestRandomProxySpider(scrapy.Spider): name = &quot;random_proxy&quot; def start_requests(self): for _ in range(100): yield Request(&quot;http://httpbin.org/ip&quot;, dont_filter=True) yield Request(&quot;https://httpbin.org/ip&quot;, dont_filter=True) def parse(self, response): print json.loads(response.text) # settings.py DOWNLOADER_MIDDLEWARES = { &apos;proxy_example.middlewares.RandomHttpProxyMiddleware&apos;: 543, } HTTPPROXY_PROXY_LIST_FILE = &quot;proxy.json&quot; 3. 实战: 豆瓣电影# Spider.py import json import re import scrapy from scrapy import Request class DmovieSpider(scrapy.Spider): BASE_URL = &quot;https://movie.douban.com/j/search_subjects?type=movie&amp;tag=%s&amp;sort=recommend&amp;page_limit=%s&amp;page_start=%s&quot; MOVIE_TAG = &quot;豆瓣高分&quot; PAGE_LIMIT = 20 page_start = 0 name = &apos;dmovie&apos; allowed_domains = [&apos;movie.douban.com&apos;] start_urls = [BASE_URL % (MOVIE_TAG, PAGE_LIMIT, page_start)] def parse(self, response): infos = json.loads(response.body.decode(&quot;utf-8&quot;)) for movie_info in infos[&quot;subjects&quot;]: movie_item = {} movie_item[&quot;片名&quot;] = movie_info[&quot;title&quot;] movie_item[&quot;评分&quot;] = movie_info[&quot;rate&quot;] yield Request(movie_info[&quot;url&quot;], callback=self.parse_movie, meta={&quot;_movie_item&quot;: movie_item}) if len(infos[&quot;subjects&quot;]) == self.PAGE_LIMIT: self.page_start += self.PAGE_LIMIT url = self.BASE_URL % (self.MOVIE_TAG, self.PAGE_LIMIT, self.page_start) yield Request(url) def parse_movie(self, response): movie_item = response.meta[&quot;_movie_item&quot;] info = response.css(&quot;div.subject div#info&quot;).xpath(&quot;string(.)&quot;).extract_first() fields = [s.strip().replace(&quot;:&quot;, &quot;&quot;) for s in response.css(&quot;div#info span.pl::text&quot;).extract()] values = [re.sub(&quot;\s+&quot;, &quot;&quot;, s.strip()) for s in re.split(&apos;\s*(?:%s):\s*&apos; % &quot;|&quot;.join(fields), info)][1:] movie_item.update(dict(zip(fields, values))) yield movie_item # settings.py DOWNLOADER_MIDDLEWARES = { &apos;douban_movie.middlewares.RandomHttpProxyMiddleware&apos;: 543, } HTTPPROXY_PROXY_LIST_FILE = &quot;proxy.json&quot; USER_AGENT = &quot;Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN) AppleWebKit/523.15 (KHTML, like Gecko, Safari/419.3) Arora/0.3 (Change: 287 c9dfb30)&quot; DOWNLOAD_DELAY = 2 ROBOTSTXT_OBEY = False # middleware.py class RandomHttpProxyMiddleware(HttpProxyMiddleware): def __init__(self, auth_encoding=&quot;latin-1&quot;, proxy_list_file=None): if not proxy_list_file: raise NotConfigured self.auth_encoding = auth_encoding # 用两个列表维护 HTTP 和 HTTPS 代理, {&quot;http&quot;: [...], &quot;https&quot;: [...]} self.proxies = defaultdict(list) with open(proxy_list_file) as f: proxy_list = json.load(f) for proxy in proxy_list: scheme = proxy[&quot;proxy_scheme&quot;] url = proxy[&quot;proxy&quot;] self.proxies[scheme].append(self._get_proxy(url, scheme)) @classmethod def from_crawler(cls, crawler): auth_encoding = crawler.settings.get(&quot;HTTPPROXY_AUTH_ENCODING&quot;, &quot;latin-1&quot;) proxy_list_file = crawler.settings.get(&quot;HTTPPROXY_PROXY_LIST_FILE&quot;) return cls(auth_encoding, proxy_list_file) def _set_proxy(self, request, scheme): creds, proxy = random.choice(self.proxies[scheme]) request.meta[&quot;proxy&quot;] = proxy if creds: request.headers[&quot;Proxy-Authorization&quot;] = b&quot;Basic&quot; + creds # proxy.json [ {&quot;proxy_scheme&quot;: &quot;http&quot;, &quot;proxy&quot;: &quot;http://111.155.116.237:8123&quot;}, {&quot;proxy_scheme&quot;: &quot;https&quot;, &quot;proxy&quot;: &quot;https://222.188.190.99:6666&quot;}, {&quot;proxy_scheme&quot;: &quot;https&quot;, &quot;proxy&quot;: &quot;https://60.23.36.250:80&quot;}, {&quot;proxy_scheme&quot;: &quot;https&quot;, &quot;proxy&quot;: &quot;https://120.79.216.57:6666&quot;}, {&quot;proxy_scheme&quot;: &quot;https&quot;, &quot;proxy&quot;: &quot;https://120.92.88.202:10000&quot;}, {&quot;proxy_scheme&quot;: &quot;https&quot;, &quot;proxy&quot;: &quot;https://120.79.151.197:6666&quot;}, {&quot;proxy_scheme&quot;: &quot;http&quot;, &quot;proxy&quot;: &quot;http://118.114.77.47:8080&quot;}, {&quot;proxy_scheme&quot;: &quot;http&quot;, &quot;proxy&quot;: &quot;http://112.74.62.69:8081&quot;}, {&quot;proxy_scheme&quot;: &quot;https&quot;, &quot;proxy&quot;: &quot;https://218.93.166.4:6666&quot;}, {&quot;proxy_scheme&quot;: &quot;http&quot;, &quot;proxy&quot;: &quot;http://58.216.202.149:8118&quot;}, {&quot;proxy_scheme&quot;: &quot;http&quot;, &quot;proxy&quot;: &quot;http://14.118.253.233:6666&quot;} ] 十二. scrapy-redis 分布式爬虫Scrapy-redis 利用 Redis 数据库重新实现了 Scrapy 中的某些组件. 基于 Redis 的请求队列(优先队列, FIFO, LIFO) 基于 Redis 的请求去重过滤器(过滤掉重复的请求) 基于以上两个组件的调度器 Scrapy-redis 为多个爬虫分配爬取任务的方式是: 让所有爬虫共享一个存在于 Redis 数据库中的请求队列(替代各爬虫独立的请求队列), 每个爬虫从请求队列中获取请求, 下载并解析出新请求再添加到 请求队列中, 因此, 每个爬虫即是下载任务的生产者, 又是消费者. 搭建分布式环境 # 在所有机器上安装包 $ pip install scrapy $ pip install scrapy-redis # 启动redis server, 确保分布式环境中每台机器均可访问 redis-server $ redis-cli -h REDIS_SERVER ping 配置项目 # settings.py ## 指定爬虫使用的 redis 数据库 REDIS_URL = &quot;redis://192.168.1.10:6379&quot; ## 使用 scrapy-redis 的调度器替代 scrapy 原版调度器 SCHEDULER = &quot;scrapy_redis.scheduler.Scheduler&quot; ## 使用 scrapy-redis 的 RFPDupeFilter 作为去重过滤器 DUPEFILTER_CLASS = &quot;scrapy_redis.dupefilter.RFPDupeFilter&quot; ## 启动 scrapy_redis 的 RedisPipeline 将爬取到的数据汇总到 数据库. ITEM_PIPELINES = { &quot;scrapy_redis.pipelines.RedisPipeline&quot;: 300, } ## 爬虫停止后, 保留/清理 redis 中的请求队列及去重即可. True: 保留, False: 清理(默认). SCHEDULER_PERSIST = True Scrapy-redis 提供了一个新的 Spider 基类 RedisSpider, RedisSpider 重写了 start_requests 方法, 他重试从 redis 数据库的某个特定列表中获取起始爬取点, 并构造 Request 对象(dont_filter=False), 该列表的键可通过配置文件设置(REDIS_START_URLS_KEY), 默认为 &lt;spider_name&gt;:start_urls. 在分布式爬取时, 用户运行所有爬虫后, 需要手动使用 Redis 命令向该列表添加起始爬取点, 从而避免重复. # spider.py from scrapy_redis.spiders import RedisSpider class BooksSpider(RedisSpider): # 爬虫 继承 RedisSpider 类 pass # 注释 start_urls # start_urls = [&quot;http://book.toscrape.com&quot;] # 命令行 写入队列开始值. $ redis-cli -h 192.168.1.10 &gt; lpush books:start_urls &quot;http://books.toscrape.com/&quot; 十三. 奇技淫巧1. scrapy 项目的一般步骤 创建 项目 $ scrapy startproject PROJECT_NAME 创建 spider $ cd PROJECT_NAME $ scrapy genspider SPIDER_NAME DOMAIN 封装 Item 类 完成 Spider 类 配置 settings.py ## 指定输出序列 FEED_EXPORT_FIELDS = [] ## 绕过 roobot.txt ## USER_AGENT 配置 编写 Pipeline, 实现 item 字段转换 : settings.py ITEM_PIPELINES = { PIPELINE_NAME: rate, } 运行 crawl $ scrapy list $ scrapy crawl MySpider 2. User-Agent 使用 fake-useragent GitHub - hellysmile/fake-useragent: up to date simple useragent faker with real world database $ pip install fake-useragent 各大搜索引擎的 UA 可以伪装成各大搜索引擎网站的UA， 比如 Google UA 添加referfer字段为 搜索引擎网站 也是有用的，因为网站是希望被索引的，所以会放宽搜索引擎的爬取策略。 useragentstring.com 3. 代理网上的开源代理: https://github.com/xiaosimao/IP_POOL 代理网站: http://www.kuaidaili.com/free/ http://www.66ip.cn/ http://www.goubanjia.com/free/gngn/index.shtml http://www.xicidaili.com/ data5u proxydb 测试网站: 百度 https://httpbin.org/get 十四. 参考链接 精通 Scrapy 网络爬虫 Scrapy 文档 1. First Step2. 基本概念2.1 命令行工具2.2 spiders 爬虫2.3 Selectors 选择器2.4 Items2.5 Item Loaders2.6 Item Pipeline2.7 Feed exports : 输出和存储数据2.8 请求和响应2.9 连接提取2.10 设置, 配置2.11 Exceptions3. 内置服务3.1 Logging3.2 Stats Collection3.3 Sending e-mail3.4 Telnet Console3.5 Web Service4. Q&amp;A5.扩展 scrapy]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>Scrapy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ansible-基本原理与安装配置]]></title>
    <url>%2F2018%2F03%2F15%2FAnsible-%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[Ansible 基本原理, 安装配置与命令行工具的使用. Ansible 学习总结 1. Ansible 简介Ansible 是使用 Python 开发的, 基于 SSH 协议的, Agentless 的配置管理工具. 其源代码存放于 githb 上, 分隔成 三部分 分别存放在不同的代码仓库上. 主仓库 : https://github.com/ansible/ansible 核心模块 : https://github.com/ansible/ansible-modules-core 其他模块 : https://github.com/ansible/ansible-modules-extras 2. Ansible 任务的执行细节原理2.1 角色与依赖: 被管理主机 : 需要 ssh 和 python2.5 或者更高版本. 管理主机 : 需要 Python2.6 或者更高的版本. 有些模块需要额外的依赖, 如 ec2模块 依赖 boto模块, docker 模块依赖 docker-py 等. 2.2 工作机制Ansible 默认采用推送模式, 但是也支持 拉取模式, 使用 ansible-pull 命令. 2.3 工作原理示例代码: - name: install nginx apt: name=nginx ansible 操作如下: 在管理主机生成安装 nginx 软件包的 python 程序 将该程序复制到 目标服务器. 在目标服务器上完成操作, 执行 程序 等待改程序在所有主机上完成. ansible 执行模块指令的注意事项: 对于每一个任务, ansible 都是并行执行的. 在开始下一个任务之前, ansible 会等待所有主机都完成上一个任务. ansible 任务的执行顺序, 为管理员定义的执行顺序. 幂等性 3. 安装配置3.1 安装12$ yum install python-pip$ pip install ansible 3.2 配置文件 配置段 ansible.cfg 有 defaults, ssh_connection, paramiko, accelerate 四个配置段, 其具体配置项见ansible 番外篇之 ansible.cfg 配置参数 配置文件及优先级 ansible 配置文件 : ansible.cfg, ansible 使用如下位置和顺序来查找 ansible.cfg 文件 ANSIBLE_CONFIG 环境变量指向的文件 ./ansible.cfg ~/.ansible.cfg /etc/ansible/ansible.cfg 基础示例: [defaults] hostfile = hosts remote_user = ec2-user private_key_file = /path/to/my_private_key host_key_checking = False # 关闭 host key 检查. 4. Ansible 抽象实体4.1 inventoryAnsible 执行的目标主机的配置文件. Ansible 支持静态 Inventory 文件 和 动态 Inventory , 默认的 静态 Inventory 文件为 /etc/ansible/hosts, 同时支持 Cobbler Inventory , AWS ec2.py 等动态 Inventory. 4.2 变量与factAnsible 支持如下变量类型及定义, 可以提高 task 的可复用性和适用性: 自定义变量 注册变量 内置变量 fact 变量 4.3 模块模块定义 Ansible 可以在目标主机执行的具体操作, 是由 Ansible 包装好后在这几上执行一系列操作的脚本, 是 Ansible 执行操作的最小粒度.Ansible 支持如下模块类型: 内置模块 自定义模块: 支持多种编程语言, 如 shell, python, ruby 等. 4.4 task/play/role/playbook task : 由模块定义的具体操作. play : 多个包含 host, task 等多个字段部分的任务定义集合. 是一个 YAML 字典结构. playbook : 多个 play 组成的列表 role : 是将 playbook 分割为多个文件的主要机制, 用于简化 playbook 的编写, 并提高 playbook 的复用性. 5. Ansible 命令5.1 ansible123456789101112131415161718192021$ ansible -i INVENTORY HOST_GROUP [ -s ] -m MODEL -a ARGS [-vvvv] -i INVENTORY : 指定 INVENTORY -s : sudo 为 root 执行 -m MODEL : 模块 -a ARGS : 模块参数 -vvvv : 输出详细信息.# 检测是否可以连接到服务器.$ ansible testserver -i hosts -m ping [-vvvv]# 查看服务器运行时间$ ansible testserver -i hosts -m command -a uptime# 参数中包含空格, 应该使用 引号 引起来.$ ansible testserver -i hosts -m command -a "tail /var/log/messages"# 安装 nginx 包$ ansible testserver -s -m apt -a name=nginx# 重启 nginx 服务$ ansible testserver -s -m service -a name=nginx state=restarted 5.2 ansible-docAnsible 模块的帮助文档. 12345678# 列出所有可用模块$ ansible-doc --list # 查看指定 模块的帮助$ ansible-doc MOD_NAME# 查看模块的示例$ ansible-doc MOD_NAME -s 5.3 ansible-galaxyansible-galaxy : 创建 role 初始文件和目录 5.3.1 创建初始 role 文件和目录$ ansible-galaxy init -p playbook/roles web -p /path/to/roles : 指定 roles 的目录, 未指定则为当前目录. 5.3.2 从 role 仓库中检索, 安装,删除 role.ansible-galaxy [delete|import|info|init|install|list|login|remove|search|setup] [--help] [options] # 检索 $ ansible-galaxy search ntp # 安装 $ ansible-galaxy install -p ./roles bennojoy.ntp # 列出 $ ansible-galaxy list # 删除 $ ansible-galaxy remove bennojoy.ntp 5.4 ansible-vaultansible-vault 用于创建和编辑加密文件, ansible-playbook 可以自动识别并使用密码解密这些文件. 5.4.1 ansible-vault 命令$ ansible-vault [create|decrypt|edit|encrypt|encrypt_string|rekey|view] [--help] [options] vaultfile.yml SubCmd: - encrypt : 加密 - decrypt : 解密 - create : 创建 - edit : 编辑 - view : 查看 - rekey : 修改密码 Options: --ask-vault-pass : ask for vault password --new-vault-password-file=NEW_VAULT_PASSWORD_FILE : new vault password file for rekey --output=OUTPUT_FILE : output file name for encrypt or decrypt; use - for stdout --vault-password-file=VAULT_PASSWORD_FILE : vault password file -v, --verbose : verbose mode (-vvv for more, -vvvv to enable connection debugging) --version : show program&apos;s version number and exit 5.4.2 与playbook 结合的使用 在 playbook 中引用 vault 文件: 可以在 vars_file 区段像一般文件一样易用 vault 加密的文件. 即, 如果加密了一个 file 文件, 在 playbook 中也无需修改. ansible-playbook 使用用 --ask-value-pass 或 --vault-password-file 参数 $ ansible-playbook myplay.yml --ask-value-pass # password.file 可以为文本文件, 如果该为文件可执行脚本, 则 ansible 使用它的标准输出内容作为密码 $ ansible-playbook myplay.yml --vault-password-file /path/to/password.file 5.5 ansible-playbook5.5.1 命令行参数Usage: ansible-playbook playbook.yml Options: --ask-vault-pass ask for vault password -C, --check don&apos;t make any changes; instead, try to predict some of the changes that may occur -D, --diff when changing (small) files and templates, show the differences in those files; works great with --check -e EXTRA_VARS, --extra-vars=EXTRA_VARS set additional variables as key=value or YAML/JSON --flush-cache clear the fact cache --force-handlers run handlers even if a task fails -f FORKS, --forks=FORKS specify number of parallel processes to use (default=5) -i INVENTORY, --inventory-file=INVENTORY specify inventory host path (default=./hosts) or comma separated host list. -l SUBSET, --limit=SUBSET further limit selected hosts to an additional pattern --list-hosts outputs a list of matching hosts; does not execute anything else --list-tags list all available tags --list-tasks list all tasks that would be executed -M MODULE_PATH, --module-path=MODULE_PATH specify path(s) to module library (default=None) --new-vault-password-file=NEW_VAULT_PASSWORD_FILE new vault password file for rekey --output=OUTPUT_FILE output file name for encrypt or decrypt; use - for stdout --skip-tags=SKIP_TAGS only run plays and tasks whose tags do not match these values --start-at-task=START_AT_TASK start the playbook at the task matching this name --step one-step-at-a-time: confirm each task before running --syntax-check perform a syntax check on the playbook, but do not execute it -t TAGS, --tags=TAGS only run plays and tasks tagged with these values --vault-password-file=VAULT_PASSWORD_FILE vault password file -v, --verbose verbose mode (-vvv for more, -vvvv to enable connection debugging) Connection Options: control as whom and how to connect to hosts -k, --ask-pass ask for connection password --private-key=PRIVATE_KEY_FILE, --key-file=PRIVATE_KEY_FILE use this file to authenticate the connection -u REMOTE_USER, --user=REMOTE_USER connect as this user (default=root) -c CONNECTION, --connection=CONNECTION connection type to use (default=smart) -T TIMEOUT, --timeout=TIMEOUT override the connection timeout in seconds (default=10) --ssh-common-args=SSH_COMMON_ARGS specify common arguments to pass to sftp/scp/ssh (e.g. ProxyCommand) --sftp-extra-args=SFTP_EXTRA_ARGS specify extra arguments to pass to sftp only (e.g. -f, -l) --scp-extra-args=SCP_EXTRA_ARGS specify extra arguments to pass to scp only (e.g. -l) --ssh-extra-args=SSH_EXTRA_ARGS specify extra arguments to pass to ssh only (e.g. -R) Privilege Escalation Options: control how and which user you become as on target hosts -s, --sudo run operations with sudo (nopasswd) (deprecated, use become) -U SUDO_USER, --sudo-user=SUDO_USER desired sudo user (default=root) (deprecated, use become) -S, --su run operations with su (deprecated, use become) -R SU_USER, --su-user=SU_USER run operations with su as this user (default=root) (deprecated, use become) -b, --become run operations with become (does not imply password prompting) --become-method=BECOME_METHOD privilege escalation method to use (default=sudo), valid choices: [ sudo | su | pbrun | pfexec | doas | dzdo | ksu | runas ] --become-user=BECOME_USER run operations as this user (default=root) --ask-sudo-pass ask for sudo password (deprecated, use become) --ask-su-pass ask for su password (deprecated, use become) -K, --ask-become-pass ask for privilege escalation password 部分示例 -e var=valur : 传递变量给 playbook # 列出主机, 但不会执行 playbook $ ansible-playbook -i hosts --list-hosts web-tls.yml # 语法检查 $ ansible-playbook --syntax-check web-tls.yml # 列出 task, 但不会执行 playbook $ ansible-playbook -i hosts --list-tasks web-tls.yml # 检查模式, 会检测 playbook 中的每个任务是否会修改主机的状态, 但并不会对主机执行任何实际操作. # 需要注意 playbook 中的 task 中的依赖关系, 可能会报错. $ ansible-playbook [ -C | --check ] web-tls.yml # diff 将会为任何变更远程主机状态的文件输出差异信息. 与 --check 结合尤其好用. $ ansible-playbook [ -D | --diff ] playbook.yml 5.5.2 ansible-playbook 控制 task 的执行 step --step 参数会在执行每个 task 之前都做提示. $ ansible-playbook --step playbook.yml Perform task: install package (y/n/c) : y : 执行 n : 不执行, 跳过 c : 继续执行剩下 playbook , 并不再提示. start-at-task --start-at-task 用于让 ansible 从指定 task 开始运行 playbook, 而不是从头开始. 常用于 playbook 中存在 bug , 修复之后, 从bug处再次重新运行. tags ansible 允许对一个 task 或者 play 添加一个或多个 tags, 如: - hosts: myservers tags: - foo tasks: - name: install packages apt: name={{ item }} with_items: - vim - emacs - nano - name: run arbitrary command command: /opt/myprog tags: - bar - quux -t TAG_NAME 或 --tags TAG_NAME 告诉 ansible 仅允许具有指定 tags 的 task 或 play. --skip-tags TAG_NAME 告诉 ansible 跳过具有指定 tags 的 task 或者 play. # 仅允许指定 tags 的 task/play $ ansible-playbook -t foo,bar playbook.yml $ ansible-playbook --tags=foo,bar playbook.yml # 跳过指定 tags 的 task/play $ ansible-playbook --skip-tags=baz,quux playbook.yml 5.6 ansible-consoleREPL console for executing Ansible tasks $ ansible-console [&lt;host-pattern&gt;] [options] 5.7 ansible-configView, edit and manage ansible configuratin $ ansible-config [view|dump|list] [--help] [options] [ansible.cfg] 5.8 ansible-inventoryused to display or dump the configured inventory as Ansible sees it. $ ansible-inventory [options] [host|group] 5.9 ansible-pullpulls playbooks from a VCS repo and executes them for the local host. $ ansible-pull -U &lt;repository&gt; [options] [&lt;playbook.yml&gt;] 6. ansible 优化加速6.1 SSH Multiplexing (ControlPersist)原理 : 第一次尝试 SSH 连接到远程主机时, OpenSSH 创建一个主链接 OpenSSH 创建一个 UNIX 域套接字(控制套接字), 通过主链接与远程主机相连接 在 ControlPersist 超时时间之内, 再次连接到该远程主机, OpenSSH 将使用控制套接字与远程主机通信, 而不创建新的 TCP 连接, 省去了 TCP 三次握手的时间. Ansible 支持的 SSH Multiplexing 选项列表: 选项 值 说明 ControlMaster auto 开启 ControlPersist ControlPath $HOME/.ansible/cp/ansible-ssh-%h-%p-%r UNIX 套接字文件存放路径, 操作系统对 套接字 的最大长度有限制, 所以太长的套接字, 则 ControlPersist 将不工作, 并且 Ansible 不会报错提醒. ControlPersist 60s SSH 套接字连接空闲时间, 之后关闭 如果启用了 SSH Multiplexing 设置, 并且变更了 SSH 连接的配置, 如修改了 ssh_args 配置项, 那么, 新配置对于之前连接打开的未超时的控制套接字不会生效. 6.2 fact 缓存 关闭 fact 缓存 # ansible.cfg [defaults] gathering=explicit 开启 fact 缓存 请确保, playbook 中没有指定 gather_facts: True 或 gather_facts: False 配置项. # ansible.cfg [defaults] gathering=smart # 缓存过期时间, 单位 秒 fact_cache_timeout=86400 # 缓存实现机制. fact_caching=... JSON ansible 将fact 缓存写入到 JSON 文件中, Ansible 使用文件修改时间来决定fact 缓存是否过期. # ansible.cfg [defaults] gathering=smart # 缓存过期时间, 单位 秒 fact_cache_timeout=86400 fact_caching = jsonfile # 指定 fact 缓存文件保存目录 fact_caching_connection = /tmp/ansible_fact/cache redis 需要安装 redis 包, $ pip install redis; 需要本机提供 redis 服务. # ansible.cfg [defaults] gathering=smart # 缓存过期时间, 单位 秒 fact_cache_timeout=86400 fact_caching = redis memcache 需要安装 python-memcached 包, $ pip install python-memcached; 需要本机提供 memcached 服务. # ansible.cfg [defaults] gathering=smart # 缓存过期时间, 单位 秒 fact_cache_timeout=86400 fact_caching = memcached 希望在 playbook 云子能够之前清除 fact 缓存, 使用 --flush-cache 参数 6.3 pipeline原理: 默认执行 task 步骤: 首先, 基于调用的 module 生成一个 python 脚本; 将 Python 脚本复制到远程主机; 最后, 执行 Python 脚本. 将产生两个 SSH 会话. pipeline 模式 : Ansible 执行 Python 脚本时, 并不复制他, 而是通过管道传递给 SSH 会话, 从而减少了 SSH 会话的数目, 节省时间. 配置: 控制主机开启 pipeline # ansible.cfg [defaults] pipeline=True 远程主机 /etc/sudoers 中的 requiretty 没有启用. Defaults: !requiretty 6.4 并发 设置 ANSIBLE_FORKS 环境变量 修改 ansible.cfg 配置文件, forks = 20.]]></content>
      <categories>
        <category>DevOps</category>
      </categories>
      <tags>
        <tag>ansible</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ansible task/role/playbook]]></title>
    <url>%2F2018%2F03%2F15%2FAnsible-task-role-playbook%2F</url>
    <content type="text"><![CDATA[使用 Ansible task, role, playbook 定义任务, 实现自动化服务器管理. Ansible 学习总结 task name : 可选配置, 用于提示task 的功能. 另, ansible-playbook –start-at-task &lt;task_name&gt; 可以调用 name, 从 task 的中间开始执行. 模块(功能) : 必选配置, 有模块的名称组成的 key 和 模块参数组成的 value. 从 Ansible 前段所使用的 YAML 解析器角度看, 参数将被按照字符串处理, 而不是字典 apt: name=nginx update_cache=yes 复杂参数: ansible 提供一个将模块调用分隔成多行的选择, 可以传递 key 为变量名的字典, 而不是传递字符串参数. 这种方式, 在调用拥有复杂参数的模块时, 十分有用. 如 ec2 模块. - name: install pip pkgs pip: name: &quot;{{ item.name }}&quot; version: &quot;{{ item.version }}&quot; virtualenv: &quot;{{ venv_path }}&quot; with_items: - {name: mazzanine, version: 3.1.10} - {name: gunicorn, version: 19.1.1} environment : 设置环境变量 传入包含变量名与值的字典, 来设置环境变量. - name: set the site id script: scripts/setsite.py environment: PATH: &quot;{{ venv_path }}/bin&quot; PROJECT_DIR: &quot;{{ proj_path }}&quot; ADMIN_PASS: &quot;{{ admin_pass }}&quot; sudo, sudo_user notify 触发 handler 任务. when 当 when 表达式返回 True 时, 执行该 task , 否则不执行. local_action : 运行本地任务 在控制主机本机上(而目标主机)执行命令. 如果目标主机为多台, 那么, local_action 执行的task 将执行多次, 可以指定 run_once , 来限制 local_action 的执行次数. # 调用 wait_for 模块 : 注: inventory_hostname 的值仍然是远程主机, 因为这些变量的范围仍然是远程主机, 即使 task 在本机执行. - name: wait for ssh server to be running local_action: wait_for port=22 host=&quot;{{ inventory_hostname }}&quot; search_regex=OpenSSH # 调用 command 模块 - name: run local cmd hosts: all gather_facts: False tasks: - name: run local shell cmd local_action: command touch /tmp/new.txtxt delegate_to: 在涉及主机之外的主机上运行 task 使用场景: 在报警主机中, 启用基于主机的报警, 如 Nagios 向负载均衡器中, 添加一台主机, 如 HAProxy # 配置 Nagios 示例, inventory_hostname 仍然指 web 主机, 而非 nagios_server.example.com . - name: enable alerts for web servers hosts: web tasks: - name: enable alerts nagios: action=enanle_alerts service=web host={{ inventory_hostname }} delegate_to: nagios_server.example.com --- # This playbook does a rolling update for all webservers serially (one at a time). # Change the value of serial: to adjust the number of server to be updated. # # The three roles that apply to the webserver hosts will be applied: common, # base-apache, and web. So any changes to configuration, package updates, etc, # will be applied as part of the rolling update process. # # gather facts from monitoring nodes for iptables rules - hosts: monitoring tasks: [] - hosts: webservers serial: 1 # These are the tasks to run before applying updates: pre_tasks: - name: disable nagios alerts for this host webserver service nagios: &apos;action=disable_alerts host={{ inventory_hostname }} services=webserver&apos; delegate_to: &quot;{{ item }}&quot; with_items: groups.monitoring - name: disable the server in haproxy haproxy: &apos;state=disabled backend=myapplb host={{ inventory_hostname }} socket=/var/lib/haproxy/stats&apos; delegate_to: &quot;{{ item }}&quot; with_items: groups.lbservers roles: - common - base-apache - web # These tasks run after the roles: post_tasks: - name: wait for webserver to come up wait_for: &apos;host={{ inventory_hostname }} port=80 state=started timeout=80&apos; - name: enable the server in haproxy haproxy: &apos;state=enabled backend=myapplb host={{ inventory_hostname }} socket=/var/lib/haproxy/stats&apos; delegate_to: &quot;{{ item }}&quot; with_items: groups.lbservers - name: re-enable nagios alerts nagios: &apos;action=enable_alerts host={{ inventory_hostname }} services=webserver&apos; delegate_to: &quot;{{ item }}&quot; with_items: groups.monitoring run_once : 值为 True/False 该 task 是否只运行一次, 与 local_action 配合十分好用. changed_when &amp; failed_when 使用 changed_when 和 failed_when 语句改变 Ansible 对 task 是 chenged 状态还是 failed 状态的认定.需要了解命令的输出结果 - name: initialize the database django_manage: command: createdb --noinput --nodata app_path: &quot;{{ proj_path }}&quot; virtualenv: &quot;{{ venv_path }}&quot; register: result changed_when: not result.failed and &quot;Creating tables&quot; in result.out failed_when: result.failed and &quot;Database already created&quot; not in result.msg 循环 playbook 执行后, 跟踪主机状态.playplay 可以想象为连接到主机(host)上执行任务(task)的事务. 选项: host : 必选配置, 需要配置的一组主机 task : 必选配置, 需要在主机上执行的任务 name : 可选配置, 一段注释, 用来描述 play 的功能, ansible 在 play 开始执行的时候, 会把 name 打印出来. sudo : 可选配置, 如果为真, ansible 会在运行每个 task 的时候, 都是用 sudo 命令切换为 (默认) root. vars : 可选配置, 变量与其值组成的列表. 任何合法的 YAML 对象都可以作为变量的值. 变量不仅可以在 tasks 中使用, 还可以在 模板文件 中使用. vars_files : 可选, 把变量放到一个或者多个文件中. gather_facts : 是否收集 fact. handlers : 可选, ansible 提供的 条件机制, 和 task 类似, 但只有在被 task 通知的时候才会运行. 如果 ansible 识别到 task 改变了系统的状态, task 就会触发通知机制. task 将 handler 的名字作为参数传递, 依此来通知 handler. handler 只会在所有任务执行完成之后执行, 而且即使被通知了多次, 也只会执行一次. handler 按照play 中定义的顺序执行, 而不是被通知的顺序. handler 常见的用途就是重启服务和重启服务器. serial, max_fail_percentage 默认情况下, Ansible 会并行的在所有相关联主机上执行每一个 task. 可以使用 serial 限制并行执行 play 的主机数量. 一般来说, 当 task 失败时, Ansible 会停止执行失败的那台主机上的任务, 但是继续对其他主机执行. 在负载均衡场景中, 可能希望 Ansible 在所有主机都发生失败前让整个 play 停止执行, 否则将会导致, 所有主机都从 负载均衡器上移除, 并且全部执行失败, 最终负载均衡器上没有任何主机的局面. 此时, 可以使用 serial 和 max_fail_percentage 语句来指定, 最大失败主机比例达超过 max_fail_percentage 时, 让整个 play 失败. 如果希望 Ansible 在任何主机出现 task 执行失败的时候, 都放弃执行, 则需要设置max_fail_percentage=0. - name: upgrade packages on servers behind load balancer hosts: myhosts serial: 1 max_fail_percentage: 25 tasks: - name: get the ec2 instance id and elastic load balancer id ec2_facts: - name: task the out of the elastic load balancer local_action: ec2_elb args: instance_id: &quot;{{ ansible_ec2_instance_id }}&quot; state: absent - name: upgrade packages apt: update_cache=yes upgrade=yes - name: put the host back in the elastic load balancer local_action: ec2_elb args: instance_id: &quot;{{ ansible_ec2_instance_id }}&quot; state: present ec2_elbs: &quot;{{ item }}&quot; with_items: ec2_elbs roles pre-task post-task rolerole 是将 playbook 分隔为多个文件的主要机制, 他大大简化了复杂 playbook 的编写, 同时使得 role 更加易于复用. role 的基本构成.每个 role 都会用一个名字, 如 ‘database’, 与该 role 相关的文件都放在 roles/database 目录下. 其结构如下: 每个单独文件都是可选的 task: task 定义 roles/database/tasks/main.yml files 需要上传到目标主机的文件: roles/database/files/ templates Jinja2 模板文件 roles/database/templates handlers handler roles/database/handler/main.yml vars 不应被覆盖的变量 roles/database/vars/main.yml defaults 可以被覆盖的默认变量 roles/database/default/main.yml meta role 的依赖信息 roles/database/meta/main.yml default 变量与 vars 变量: default : 希望在 role 中变更变量的值. vars : 不希望变量的值变更. role 中变量命名的一个良好实践: 变量建议以 role 的名称开头, 因为在 Ansible 中不同的 role 之间没有命名空间概念, 这意味着在其他 role 中定义的变量, 或者再 playbook 中其他地方定义的变量, 可以在任何地方被访问到. 如果在两个不同的 role 中使用了同名的变量, 可能导致意外的行为. role 的存放位置 playbook 并列的 roles 目录下; /etc/ansible/roles/ 下 ansible.cfg 中 default 段 roles_path 指向的位置 环境变量 ANSIBLE_ROLES_PATH 指向的位置 在 playbook 中使用 role# mezzaning-single-host.yml - name: deploy mezzanine on vagrant hosts: web vars_file: - secrets.yml roles: - role: database database_name: &quot;{{ mezzanine_proj_name }}&quot; # 定义覆盖变量 database_pass: &quot;{{ mezzanine_proj_name }}&quot; # 定义覆盖变量 - role: mezzanine live_hostname: 192.168.33.10.xip.io domains: - 192.168.33.10.xip.io - www.192.168.33.10.xip.io # mezzaning-across-host.yml - name: deploy postgres vagrant hosts: db vars_files: - secrets.yml roles: - role: database database_name: &quot;{{ mezzanine_proj_name }}&quot; # 定义覆盖变量 database_pass: &quot;{{ mezzanine_proj_name }}&quot; # 定义覆盖变量 - name: deploy mezzanine on vagrant hosts: web vars_files: - secrets.yml roles: - role: mezzanine database_host: &quot;{{ hostvars.db.ansible_eth1.ipv4.address }}&quot; live_hostname: 192.168.33.10.xip.io domains: - 192.168.33.10.xip.io - www.192.168.33.10.xip.io pre-task &amp; post-taskpre-task : 定义在 role 执行之前, 执行的 taskpost-task : 定义在 role 执行之后, 执行的 task. - name: deploy mezzanine on vagrant hosts: web vars_files: - secrets.yml pre_tasks: - name: update the apt cache apt: update_cache=yes roles: - role: mezzanine database_host: &quot;{{ hostvars.db.ansible_eth1.ipv4.address }}&quot; live_hostname: 192.168.33.10.xip.io domains: - 192.168.33.10.xip.io - www.192.168.33.10.xip.io post_tasks: - name: notify Slack that the servers have been updated local_action:&gt; slack domain=acme.slack.com token={{ slack_token }} msg=&quot;web server {{ inventory_hostname }} configured&quot; inclued用于调用位于同一目录下的其他 定义文件, 可用于 Tasks,Playbook, Vars, Handler, Files 等. # task example --- - name: install apt packages apt: pkg={{ item }} update_cache=yes cache_valid_time=3600 sudo: True with_items: - git - libjpeg-dev - libpq-dev - memcached - nginx - include: django.yml - include: nginx.yml # example 2 --- - name: check host environment include: check_environment.yml - name: include OS family/distribution specific variables include_vars: &quot;{{ item }}&quot; with_first_found: - &quot;../defaults/{{ ansible_distribution | lower }}-{{ ansible_distribution_version | lower }}.yml&quot; - &quot;../defaults/{{ ansible_distribution | lower }}.yml&quot; - &quot;../defaults/{{ ansible_os_family | lower }}.yml&quot; - name: debug variables include: debug.yml tags: - debug ansible-galaxy : 创建 role 初始文件和目录 创建初始 role 文件和目录 $ ansible-galaxy init -p playbook/roles web -p /path/to/roles : 指定 roles 的目录, 未指定则为当前目录. 从 role 仓库中检索, 安装,删除 role. ansible-galaxy [delete|import|info|init|install|list|login|remove|search|setup] [--help] [options] 检索 $ ansible-galaxy search ntp 安装 $ ansible-galaxy install -p ./roles bennojoy.ntp 列出 $ ansible-galaxy list 删除 $ ansible-galaxy remove bennojoy.ntp 在线网站 https://galaxy.ansible.com dependent role:dependent role 用于指定 role 依赖的其他一个或多个 role, Ansible 会确保被指定依赖的role 一定会优先被执行. Ansible 允许向 dependent role 传递参数 dependent role 一般在 myrole/meta/main.yml 中指定. # roles/web/meta/main.yml dependencies: - { role: ntp, ntp_server=ntp.ubuntu.com } - { role: common } - { role: memcached } playbook : 用于实现 ansible 配置管理的脚本.playbook 其实就是一个字典组成的列表. 一个 playbook 就是一组 play 组成的列表. 一个 play 由 host 的无序集合与 task 的有序列表组成. 每一个 task 由一个模块构成. ansible 中的 True/False 和 yes/no模块参数(如 update_cache=yes)对于值的处理, 使用字符串传递: 真值 yes,on,1,true 假值 no,off,0,false 其他使用 YAML 解析器来处理: 真值 true,True,TRUE,yes,Yes,YES,on,On,ON,y,Y 假值 false,False,FALSE,no,No,NO,off,Off,OFF,n,N 推荐做法: 模块参数: yes/no 其他地方: True,False playbook 文件的执行方法: 使用 ansible-playbook 命令 $ ansible-playbook myplaybook.yml shebang $ chmod +x myplaybook.yml $ head -n 1 myplaybook.yml #!/usr/bin/env ansible-playbook $ ./myplaybook.yml 当 Ansible 开始运行 playbook 的时候, 他做的第一件事就是从他连接到的服务器上收集各种信息. 这些信息包括操作系统,主机名,网络接口等. 建立 nginx web 服务器$ cat web-notls.yml - name: Configure webserver with nginx and tls hosts: webservers sudo: true vars: key_file: /etc/nginx/ssl/nginx.key cert_file: /etc/nginx/ssl/nginx.crt conf_file: /etc/nginx/sites-available/default server_name: localhost tasks: - name: install nginx apt: name=nginx update_cache=yes cache_valid_time=3600 - name: create directories for ssl certificates file: path=/etc/nginx/ssl state=directory - name: copy TLS key copy: src=files/nginx.key desc={{ key_file }} owner=root mode=06-- notify: restart nginx - name: copy TLS certificate copy: src=files/nginx.crt dest={{ cert_file }} notify: restart nginx - name: copy nginx config file copy: src=files/nginx.conf.j2 dest={{ conf_file }} notify: restart nginx - name: enable configuration file: dest=/etc/nginx/sites-enabled/default src={{ conf_file }} state=link notify: restart nginx - name: copy index.html template: src=templates/index.html.j2 dest=/usr/share/nginx/html/index.html mode=0644 handlers: - name: restart nginx service: name=nginx state=restarted 内部变量 ansible_managed : 和模板文件生成时间相关的信息. inventory 文件使用 .ini 格式, 默认为 hosts 文件. [webservers] testserver ansible_ssh_host=127.0.0.1 ansible_ssh_port=22 YAML 文件格式 文件开始. --- 如果没有---标记, 也不影响 ansible 的运行. 注释: # 字符串 : 即使字符串中有空格, 也无需使用引号. 布尔型 : 有多种, 推荐使用 True/False 列表: 使用-作为分隔符 标准列表 - My Fair Lady - Oklahoma - The Pirates of Penzance 内联式列表 [My Fair Lady, Oklahoma, The Pirates of Penzance] 字典: 标准字典 name: tom age: 12 job: manager 内联式字典 {name: tom, age: 12, job: manager} 折行: 使用大于号(&gt;)表示折行]]></content>
      <categories>
        <category>DevOps</category>
      </categories>
      <tags>
        <tag>ansible</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ansible-配置文件]]></title>
    <url>%2F2018%2F03%2F15%2FAnsible-%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[Ansible 配置文件 ansible.cfg 参数总结.Ansible 学习总结 #ansible.cfg 有 defaults, ssh_connection,paramiko,accelerate四个配置段. 1. defaults 段 配置名称 环境变量 默认值 hostfile ANSIBLE_HOSTS /etc/ansible/hosts library ANSIBLE_LIBRARY (none) roles_path ANSIBLE_ROLES_PATH /etc/ansible/roles remote_tmp ANSIBLE_REMOTE_TEMP $HOME/.ansible/tmp module_name (none) command pattern (none) * forks ANSIBLE_FORKS 5 module_args ANSIBLE_MODULE_ARGS (empty string) module_lang ANSIBLE_MODULE_LANG en_US.UTF-8 timeout ANSIBLE_TIMEOUT 10 poll_interval ANSIBLE_POLL_INTERVAL 15 remote_user ANSIBLE_REMOTE_USER current user ask_pass ANSIBLE_ASK_PASS false private_key_file ANSIBLE_PRIVATE_KEY_FILE (none) sudo_user ANSIBLE_SUDO_USER root ask_sudo_pass ANSIBLE_ASK_SUDO_PASS false remote_port ANSIBLE_REMOTE_PORT (none) ask_vault_pass ANSIBLE_ASK_VAULT_PASS false vault_password_file ANSIBLE_VAULT_PASSWORD_FILE (none) ansible_managed (none) Ansible managed: { file} modi ed on %Y-%m-%d %H:%M:%S by {uid} on {host} syslog_facility ANSIBLE_SYSLOG_FACILITY LOG_USER keep_remote_ les ANSIBLE_KEEP_REMOTE_FILES true sudo ANSIBLE_SUDO false sudo_exe ANSIBLE_SUDO_EXE sudo sudo_flags ANSIBLE_SUDO_FLAGS -H hash_behaviour ANSIBLE_HASH_BEHAVIOUR replace jinja2_extensions ANSIBLE_JINJA2_EXTENSIONS (none) su_exe ANSIBLE_SU_EXE su su ANSIBLE_SU false su_flags ANSIBLE_SU_FLAGS (empty string) su_user ANSIBLE_SU_USER root ask_su_pass ANSIBLE_ASK_SU_PASS false gathering ANSIBLE_GATHERING implicit action_plugins ANSIBLE_ACTION_PLUGINS /usr/share/ansible_plugins/action_plugins cache_plugins ANSIBLE_CACHE_PLUGINS /usr/share/ansible_plugins/cache_plugins callback_plugins ANSIBLE_CALLBACK_PLUGINS /usr/share/ansible_plugins/callback_plugins connection_plugins ANSIBLE_CONNECTION_PLUGINS /usr/share/ansible_plugins/connection_plugins lookup_plugins ANSIBLE_LOOKUP_PLUGINS /usr/share/ansible_plugins/lookup_plugins vars_plugins ANSIBLE_VARS_PLUGINS /usr/share/ansible_plugins/vars_plugins filter_plugins ANSIBLE_FILTER_PLUGINS /usr/share/ansible_plugins/ lter_plugins log_path ANSIBLE_LOG_PATH (empty string) fact_caching ANSIBLE_CACHE_PLUGIN memory fact_caching_connection ANSIBLE_CACHE_PLUGIN_CONNECTION (none) fact_caching_prefix ANSIBLE_CACHE_PLUGIN_PREFIX ansible_facts fact_caching_timeout ANSIBLE_CACHE_PLUGIN_TIMEOUT 86400 (seconds) force_color ANSIBLE_FORCE_COLOR (none) nocolor ANSIBLE_NOCOLOR (none) nocows ANSIBLE_NOCOWS (none) display_skipped_hosts DISPLAY_SKIPPED_HOSTS true error_on_unde ned_vars ANSIBLE_ERROR_ON_UNDEFINED_VARS true host_key_checking ANSIBLE_HOST_KEY_CHECKING true system_warnings ANSIBLE_SYSTEM_WARNINGS true deprecation_warnings ANSIBLE_DEPRECATION_WARNINGS true callable_whitelist ANSIBLE_CALLABLE_WHITELIST (empty list) command_warnings ANSIBLE_COMMAND_WARNINGS false bin_ansible_callbacks ANSIBLE_LOAD_CALLBACK_PLUGINS false 2. ssh_connection 段 配置名称 环境变量 默认值 ssh_args ANSIBLE_SSH_ARGS -o ControlMaster=auto -o ControlPersist=60s -o ControlPath=&quot;$ANSIBLE_SSH_CONTROL_PATH” control_path ANSIBLE_SSH_CONTROL_PATH %(directory)s/ansible-ssh-%%h-%%p-%%r pipelining ANSIBLE_SSH_PIPELINING false scp_if_ssh ANSIBLE_SCP_IF_SSH false 3. paramiko 段 配置名称 环境变量 默认值 record_host_keys ANSIBLE_PARAMIKO_RECORD_HOST_KEYS true pty ANSIBLE_PARAMIKO_PTY true 4. accelerate 段 (不推荐使用)]]></content>
      <categories>
        <category>DevOps</category>
      </categories>
      <tags>
        <tag>ansible</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ansible-API]]></title>
    <url>%2F2018%2F03%2F15%2FAnsible-API%2F</url>
    <content type="text"><![CDATA[ansible api 开发篇Ansible 学习总结 ansible api Callbacks Inventory Playbook Script]]></content>
      <categories>
        <category>DevOps</category>
      </categories>
      <tags>
        <tag>ansible</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ansible-模块]]></title>
    <url>%2F2018%2F03%2F15%2FAnsible-%E6%A8%A1%E5%9D%97%2F</url>
    <content type="text"><![CDATA[Ansible 常用模块使用方法 自定义 Ansible 模块 Ansible 学习总结 1. 内置模块是由 ansible 包装后, 在主机上执行一系列操作的脚本. 1.1 查看模块帮助$ ansible-doc MOD_NAME 1.2 查找第三方模块$ ansible-galaxy search MOD_NAME 1.3 常用模块apt update_cache=yes 在安装软件之前, 首先更新 repo 缓存. cache_valid_time=3600 上次 repo 缓存的有效时间. upgrade=yes pipAnsible 的 pip 模块支持向 virtualenv 中安装软件包, 并且还支持在没有可用的 virtualenv 时, 自动创建一个. - name: install required python packages pip: name={{ item }} virtualenv={{ venv_path }} with_items: - gunicorn - django - django-compressor 支持 requirements 文件 - name: install required python pkg pip: requirements={{ proj_path }}/{{ reqs_file }} virtualenv={{ venv_path }} Options : chdircd into this directory before running the command [Default: None] editablePass the editable flag for versioning URLs. [Default: True] executableThe explicit executable or a pathname to the executable to be used to run pip for a specific version of Python installed in the system. For example `pip-3.3&apos;, if there are both Python 2.7 and 3.3 installations in the system and you want to run pip for the Python 3.3 installation. It cannot be specified together with the &apos;virtualenv&apos; parameter (added in 2.1). By default, it will take the appropriate version for the python interpreter use by ansible, e.g. pip3 on python 3, and pip2 or pip on python 2. [Default: None] extra_argsExtra arguments passed to pip. [Default: None] nameThe name of a Python library to install or the url of the remote package. As of 2.2 you can supply a list of names. [Default: None] requirementsThe path to a pip requirements file, which should be local to the remote system. File can be specified as a relative path if using the chdir option. [Default: None] stateThe state of module The &apos;forcereinstall&apos; option is only available in Ansible 2.1 and above. (Choices: present, absent, latest, forcereinstall)[Default: present] umaskThe system umask to apply before installing the pip package. This is useful, for example, when installing on systems that have a very restrictive umask by default (e.g., 0077) and you want to pip install packages which are to be used by all users. Note that this requires you to specify desired umask mode in octal, with a leading 0 (e.g., 0077). [Default: None] versionThe version number to install of the Python library specified in the `name&apos; parameter [Default: None] virtualenvAn optional path to a `virtualenv&apos; directory to install into. It cannot be specified together with the &apos;executable&apos; parameter (added in 2.1). If the virtualenv does not exist, it will be created before installing packages. The optional virtualenv_site_packages, virtualenv_command, and virtualenv_python options affect the creation of the virtualenv. [Default: None] virtualenv_commandThe command or a pathname to the command to create the virtual environment with. For example `pyvenv&apos;, `virtualenv&apos;, `virtualenv2&apos;, `~/bin/virtualenv&apos;, `/usr/local/bin/virtualenv&apos;. [Default: virtualenv] virtualenv_pythonThe Python executable used for creating the virtual environment. For example `python3.5&apos;, `python2.7&apos;. When not specified, the Python version used to run the ansible module is used. [Default: None] virtualenv_site_packagesWhether the virtual environment will inherit packages from the global site-packages directory. Note that if this setting is changed on an already existing virtual environment it will not have any effect, the environment must be deleted and newly created. (Choices: yes, no)[Default: no] copyfileservicetemplatesetup实现 fact 收集的模块. 一般无需再 playbook 中调用该模块, Ansible 会在采集 fact 时, 自动调用. $ ansible server_name -m setup -a &#39;filter=ansible_eth*&#39; 其返回值为一个字典, 字典的 key 是 ansible_fact, 他的 value 是一个有实际 fact 的名字与值组成的字典. setup 模块支持 filter 参数, 可以实现 shell 通配符的匹配过滤. - name: gather facts setup: set_fact使用 set_fact 模块在 task 中设置 fact(与定义一个新变量是一样的). 可以在 register 关键字后, 立即使用 set_fact , 这样使得变量引用更简单. - name: get snapshot id shell: &gt; aws ec2 describe-snapshot --filters Name=tag:Name, Valuse=my-snapshot | jq --raw-outpuy &quot;.Snapshots[].SnapshtId&quot; register: snap_result - set_fact: snap={{ snap_result.stdout }} - name: delete old snapshot command: aws ec2 delete-snapshot --snapshot-id &quot;{{ snap }}&quot; command在 command 中保持幂等性的方法: 指定 creates 参数. # 当 Vagrantfile 存在, 则表示已经处于正确状态, 而且不需要再次执行命令, 从而实现幂等性. - name: create a vagrantfile command: vagrant init {{ box }} creates=Vagrantfile 官方文档: - creates a filename or (since 2.0) glob pattern, when it already exists, this step will *not* be run. [Default: None] - removes a filename or (since 2.0) glob pattern, when it does not exist, this step will *not* be run. [Default: None] script实现幂等性方法: creates 和 removes 参数. 官方文档: - creates a filename, when it already exists, this step will *not* be run. [Default: None] - removes a filename, when it does not exist, this step will *not* be run. [Default: None] debug&gt; DEBUG (/opt/virtualEnv/ansibleEnv/lib/python2.7/site-packages/ansible/modules/utilities/logic/debug.py) This module prints statements during execution and can be useful for debugging variables or expressions without necessarily halting the playbook. Useful for debugging together with the &apos;when:&apos; directive. * note: This module has a corresponding action plugin. Options (= is mandatory): - msg The customized message that is printed. If omitted, prints a generic message. [Default: Hello world!] - var A variable name to debug. Mutually exclusive with the &apos;msg&apos; option. [Default: (null)] - verbosity A number that controls when the debug is run, if you set to 3 it will only run debug when -vvv or above [Default: 0] EXAMPLES: # Example that prints the loopback address and gateway for each host - debug: msg: &quot;System {{ inventory_hostname }} has uuid {{ ansible_product_uuid }}&quot; - debug: msg: &quot;System {{ inventory_hostname }} has gateway {{ ansible_default_ipv4.gateway }}&quot; when: ansible_default_ipv4.gateway is defined - shell: /usr/bin/uptime register: result - debug: var: result verbosity: 2 - name: Display all variables/facts known for a host debug: var: hostvars[inventory_hostname] verbosity: 4 postgresql_userpostgresql_dbdjango_managecron :# 安装 cron job, 注意 name 参数, 该参数必须要有, 该参数将用于删除计划任务时所使用的名称. - name: install poll twitter cron job cron: name=&quot;Poll twitter&quot; minute=&quot;*/5&quot; user={{ user }} job=&quot;{{ manage }} poll_twitter&quot; # 删除计划任务, 基于 name 参数, 在删除时, 会连带注释一起删掉. - name: remote cron job cron: name=&quot;Poll twitter&quot; state=absent git :- name: check out the repository on the host git: repo={{ repo_url }} dest={{ proj_path }} accept_host_key=yes wait_for:You can wait for a set amount of time `timeout’, this is the default if nothing is specified. Waiting for a port to become available is useful for when services are not immediately available after their init scripts return which is true of certain Java application servers. It is also useful when starting guests with the [virt] module and needing to pause until they are ready. This module can also be used to wait for a regex match a string to be present in a file. In 1.6 and later, this module can also be used to wait for a file to be available or absent on the filesystem. In 1.8 and later, this module can also be used to wait for active connections to be closed before continuing,useful if a node is being rotated out of a load balancer pool. Options: active_connection_statesThe list of tcp connection states which are counted as active connections [Default: [u&apos;ESTABLISHED&apos;, u&apos;SYN_SENT&apos;, u&apos;SYN_RECV&apos;, u&apos;FIN_WAIT1&apos;, u&apos;FIN_WAIT2&apos;, u&apos;TIME_WAIT&apos;]] connect_timeoutmaximum number of seconds to wait for a connection to happen before closing and retrying [Default: 5] delaynumber of seconds to wait before starting to poll [Default: 0] exclude_hostslist of hosts or IPs to ignore when looking for active TCP connections for `drained&apos; state [Default: None] hostA resolvable hostname or IP address to wait for [Default: 127.0.0.1] pathpath to a file on the filesytem that must exist before continuing [Default: None] portport number to poll [Default: None] search_regexCan be used to match a string in either a file or a socket connection. Defaults to a multiline regex. [Default: None] sleepNumber of seconds to sleep between checks, before 2.3 this was hardcoded to 1 second. [Default: 1] stateeither `present&apos;, `started&apos;, or `stopped&apos;, `absent&apos;, or `drained&apos; When checking a port `started&apos; will ensure the port is open, `stopped&apos; will check that it is closed, `drained&apos; will check for active connections When checking for a file or a search string `present&apos; or `started&apos; will ensure that the file or string is present before continuing, `absent&apos; will check that file is absent or removed (Choices: present, started, stopped, absent, drained)[Default: started] timeoutmaximum number of seconds to wait for [Default: 300] wait_for_connection : 默认超时时间 600s Waits until remote system is reachable/usable 等待目标主机可以成为 reachable/usable 状态, 即 ssh 22 端口可以连通. - name: Wait 300 seconds, but only start checking after 60 seconds wait_for_connection: delay: 60 # 等待 60s 之后执行 本task timeout: 300 # 超时时间, 默认300s sleep: 2 # 在检查期间, 每次检查之间的间隔时间, 默认为 1s - name: Wait 600 seconds for target connection to become reachable/usable wait_for_connection: wait_for : Waits for a condition before continuing 等待某个主机或端口可用, 适用范围比 wait_for_connection 更加广泛. 可以在本机或目标主机检查其他或本地主机的端口,. - name: Wait 300 seconds for port 22 to become open wait_for: port: 22 sleep: 3 # A resolvable hostname or IP address to wait for. host: &apos;{{ (ansible_ssh_host|default(ansible_host))|default(inventory_hostname) }}&apos; # Can be used to match a string in either a file or a socket connection. search_regex: OpenSSH timeout: 300 # This overrides the normal error message from a failure to meet the required conditions. msg: Timeout to connect through OpenSSH # Either present, started, or stopped, absent, or drained. # When checking a port started will ensure the port is open, stopped will check that it is closed, drained will check for active connections. # When checking for a file or a search string present or started will ensure that the file or string is present before continuing, absent will check that file is absent or removed. state: drained # List of hosts or IPs to ignore when looking for active TCP connections for drained state. exclude_hosts: 10.2.1.2,10.2.1.3 delegate_to: localhost - name: Wait until the process is finished and pid was destroyed wait_for: # Path to a file on the filesystem that must exist before continuing. path: /proc/3466/status state: absent lineinfilestat收集关于文件路径状态的各种信息, 返回一个字典, 该字典包含一个 stat 字段. 部分字段返回值表: 字段 描述 dev inode 所在设备 ID 编号 gid 路径的所属组 ID 编号 inode inode 号 mode 字符串格式的八进制文件模式,如 1777 atime 路径的最后访问时间, 使用 UNIX 时间戳 ctime 路径的创建时间, 使用 UNIX 时间戳, 文件元数据变更时间 mtime 路径的最后修改时间, 使用 UNIX 时间戳 , 文件内容修改时间. nlink 文件硬链接的数量 pw_name 文件所属者的登录名 size 如果是文件, 返回字节单位的文件大小 uid 路径所属者的 uid isblk 如果路径为指定块设备文件, 返回 true ischr 如果路径为指定字符设备文件,返回 true isdir 如果路径为目录, 返回 true isfifo 如果路径为 FIFO(管道), 返回 true isgid 如果文件设置了 setgid , 返回 true isuid 如果文件设置了 setuid , 返回 true islnk 如果文件时符号链接, 返回 true isreg 如果路径是常规文件, 返回 true issock 如果路径是UNIX 域socket, 返回 true rgrp 如果设置所属组可读权限, 返回 true roth 如果设置其他人可读权限, 返回 true rusr 如果设置了属主可读权限, 返回 true wgrp 如果设置所属组可写权限, 返回 true woth 如果设置所属组可写权限, 返回 true wusr 如果设置所属组可写权限, 返回 true xgrp 如果设置所属组可执行权限, 返回 true xoth 如果设置所属组可执行权限, 返回 true xusr 如果设置所属组可执行权限, 返回 true exists 如果存在, 返回 true md5 文件的 md5 值 checksum 文件的hash 值, 可以设置 sha 算法. assertassert 模块在指定的条件不符合是,返回错误, 并失败退出. 主要用于调试.that : 后跟计算表达式msg : 失败后的提示信息. - name: stat /opt/foo stat: path=/opt/foo register: st - name: assert that /opt/foo is a directory assert: that: st.stat.isdir ------- - assert: that: - &quot;my_param &lt;= 100&quot; - &quot;my_param &gt;= 0&quot; msg: &quot;&apos;my_param&apos; must be between 0 and 100&quot; 2. 自定义模块自定义模块存放路径: playbooks/library 2.1 使用 script 自定义 模块2.2 使用 Python 自定义模块.]]></content>
      <categories>
        <category>DevOps</category>
      </categories>
      <tags>
        <tag>ansible</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ansible-变量]]></title>
    <url>%2F2018%2F03%2F15%2FAnsible-%E5%8F%98%E9%87%8F%2F</url>
    <content type="text"><![CDATA[摘要 Ansible 学习总结 变量与 fact在 Ansible 中, 变量的作用域是按照主机划分的, 只有针对特定主机讨论变量的值才有意义. 1. 变量1.1. 定义变量vars : 定义变量的列表或字典vars_file : 指定 定义变量的文件列表 vars 区段的定义, 实际上是在 当前 play 中针对一组主机定义了变量, 但 Ansible 实际做法其实时, 对这个群组的每一个主机创建一个变量的副本. ansible 允许定义于主机或群组有关的变量, 这些变量可以定义在 inventory 文件中, 也可以定义在与 inventory 文件放在一起的独立文件中. Ansible 变量定义位置 变量标识 描述 vars playbook 区段, 为字典列表 vars_file playbook 区段, 为指向文件的列表 host_vars 目录, 主机变量 group_vars 目录, 群组变量 主机变量 inventory中, 单独针对主机的变量 群组变量 inventory中, 单独针对单个群组的变量 1.2. 显示变量: debug 模块- debug: var=myvarname 1.3. register 注册变量: 基于 task 的执行结果, 设置变量的值.示例: - name: Run MyProg command: /opt/myprog register: result ignore_errors: True - debug: var=result ignore_errors 语句, 可以实现, 在 task 失败的时候, 是否忽略错误, 继续执行下面的 task, 默认为 False. 访问变量中字典的key, 有两种方式: { { login.stdout } } { { ansible_eth1[&quot;ipv4&quot;][&quot;address&quot;] } } 当 task 在目标主机, 没有执行命令时, 即当目标主机已经符合目标结果时, 输出中没有 stdout,stderr,stdout_lines 三个键值. 如果在 playbook 中使用了注册变量, 那么无论模块是否改变了主机的状态, 请确保你了解变量的内容, 否则, 当你的 playbook 尝试访问注册变量中不存的 key时, 可能会导致失败. 1.4. set_fact 定义新变量使用 set_fact 模块在 task 中设置 fact(与定义一个新变量是一样的). 可以在 register 关键字后, 立即使用 set_fact , 这样使得变量引用更简单. - name: get snapshot id shell: &gt; aws ec2 describe-snapshot --filters Name=tag:Name, Valuse=my-snapshot | jq --raw-outpuy &quot;.Snapshots[].SnapshtId&quot; register: snap_result - set_fact: snap={ { snap_result.stdout } } - name: delete old snapshot command: aws ec2 delete-snapshot --snapshot-id &quot;{ { snap } }&quot; 1.5. 内置变量 参数 说明 hostvars 字典, key 为 Ansible 主机的名字, value 为所有变量名与相应变量值映射组成的字典 inventory_hostname 当前主机被 Ansible 识别的名字, 如果定义了别名, 则为别名. group_names 列表, 由当前主机所属的所有群组组成 groups 字典, key 为 ansible 群组名, value 为群组成员的主机名所组成的列表. 包括 all 分组和 ungrouped 分组 play_hosts 列表, 成员是当前 play 涉及的主机的 inventory 主机名. ansible_version 字典, 由 Ansible 版本信息组成. hostvars : 在 Ansible 中, 变量的作用域是按照主机划分的, 只有针对特定主机讨论变量的值才有意义. 有时候 , 针对一组主机定义的变量, 该变量实际始于特定的主机相关联的. 例如 vars 区段的定义, 实际上是在 当前 play 中针对一组主机定义了变量, 但 Ansible 实际做法其实时, 对这个群组的每一个主机创建一个变量的副本. hostvars变量包含了在所有主机上定义的所有变量, 并以 ansible 识别的主机名作为 key. 如果 Ansible 还未对主机采集 fact, 那么除非启动 fact 缓存, 否则无法使用 hostvars 访问fact. 有时, 在某一个主机上运行的 task 可能会需要在另一台主机上定义的变量. 例如, web 服务器, 可能需要 数据库服务器的 ansible_eth1.ipv4.address 这个 fact. 如果 数据库服务器为 db.example.com, 那么, 其变量引用为: { { hostvars[&apos;db.example.com&apos;].ansible_eth1.ipv4.address } } - debug: var=hostvars[inventory_hostname] : 输出与当前主机相关联的所有变量. groups : 代表当前 inventory 所定义的所有组的集合, 为一个字典. 示例: web 负载均衡配置文件 backend web-backend {% for host in groups.web %} server { { host.inventory_hostname } } { { host.ansible_default_ipv4.address } }:80 {% endfor %} 1.6. 在命令行设置变量向 ansible-playbook 传入 -e var=value 参数设置变量或传递参数, 有最高优先级. 可以覆盖已定义的变量值. $ ansible-playbook example.yml -e token=123456 希望在变量中出现空格, 需要使用引号: $ ansible-playbook playbooks/greeting.yml -e &apos;greeting=&quot;Oops you have another hello world&quot;&apos; `@filename.yml` 传递参数: $ cat greetvars.yml greeting: &quot;ops you have another hello world&quot; $ ansible-playbook playbooks/greeting.yml -e @greetvars.yml 2. fact当 Ansible 采集 fact 的时候, 他会连接到目标主机收集各种详细信息: CPU 架构,操作系统,IP地址,内存信息,磁盘信息等. 这些信息保存在被称为 fact 的变量中. fact 与其他变量的行为一模一样. 2.1. setup 模块实现 fact 收集的模块. 一般无需再 playbook 中调用该模块, Ansible 会在采集 fact 时, 自动调用. `$ ansible server_name -m setup -a &apos;filter=ansible_eth*&apos;` 其返回值为一个字典, 字典的 key 是 ansible_fact, 他的 value 是一个有实际 fact 的名字与值组成的字典. setup 模块支持 filter 参数, 可以实现 shell 通配符的匹配过滤. 2.2. 模块返回 fact如果一个模块返回一个字典且包含名为 ansible_facts 的key, 那么 ansible 将会根据对应的 value 创建响应的变量, 并分配给相对应的主机. 对于返回 fact 的模块, 并不需要使用注册变量, 因为 ansible 会自动创建. 可以自动返回 fact 的模块: $ ansible-doc --list |grep facts - ec2_facts - docker_image_facts 2.3. 本地 fact可将一个或者多个文件放置在目标主机的 /etc/ansible/facts.d/ 目录下, 如果该目录下的文件以 init格式, JSON格式 或者输出JSON格式的可执行文件(无需参数), 以这种形式加载的 fact 是 ansible_local 的特殊变量. 示例: # 目标主机 $ /etc/ansible/facts.d/books.fact [book] title=Ansible: Up and Running author=Lorin Hochstein publisher=P&apos;Reilly Media # ansible 主机 $ cat playbooks/local.yml - name: get local variables hosts: host_c gather_facts: True tasks: - name: print local variables; debug: var=ansible_local - name: print book title debug: msg=&quot;The Book Title is { { ansible_local.books.book.title } }&quot; 注意 ansible_local 变量值的结构, 因为 fact 文件的名称为 books, 所以 ansible_local 变量是一个字典, 且包含一个名为 &quot;books&quot; 的 key. 3. 变量优先级:以下优先级依次降低 命令行参数 其他 通过 inventory 文件或 YAML 文件定义的主机变量或群组变量 Fact 在 role 的 defaults/mail.yml 文件中的变量. 4. 过滤器: 变量加工处理Ansible 除了使用 Jinja2 作为模板之外, 还将其用于变量求值. 即, 可以在 playbook 中在 { {} } 内使用过滤器.除了可以用 Jinja2 的内置过滤器外, Ansible 还有一些自己扩展的过滤器. 有些参数, 需要参数, 有些则不需要. Jinja2 内置过滤器Ansible 过滤器 4.1. default : 设置默认值.# 设置 HOST 变量的默认值, 如果 database 没有被定义, 则使用 localhost . &quot;HOST&quot;: &quot;{ { database | default(&apos;localhost&apos;) } }&quot; 4.2. 用于注册变量的过滤器对注册变量状态检查状态的过滤器 名称 描述 failed 如果注册变量的值是任务 failed , 则返回 True changed 如果注册变量的值是任务 changed , 则返回 True success 如果注册变量的值是任务 success , 则返回 True skipped 如果注册变量的值是任务 skipped , 则返回 True 示例: - name: Run myprog command: /opt/myprog register: result ignore_errors: True - debug: var=result - debug: msg=&quot;Stop Running the playbook if myprog failed&quot; failed_when: result|failed 4.3. 用于文件路径的过滤器用于处理包含控制主机文件系统的路径的变量. 过滤器 描述 basename 文件路径中的目录 dirname 文件路径中的目录 expanduser 将文件路径中的 ~ 替换为用户家目录 realpath 处理符号链接后的文件实际路径 示例: vars: homepages: /usr/share/nginx/html/index.html tasks: - name: copy home page copy: src=files/{ { homepages| basename } } desc={ { homepages } } 4.4. 自定义过滤器Ansible 会在存放 playbook 的目录下的 filter_plugins 目录中寻找自定义过滤器. 也可以放在 /usr/share/ansible_plugins/filter_plugins/ 目录下, 或者 环境变量ANSIBLE_FILTER_PLUGINS 环境变量设置的目录. # filter_plugins/surround_by_quotes.py def surround_by_quote(a_list): return [&apos;&quot;%S&quot;&apos; % an_element for an_element in a_list] class FilterModule(object): def filters(self): return {&apos;surround_by_quote&apos;: surround_by_quote} surround_by_quote 函数定义了 Jinja2 过滤器.FilterModule 类定义了一个 filter 方法, 该方法返回由过滤器名称和函数本身组成的字典. FilterModule 是 Ansible 相关代码, 他使得 Jinja2 过滤器可以再 Ansible 中使用. 5. lookup: 从多种来源读取配置数据.lookup 官方文档说明Ansible 所有的 lookup 插件都是在控制主机, 而不是远程主机上执行的 支持的数据来源表: 名称 描述 file 文件的内容 password 随机生成密码 pipe 本地命令执行的输出 env 环境变量 template Jinja2 模板渲染的结果 csvfile .csv 文件中的条目 dnstxt DNS 的 TXT 记录 redis_ke 对 Redis 的key 进行查询 etcd 对 etcd 中的key 进行查询 file 示例: 在 playbook 中调用 lookup - name: Add my public key as an EC2 key ec2_key: name=mykey key_material=&quot;{ { lookup(&apos;file&apos;, &apos;/home/me/.ssh/id_rsa.pub&apos;) } }&quot; 示例: 使用 Jinja2 模板 # authorized_keys.j2 { { lookup(&apos;file&apos;, &apos;/home/me/.ssh/id_rsa.pub&apos;) } } # playbook - name: copy authorized_host file template: src=authorized_keys.j2 desc=/home/deploy/.ssh/authorized_keys pipe 在控制主机上调用一个外部程序, 并将这个程序的输出打印到标准输出上. 示例: 得到最新的 git commit 使用的 SHA-1 算法的值. - name: get SHA of most recent commit debug: msg=&quot;{ { lookup(&apos;pipe&apos;, &apos;git rev-parse HEAD&apos;) } }&quot; env 获取在控制主机上的某个环境变量的值. 示例: - name: get the current shell debug: msg=&quot;{ { lookup(&apos;env&apos;, &apos;SHELL&apos;) } }&quot; password 随机生成一个密码, 并将这个密码写入到参数指定的(控制主机)文件中. 示例: 生成 deploy 的 Postgre 用户和密码, 并将密码写入到 deploy-password.txt 中: - name: create deploy postgre user postgresql_user: name: deploy password: &quot;{ { lookup(&apos;password&apos;, &apos;deploy-password.txt&apos;) } }&quot; template 指定一个 Jinji2 模板文件, 并返回这个模板渲染的结果. # message.j2 This host runs { { ansible_distribution } } # task - name: output message from template debug: msg=&quot;{ { lookup(&apos;template&apos;, &apos;message.j2&apos;) } }&quot; csvfile 从 csv 文件中读取一个条目. # users.csv username, email lorin, lorin@example.com john, john@example.com sue, sue@example.com # 调用 : 查看名为 users.csv 的文件, 使用逗号作为分隔符来定位区域, 寻找第一列的值是 sue 的那一行, 返回第二列(索引从 0 开始)的值. lookup(&apos;csvfile&apos;, &apos;sue file=users.csv delimiter=, col=1&apos;) --&gt; sue@example.com # 用户名被存储在 username 变量中, 可以用 &quot;+&quot; 连接其他参数, 构建完整的参数字符串. lookup(&apos;csvfile&apos;, username + &apos;file=users.csv delimiter=, col=1&apos;) dnstxt 需要安装 dnspython 包, $ pip install dnspython TXT 记录是 DNS 中一个可以附加在主机名上的任意字符串, 一旦为主机名关联了一条 TXT 记录, 则任何人都可以使用 DNS 客户端获取这段文本. # 使用 dig 查看 TXT 记录 $ dig +short ansiblebook.com TXT &quot;isbn=97801491915325&quot; # task - name: look up TXT record debug: msg=&quot;{ { lookup(&apos;dnstxt&apos;, &apos;ansiblebook.com&apos;) } }&quot; redis-kv 需要安装 redis 包: $ pip install pip 可以使用 redis-kv 获取一个 key 的value, key 必须为字符串. # 设置一个值: $ redis-cli SET weather sunny # task - name: look up value in redis debug: msg=&quot;{ { lookup(&apos;redis_kv&apos;, &apos;redis://localhost:6379,weather&apos;) } }&quot; etcd etcd lookup 默认在 http://127.0.0.1:4001 上查找 etcd 服务器, 可以在执行 ansible-playbook 之前, 通过设置 ANSIBLE_ETCD_URL 改变这个值. # 设置测试值 $ curl -L http://127.0.0.1:4001/v2/keys/weather -XPUT -d value=cloudy # task - name: loop up value in etcd debug: msg=&quot;{ { lookup(&apos;etcd&apos;, &apos;weather&apos;) } }&quot;]]></content>
      <categories>
        <category>DevOps</category>
      </categories>
      <tags>
        <tag>ansible</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ansible-流程控制]]></title>
    <url>%2F2018%2F03%2F15%2FAnsible-%E6%B5%81%E7%A8%8B%E6%8E%A7%E5%88%B6%2F</url>
    <content type="text"><![CDATA[Ansible 的流程控制机制, 与使用方法. Ansible 学习总结 Ansible 本身除了配置管理工具外, 也可以说是一门配置管理语言. 循环 条件 函数 与 role handler 触发器. 循环迭代 Ansible 总是使用 item 作为 循环迭代变量的名字. 循环结构汇总: 官方文档 | 名称 | 输入 | 循环策略 | | — | — | — | | with_items | 列表 | 对列表元素进行循环 | | with_lines | 要执行的命令 | 对命令输出结果进行逐行循环 | | with_fileglob | glob | 对文件名进行循环 | | with_first_found | 路径的列表 | 输入中第一个存在的文件 | | with_dict | 字典 | 对字典元素进行循环 | | with_flattened | 列表的列表 | 对所有列表的元素顺序循环 | | with_indexed_items | 列表 | 单次迭代 | | with_nested | 列表 | 循环嵌套 | | with_random_choice | 列表 | 单次迭代 | | with_sequence | 参数数组 | 对数组进行循环 | | with_subelements | 字典的列表 | 嵌套循环 | | with_together | 列表的列表 | 对多个列表进行循环 | | with_inventory_hostname | 主机匹配模式 | 对匹配的主机进行循环 | with_items # apt 模块会一次调用整个列表作为参数, 而不是单个调用. # 有些模块支持(调用整个列表), 有些不支持, 如 pip 即是单个调用. - name: install apt packages apt: pkg={{ item }} update_cache=yes cache_valid_time=3600 sudo: True with_items: - git - python-dev - python-pip - supervisor # 另种方式: 传递字典的列表. - name: install python packages pip: name={{ item.name }} version={{ item.version }} virtualenv={{ venv_path }} with_items: - {name: mazzanine, version: 3.1.10} - {name: gunicorn, version: 19.1.1} with_lines 可以在控制主机执行任意命令, 并对命令的输出进行逐行迭代. - name: Send out a slack messag slack: domain: example.slack.com token: &quot;{{ slack_token }}&quot; msg: &quot;{{ item }} was in the list&quot; with_lines: - cat /path/to/mylist.txt with_fileglob 对于迭代控制主机上的一系列文件很有用. - name: add public keys to account authorized_key: user=deploy key=&quot;{{ lookup('file', item) }}&quot; with_fileglob: - /var/keys/*.pub - keys/*.pub with_dict 对字典进行迭代. 当使用该循环结构时, item 循环变量是一个含有如下两个 key 的字典: key: 字典中的一个 key value: 字典中与上面的key 相对应的 value 示例: ansible_eth0.ipv4 { &quot;address&quot;: &quot;10.0.2.15&quot;, &quot;netmask&quot;: &quot;255.255.255.0&quot;, &quot;network&quot;: &quot;10.0.2.0&quot; } task 示例 name: iterate over ansible_eth0debug: msg==with_dict: ansible_eth0.ipv4 条件执行 when: 当 when 表达式返回 True 时, 执行该 Task , 否则跳过该 Task. changed_when &amp; failed_when 函数 与 role 类似 其他编程语言中的 函数或类, 可以实现复用.]]></content>
      <categories>
        <category>DevOps</category>
      </categories>
      <tags>
        <tag>ansible</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ansible-Inventory.md]]></title>
    <url>%2F2018%2F03%2F15%2FAnsible-Inventory%2F</url>
    <content type="text"><![CDATA[Ansible 静态 Inventory 与 动态 Inventory 配置与使用.Ansible 学习总结 inventory : Ansible 可管理主机的集合.1. 静态 Inventory1.1 inventory 行为参数示例 : [targets] localhost ansible_connection=local other1.example.com ansible_connection=ssh ansible_ssh_user=mpdehaan ansible_ssh_pass=123456 other2.example.com ansible_connection=ssh ansible_ssh_user=mdehaan ansible_ssh_pass=123456 名称 默认值 说明 ansible_ssh_host 主机的名字 ssh 目的主机的主机名或IP ansible_ssh_port 22 ssh 默认端口号 ansible_ssh_user root ssh 登录使用的用户名 ansible_ssh_pass none ssh 认证使用的密码(这种方式并不安全,我们强烈建议使用 –ask-pass 或 SSH 密钥) ansible_sudo_pass none sudo 密码(这种方式并不安全,我们强烈建议使用 –ask-sudo-pass) ansible_sudo_exe (new in version 1.8) none sudo 命令路径(适用于1.8及以上版本) ansible_connection smart Ansible 使用何种连接模式连接到目标主机 . 与主机的连接类型.比如:local, ssh 或者 paramiko. Ansible 1.2 以前默认使用 paramiko. 1.2 以后默认使用 ‘smart’,’smart’ 方式会根据是否支持 ControlPersist, 来判断’ssh’ 方式是否可行. ansible_ssh_private_key_file none SSH 认证使用的私钥 ansible_shell_type sh 命令所使用的 shell, 除了 sh 外, 还支持 csh,fish,powershell ansible_python_interpreter /usr/bin/python 目标主机上的 python 解释器 ansible_*_interperter none 与 ansible_python_interpreter 的工作方式相同,可设定如 ruby 或 perl 的路径…. 1.2 ansible.cfg 设置 Inventory 行为参数默认值可以在 [defaults] 中改变一些行为参数的默认值: inventory 行为参数 ansible.cfg 选项 ansible_ssh_port remote_port ansible_ssh_user remote_user ansible_ssh_private_key_file private_key_file ansible_shell_type, shell 的名称 executable, shell 的绝对路径 1.3 群组 all 群组 ansible 自动定义了一个群组为 `all` 或 `*` , 包括 inventory 中的所有主机. 群组嵌套 [django:children] web mysql 模式匹配的主机 : 正则表达式永远以 ~ 开头 | 匹配行为 | 用法示例 | | — | — | | 所有主机 | all | | 所有主机 | * | | 群组的并集 | dev:staging | | 群组的交集 | dev:&amp;staging | | 排除 | dev:!staging | | 通配符 | *.example.com | | 数字范围 | web[1:20].example.com,web[01:20].example.com | | 字母范围 | web-[a-z].example.com | | 正则表达式 | ~web\d\.example\.(com | | 多种模式匹配组合使用 | hosts: dev:staging:&amp;database:!queue | 限制某些主机执行: -l 或 --limit 只针对限定的主机运行. $ ansible-playbook -l hosts playbook.yml $ ansible-playbook --limit hosts playbook.yml # 使用模式匹配语法 $ ansible-playbook -l &apos;staging:&amp;database&apos; playbook.yml 1.4 主机与群组变量 主机变量, 在 inventory 文件中: a.example.com color=red b.example.com color=green 群组变量, 在 inventory 文件中: [all:vars] ntp_server=ntp.ubuntu.com [prod:vars] db_primary_host=prod.db.com db_primary_port=5432 db_replica_host=rep.db.com db_name=mydb db_user=root db_pass=123456 [staging:vars] ... 主机变量和群组变量: 在各自的文件中 可以为每个主机和群组创建独立的变量文件. ansible 使用 YAML 格式来解析这些变量文件. host_vars 目录 : 主机变量文件 group_vars 目录 : 群组变量文件 ansible 假设这些目录在包含 playbook 的目录下 或者与 inventory 文件相邻的目录下. 键值格式 : # playbooks/group_vars/production db_primary_host: prod.db.com db_primary_port: 5432 db_replica_host: rep.db.com db_name: mydb db_user: root db_pass: 123456 # 访问方法: {{ db_primary_host }} 字典格式 : # playbooks/group_vars/production db: user: root password: 123456 name: mydb primary: host: primary.db.com port: 5432 replica: host: replica.db.com port: 5432 rabbitmq: host: rabbit.example.com port: 6379 # 访问方法 {{ db.primary.host }} 将 group_vars/production/ 定义为目录, 将多个包含变量定义的 YAML 文件存放其中; # group_vars/production/db db: user: root password: 123456 name: mydb primary: host: primary.db.com port: 5432 replica: host: replica.db.com port: 5432 # group_vars/production/rebbitmq rabbitmq: host: rabbit.example.com port: 6379 2. 动态 inventory如果 inventory 文件标记为可执行, 那么 Ansible 会假设这是一个动态 inventory 脚本, 并且会执行他, 而不是读取他的内容. 2.1 动态 inventory 脚本的接口--list : 列出所有群组. 输出为一个 JSON 对象, 该对象名为群组名, 值为主机的名字组成的数组. --host=&lt;host_name&gt; : 输出是一个名为变量名, 值为变量值的 JSON 对象. 包含主机的所有特定变量和行为参数. 2.2 在运行时添加主机或群组: add_host, group_by add_host : 调用方式如下: 当做一个模块使用即可 # 使用方法: add_host name=hostname groups=web,staging myvar=myval` # 示例 - name: add the vagrant hosts to the inventory add_host: name=vagrant ansible_ssh_host=127.0.0.1 ansible_ssh_port=2222 ansible_ssh_user=vagrant add_host 模块添加主机仅在本次 playbook 执行过程中有效, 他并不会修改 inventory 文件. group_by : 是一个 模块. 允许在 playbook 执行的时候, 使用 group_by 模块创建群组, 他可以基于已经为每台主机自动设定好的变量值(fact)来创建群组, Ansible 将这些变量称为 fact. - name: grout hosts by distribution hosts: myhosts gather_facts: True tasks: - name: create groups based on Linux distribution group_by: key={{ ansible_distribution }} - name: do something to CentOS hosts hosts: CentOS tasks: - name: install htop yum: name=htop - name: do something to Ubuntu hosts hosts: Ubuntu tasks: - name: install htop apt: name=htop 2.3 ec2.py &amp; ec2.ini 安装配置 AWS EC2 External Inventory Script ec2.py : 动态 Inventory ec2.ini : Inventory 配置文件. 只支持 Python 2.x 缓存 \$HOME/.ansible/tmp/ansible-ec2.cache \$HOME/.ansible/tmp/ansible-ec2.index 缓存过期时间: ec2.ini [ec2] cache_max_age = 0 # 默认 300s, 当值为 0 时, 为不使用缓存. $ ./ec2.py –refresh-cache # 强制刷新缓存 群组 自动生成的群组: | 类型 | 示例 | ansible 群组名 | | — | — | — | | Instance | i-123456 | i-123456 | | Instance type | c1.medium | type_c1_medium | | Security group | ssh | secutity_group_ssh | | Keypair | foo | key_foo | | Region | us-east-1 | us-east-1 | | Tag | env=staging | tag_env_staging | | Availability zone | us-easr-1b | us-easr-1b | | VPC | vpc-14dd1b70 | vpc_id_vpc-14dd1b70 | | all ec2 instance | N/A | ec2 | 在群组名中只有字母,连字符,下划线是合法的. 动态 Inventory 脚本会自动将其他的字符(如空格)转换成下划线.如 Name=My cool name 变为 tag_Name_my_cool_server. 群组操作 ec2.py 生成的群组, 支持 Ansible 群组的交集,并集等操作. 自动生成的群组与 静态 inventory 结合使用: 假设 ec2.py 生成的群组中有一个从 tag 取名的群组名为 tag_type_web, 则可以在 静态inventory文件中重新定义, 或者组合群组. 必须在 静态 inventory 中定义一个空的名为 tag_type_web 的群组, 如果没有定义, 则ansible 会报错. 示例如下: [web:children] tag_type_web [tag_type_web] 使用方法 # 简单使用方法 $ ansible -i ec2.py -u ubuntu us-east-1 -m ping # 复杂使用方法. $ cp ec2.py /etc/ansible/hosts &amp;&amp; chmod +x /etc/ansible/hosts/ec2.py $ cp ec2.ini /etc/ansible/ec2.ini $ export AWS_ACCESS_KEY_ID=&apos;AK123&apos; $ export AWS_SECRET_ACCESS_KEY=&apos;abc123&apos; # just for test, you should see your entire EC2 inventory across all regions in JSON. $ ./ec2.py --list [ --profile PROFILE ] --profile : manage multple AWS accounts, a profile example : [profile dev] aws_access_key_id = &lt;dev access key&gt; aws_secret_access_key = &lt;dev secret key&gt; [profile prod] aws_access_key_id = &lt;prod access key&gt; aws_secret_access_key = &lt;prod secret key&gt; --profile prod, --profile dev ec2.ini : is configured for all Amazon cloud services, but you can comment out any features that aren’t applicable. including cache control and destination variables. 3. 静态 Inventory 与 动态 Inventory 结合使用配置步骤如下: 将 动态inventory 和 静态inventory 放在同一目录下; 在 ansible.cfg 中将 hostfile 的值, 指向该目录即可.]]></content>
      <categories>
        <category>DevOps</category>
      </categories>
      <tags>
        <tag>ansible</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[自动化测试-Splinter]]></title>
    <url>%2F2018%2F03%2F15%2F%E8%87%AA%E5%8A%A8%E5%8C%96%E6%B5%8B%E8%AF%95-splinter%2F</url>
    <content type="text"><![CDATA[使用 Splinter 做自动化测试.Splinter Doc Splinter是一个使用Python开发的开源Web应用测试工具，它可以帮你实现自动浏览站点和与其进行交互。 Splinter 是一个基于 Selenium, PhantomJS, zope.testbrowser 等已存在浏览器的抽象层度较高的自动化测试工具. 1. 特点 API 简单 多浏览器支持, 支持的 浏览器列表如下: chrome webdriver, firefox webdriver, phantomjs webdriver, zopetestbrowser, remote webdriver 支持 CSS 选择器和 Xpath 选择器 支持 iframe 和 alert 支持执行 JavaScript 支持 ajax 调用和 async JavaScript 2. 入门2.1 安装 安装 python 支持 Python 2.7+ 版本 安装 splinter $ pip install splinter 安装浏览器驱动: 以 chrome 为例: https://chromedriver.storage.googleapis.com/index.html?path=2.35/ 2.2 示例:from splinter import Browser # 初始化 browser browser = Browser() # 打开首页 browser.visit(&apos;http://google.com&apos;) browser.fill(&apos;q&apos;, &apos;splinter - python acceptance testing for web applications&apos;) browser.find_by_name(&apos;btnG&apos;).click() if browser.is_text_present(&apos;splinter.readthedocs.io&apos;): print &quot;Yes, the official website was found!&quot; else: print &quot;No, it wasn&apos;t found... We need to improve our SEO techniques&quot; browser.quit() Browser 对象支持 上下文管理: with Browser() as browser: # code here 3. 基础浏览器行为和交互3.1 网页对象 Browser 对象初始化 browser = Browser(&apos;chrome&apos;) browser = Browser(&apos;firefox&apos;) browser = Browser(&apos;zope.testbrowser&apos;) browser = Browser(driver_name=&quot;chrome&quot;, executable_path=&quot;/path/to/chrome&quot;, user_agent=&quot;Mozilla/5.0 (iPhone; U; CPU like Mac OS X; en)&quot;, incognito=True) 浏览网页 browser.visit(&apos;http://cobrateam.info&apos;) browser.visit(&apos;http://username:password@cobrateam.info/protected&apos;) # basic HTTP 认证 重载网页 browser.reload() You can back and forward on your browsing history using back and forward methods: browser.visit(&apos;http://cobrateam.info&apos;) browser.visit(&apos;https://splinter.readthedocs.io&apos;) browser.back() browser.forward() 获取当前网页的相关内容 browser.title # 网页标题 browser.html # 网页的 html 代码 browser.url # 网页的 url 管理多个窗口, 使用 windows 对象, 例如弹出窗口. browser.windows # all open windows browser.windows[0] # the first window browser.windows[window_name] # the window_name window browser.windows.current # the current window browser.windows.current = browser.windows[3] # set current window to window 3 window = browser.windows[0] window.is_current # boolean - whether window is current active window window.is_current = True # set this window to be current window window.next # the next window window.prev # the previous window window.close() # close this window window.close_others() # close all windows except this one 3.2 查找网页元素 网页元素获取 splinter 支持 6 种元素查找方式, 每种方式均返回列表作为查找结果. 支持 first, last 快捷方式, 查找第一个和最后一个元素. 因为每个页面中, id 的值一般是不重复的, 因此 find_by_id 总是返回只有一个元素的列表. browser.find_by_css(&apos;h1&apos;) browser.find_by_xpath(&apos;//h1&apos;) browser.find_by_tag(&apos;h1&apos;) browser.find_by_name(&apos;name&apos;) browser.find_by_text(&apos;Hello World!&apos;) browser.find_by_id(&apos;firstheader&apos;) browser.find_by_value(&apos;query&apos;) browser.find_by_xpath(&apos;//h1&apos;).first browser.find_by_name(&apos;name&apos;).last browser.find_by_tag(&apos;h1&apos;)[1] 获取元素的值 browser.find_by_css(&apos;h1&apos;).first.value 查找 URL 返回列表作为结果. links_found = browser.find_link_by_text(&apos;Link for Example.com&apos;) links_found = browser.find_link_by_partial_text(&apos;for Example&apos;) links_found = browser.find_link_by_href(&apos;http://example.com&apos;) links_found = browser.find_link_by_partial_href(&apos;example&apos;) Clicking links : These methods return the first element always. # 绝对 url browser.click_link_by_href(&apos;http://www.the_site.com/my_link&apos;) # 相对 url browser.click_link_by_partial_href(&apos;my_link&apos;) browser.click_link_by_text(&apos;my link&apos;) browser.click_link_by_partial_text(&apos;part of link text&apos;) browser.click_link_by_id(&apos;link_id&apos;) 链式查找 Finding method are chainable, so you can find the descendants of a previously found element. divs = browser.find_by_tag(&quot;div&quot;) within_elements = divs.first.find_by_name(&quot;name&quot;) ElementDoesNotExist 异常 If an element is not found, the find_* methods return an empty list. But if you try to access an element in this list, the method will raise the splinter.exceptions.ElementDoesNotExist exception. Clicking buttons You can click in buttons. Splinter follows any redirects, and submits forms associated with buttons. browser.find_by_name(&apos;send&apos;).first.click() browser.find_link_by_text(&apos;my link&apos;).first.click() 表单 browser.fill(&apos;query&apos;, &apos;my name&apos;) browser.attach_file(&apos;file&apos;, &apos;/path/to/file/somefile.jpg&apos;) browser.choose(&apos;some-radio&apos;, &apos;radio-value&apos;) browser.check(&apos;some-check&apos;) browser.uncheck(&apos;some-check&apos;) browser.select(&apos;uf&apos;, &apos;rj&apos;) To trigger JavaScript events, like KeyDown or KeyUp, you can use the type method. browser.type(&quot;type&quot;, &quot;typing text&quot;) If you pass the argument slowly=True to the type method you can interact with the page on every key pressed. Useful for test field’s autocompletion (The browser will wait until next iteration to type the subsequent key). for key in browser.type(&quot;type&quot;, &quot;typing slowly&quot;, slowly=True): pass You can also use type and fill methods in an element. browser.find_by_name(&quot;name&quot;).type(&quot;Steve Jobs&quot;, slowly=True) browser.find_by_css(&quot;.city&quot;).fill(&quot;San Francisco&quot;) 判断元素是否可见. # 返回布尔值 browser.find_by_css(&apos;h1&apos;).first.visible 判断元素是否有 className # 返回布尔值 browser.find_by_css(&apos;.content&apos;).first.has_class(&apos;content&apos;) Interacting with elements through a ElementList object You can invoke any Element method on ElementList and it will be proxied to the first element of the list. So the two lines below are quivalent. assert browser.find_by_css(&apos;a.banner&apos;).first.visible assert browser.find_by_css(&apos;a.banner&apos;).visible 3.3 鼠标大多数鼠标事件目前只支持 Chrome 和 Firefox 27.0.1 支持 mouse_over, mouse_out,单击, 双击, 右击鼠标. mouse_over : puts the mouse above the element. browser.find_by_tag(&apos;h1&apos;).mouse_over() mouns_out : puts the mouse out of the element. browser.find_by_tag(&apos;h1&apos;).mouse_out() click : 单击 browser.find_by_tag(&apos;h1&apos;).click() double_click : 双击 browser.find_by_tag(&apos;h1&apos;).double_click() right_click : 右击 browser.find_by_tag(&apos;h1&apos;).right_click() drag_and_drop : You can drag an element and drop it to another element. The example below drags the &lt;h1&gt; ... &lt;/h1&gt; element and drop it to a container element (identified by a CSS class). draggable = browser.find_by_tag(&apos;h1&apos;) target = browser.find_by_css(&apos;.container&apos;) draggable.drag_and_drop(target) 3.4 Ajax &amp; Async JavaScriptWhen working with Ajax and Asynchronous JavaScript, it’s common to have elements which are not present in the HTML code(they are created with JavaScript, dynamically). In this case, you can use the methods is_element_present and is_text_present to check the existence of an element or text – Splinter will load the HTML and JavaScript in the browser and the check will be performed before processing JavaScript. There is also the optional argument wait_time (given in seconds), it’s a timeout: if the verification method gets True it will return the result (even if the wait_time is not over); if it doesn’t get True, the method will wait until the wait_time is over (sp it’ll return the result). # 检查文本是否 存在 browser = Browser() browser.visit(&apos;https://splinter.readthedocs.io/&apos;) browser.is_text_present(&apos;splinter&apos;) # True browser.is_text_present(&apos;splinter&apos;, wait_time=10) # True, using wait_time browser.is_text_present(&apos;text not present&apos;) # False # 检查文本是否 不存在 browser.is_text_not_present(&apos;text not present&apos;) # True browser.is_text_not_present(&apos;text not present&apos;, wait_time=10) # True, using wait_time browser.is_text_not_present(&apos;splinter&apos;) # False 元素(element)存在性检查, 返回布尔值. # 检查元素是否 存在 browser.is_element_present_by_css(&apos;h1&apos;) browser.is_element_present_by_xpath(&apos;//h1&apos;) browser.is_element_present_by_tag(&apos;h1&apos;) browser.is_element_present_by_name(&apos;name&apos;) browser.is_element_present_by_text(&apos;Hello World!&apos;) browser.is_element_present_by_id(&apos;firstheader&apos;) browser.is_element_present_by_value(&apos;query&apos;) browser.is_element_present_by_value(&apos;query&apos;, wait_time=10) # using wait_time # 检查元素是否 不存在 browser.is_element_not_present_by_css(&apos;h6&apos;) browser.is_element_not_present_by_xpath(&apos;//h6&apos;) browser.is_element_not_present_by_tag(&apos;h6&apos;) browser.is_element_not_present_by_name(&apos;unexisting-name&apos;) browser.is_element_not_present_by_text(&apos;Not here :(&apos;) browser.is_element_not_present_by_id(&apos;unexisting-header&apos;) browser.is_element_not_present_by_id(&apos;unexisting-header&apos;, wait_time=10) # using wait_time 3.5 cookie 管理It is possible to manipulate cookies using the cookies attribute from a Browser instance. The cookies attribute is a instance of a CookieManager class that manipulates cookies, like adding and deleting them. 添加 cookies browser.cookies.add({&quot;key&quot;: &quot;value&quot;}) 检索 cookies browser.cookies.all() 删除 cookies 删除 单个 cookies browser.cookies.delete(&quot;key1&quot;) # 删除 单个 cookies browser.cookies.delete(&quot;key1&quot;, &quot;key2&quot;) # 删除 两个 cookies 删除 所有 cookies browser.cookies.delete() 4. JavaScript 支持You can easily execute JavaScript in drivers which support it. browser.execute_script(&quot;$(&apos;body&apos;).empty()&quot;) You can return the result of the script. browser.evaluate_script(&quot;4+4&quot;) == 8 5. 其他5.1 HTTP 响应码处理及异常处理status_code and this HTTP exception handling is available only for selenium webdriver browser.visit(&quot;http://www.baidu.com&quot;) browser.status_code.is_success() # True browser.status_code == 200 # True browser.status_code.code # 200 当网页返回失败时, 触发 HttpResponseError 错误. try: browser.visit(&apos;http://cobrateam.info/i-want-cookies&apos;) except HttpResponseError, e: print &quot;Oops, I failed with the status code %s and reason %s&quot; % (e.status_code, e.reason) 5.2 iframersYou can use the get_iframe method and the with statement to interact with iframe. You can pass the iframe’s name, id, or index to get_iframe. with browser.get_iframe(&apos;iframemodal&apos;) as iframe: iframe.do_stuff() 5.3 alert and promptsOnly webdrivers (Firefox and Chrome) has support for alerts and prompts You can deal with alerts and prompts using the get_alert method. alert = browser.get_alert() alert.text alert.accept() alert.dismiss() In case of prompts, you can answer it using the fill_with method. prompts = browser.get_alert() prompts.text prompts.fill_with(&quot;text&quot;) prompts.accept() prompts.dismiss() You can use the with statement to interacte with both alerts and prompts too. with browser.get_alert() as alert: alert.do_stuff() IMPORTANT : if there’s not any prompt or alert, get_alert will return None. Remember to always use at least one of the alert/prompt ending methods(accept/dismiss). Otherwise your browser instance will be frozen until you accept or dismiss the alert/prompt correctly. 6. Drivers6.1 Chrome 安装 # 依赖 Selenium $ pip install selenium # 需要安装 chrome 浏览器 使用 headless option for Chrome browser = Browser(&quot;chrome&quot;, headless=True) incognito option: 隐身模式 browser = Browser(&quot;chrome&quot;, incognito=True) emulation option: 仿真模式 from selenium import webdriver from splinter import Browser mobile_enulation = {&quot;driverName&quot;: &quot;Google Nexus 5&quot;} chrome_options = webdriver.ChromeOptions() chrome_options.add_experimental_option(&quot;mobileEmulation&quot;, mobile_enulation) browser = Browser(&quot;chrome&quot;, options=chrome_options) screenshot: Take a screenshot of the current page and saves it locally. screenshot(name=None, suffix=&apos;.png&apos;)]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>自动化测试</tag>
        <tag>Splinter</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[fake-useragent-文档]]></title>
    <url>%2F2018%2F03%2F15%2Ffake-useragent-%E6%96%87%E6%A1%A3%2F</url>
    <content type="text"><![CDATA[使用 fake-useragent 为爬虫提供 UserAgent.grabs up to date useragent from useragentstring.comrandomize with real world statistic via w3schools.com https://fake-useragent.herokuapp.com/browsers/0.1.5 1. 安装$ pip install fake-useragent 2. 使用from fake_useragent import UserAgent ua = UserAgent() ua.random # and the best one, random via real world browser usage statistic ua.ie # Mozilla/5.0 (Windows; U; MSIE 9.0; Windows NT 9.0; en-US); ua.msie # Mozilla/5.0 (compatible; MSIE 10.0; Macintosh; Intel Mac OS X 10_7_3; Trident/6.0)&apos; ua[&apos;Internet Explorer&apos;] # Mozilla/5.0 (compatible; MSIE 8.0; Windows NT 6.1; Trident/4.0; GTB7.4; InfoPath.2; SV1; .NET CLR 3.3.69573; WOW64; en-US) ua.opera # Opera/9.80 (X11; Linux i686; U; ru) Presto/2.8.131 Version/11.11 ua.chrome # Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.2 (KHTML, like Gecko) Chrome/22.0.1216.0 Safari/537.2&apos; ua.google # Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_4) AppleWebKit/537.13 (KHTML, like Gecko) Chrome/24.0.1290.1 Safari/537.13 ua[&apos;google chrome&apos;] # Mozilla/5.0 (X11; CrOS i686 2268.111.0) AppleWebKit/536.11 (KHTML, like Gecko) Chrome/20.0.1132.57 Safari/536.11 ua.firefox # Mozilla/5.0 (Windows NT 6.2; Win64; x64; rv:16.0.1) Gecko/20121011 Firefox/16.0.1 ua.ff # Mozilla/5.0 (X11; Ubuntu; Linux i686; rv:15.0) Gecko/20100101 Firefox/15.0.1 ua.safari # Mozilla/5.0 (iPad; CPU OS 6_0 like Mac OS X) AppleWebKit/536.26 (KHTML, like Gecko) Version/6.0 Mobile/10A5355d Safari/8536.25 3. update saved database just:from fake_useragent import UserAgent ua = UserAgent() ua.update() Sometimes, useragentstring.com or w3schools.com changes their html, or down, in such case fake-useragent uses hosted cache server heroku.com fallback .If You don’t want to use hosted cache server (version 0.1.5 added) from fake_useragent import UserAgent ua = UserAgent(use_cache_server=False) 4. caech Exceptionfrom fake_useragent import FakeUserAgentError try: ua = UserAgent() except FakeUserAgentError: pass 5. if you use a unknown useragent , it will not raise a error, but return “Your favorite Browser”.import fake_useragent ua = fake_useragent.UserAgent(fallback=&apos;Your favorite Browser&apos;) ua.just_test_agent &apos;Your favorite Browser&apos;]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>UserAgent</tag>
        <tag>fake-useragent</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python-装饰器]]></title>
    <url>%2F2018%2F03%2F15%2FPython-%E8%A3%85%E9%A5%B0%E5%99%A8%2F</url>
    <content type="text"><![CDATA[摘要装饰器通常是一个命名的对象(不允许 lambda 表达式), 在被(装饰函数)调用时接受单一参数, 并返回另一个可调用对象. 这里的可调用对象, 不仅仅包含函数和方法, 还包括类. 任何可调用对象(任何实现了 call 方法的对象都是可调用的)都可用作装饰器, 他们返回的对象也不是简单的函数, 而是实现了自己的 call 方法的更复杂的类实例. @some_decorator def decorated_function(): pass # 以上写法总是可以替换为显式的装饰器调用和函数的重新赋值: decorated_function = some_decorator(decorated_function) 1. 装饰器定义/使用方法1.1 通用模式: 作为一个函数def mydecorator(function): def wrapped(*args, **kwargs): # 在函数调用之前, 做点什么 result = function(*args, **kwargs) # 在函数调用之后, 做点什么 # 返回结果 return result # 返回 wrapper 作为装饰函数 return wrapped 1.2 实现 call 方法: 作为一个类非参数化装饰器用作类的通用模式如下: class DecoratorAsClass: def __init__(self, function): self.function = function def __call__(self, *args, **kw): # 在调用原始函数之前, 做点什么 result = self.function(*args, **kwargs) # 在调用原始函数之后, 做点什么 # 返回结果 return result 1.3 参数化装饰器 : 实现第二层包装def repeat(number=3): &quot;&quot;&quot; 多次重复执行装饰函数, 返回最后一次原始函数调用的值作为结果. : param number: 重复次数, 默认值为 3 &quot;&quot;&quot; def actual_decorator(function): def wrapped(*args, **kwargs): result = None for _ in range(number): result = function(*args, **kwargs) return result return wrapped return actual_decorator @repeat(2) def foo(): print(&quot;foo&quot;) 带参数的装饰器总是可以做如下装换: foo = repeat(number=3)(foo) 即使参数化装饰器的参数有默认值, 但名字后面也必须加括号 @repeat() def bar(): print(&quot;bar&quot;) 1.4 保存内省的装饰器使用装饰器的常见缺点是: 使用装饰器时, 不保存函数元数据(主要是文档字符串和原始函数名). 装饰器组合创建了一个新函数, 并返回一个新对象, 完全没有考虑原函数的标志. 这将导致调试装饰器装饰过的函数更加困难, 也会破坏可能用到的大多数自动生产文档的工具, 应为无法访问原始的文档字符串和函数签名. 解决这个问题的方式, 就是使用 functools 模块内置的 wraps() 装饰器. from functools import wraps def preserving_decorator(function): @wraps(function) def wrapped(*args, **kwargs): &quot;&quot;&quot;包装函数内部文档&quot;&quot;&quot; return function(*args, **kwargs) return wrapped @preserving_decorator def function_with_important_docstring(): &quot;&quot;&quot;这是我们想要保存的文档字符串&quot;&quot;&quot; pass print(function_with_important_docstring.__name__) print(function_with_important_docstring.__doc__) 2. 装饰器常用示例2.1 参数检查检查函数接受或返回的参数, 在特定上下文中执行时可能有用. # 装饰器代码 rpc_info = {} # 在实际读取时, 这个类定义会填充 rpc_info 字典, 并用于检查参数类型的特定环境中. def xmlrpc(in_=(), out=(type(None), )): def _xmlrpc(function): # 注册签名 func_name = function.__name__ rpc_info[func_name] = (in_, out) def _check_types(elements, types): &quot;&quot;&quot;用来检查类型的子函数&quot;&quot;&quot; if len(elements) != len(types): raise TypeError(&quot;Argumen count is wrong&quot;) typed = enumerate(zip(elements, types)) for index, couple in typed: arg, of_the_right_type = couple if isinstance(arg, of_the_right_type): continue raise TypeError(&quot;Arg #%d should be %s&quot; % (index, of_the_right_type)) def __xmlrpc(*args): # 没有允许的关键词 # 检查输入的内容 if function.__class__ == &quot;method&quot;: checkable_args = args[1:] # 类方法, 去掉 self else: checkable_args = args[:] # 普通函数 _check_types(checkable_args, in_) # 运行函数 res = function(*args) # 检查输入内容 if not type(res) in (tuple, list): checkable_res = (res, ) else: checkable_res = res _check_types(checkable_res, out) # 函数机器类型检查成功 return res return __xmlrpc return _xmlrpc # 使用示例 class RPCView: @xmlrpc((int, int)) # two int --&gt; None def meth1(self, int1, int2): print(&quot;received %d and %d&quot; % (int1, int2)) @xmlrpc((str, ), (int, )) # string --&gt; int def meth2(self, phrase): print(&quot;received %s&quot; % phrase) return 12 # 调用输出 print(rpc_info) # 输出: # {&apos;meth1&apos;: ((&lt;class &apos;int&apos;&gt;, &lt;class &apos;int&apos;&gt;), (&lt;class &apos;NoneType&apos;&gt;,)), &apos;meth2&apos;: ((&lt;class &apos;str&apos;&gt;,), (&lt;class &apos;int&apos;&gt;,))} my = RPCView() my.meth1(1, 2) # 输出: 类型检查成功 # received 1 and 2 my.meth2(2) # 输出: 类型检查失败 # File &quot;D:\VBoxShare\Work\Documents\PyProject\PyCookbook\test.py&quot;, line 57, in &lt;module&gt; # my.meth2(2) # File &quot;D:\VBoxShare\Work\Documents\PyProject\PyCookbook\test.py&quot;, line 25, in __xmlrpc # _check_types(checkable_args, in_) # File &quot;D:\VBoxShare\Work\Documents\PyProject\PyCookbook\test.py&quot;, line 20, in _check_types # raise TypeError(&quot;Arg #%d should be %s&quot; % (index, of_the_right_type)) # TypeError: Arg #0 should be &lt;class &apos;str&apos;&gt; 2.2 缓存缓存装饰器与参数检查十分相似, 不过他重点是关注那些内容状态不会影响输入的函数, 每组参数都可以链接到唯一的结果. 因此, 缓存装饰器可以将输出与计算法所需的参数放在一起, 并在后续的调用中直接返回他(这种行为成为 memoizing). import time import hashlib import pickle cache = {} def is_obsolete(entry, duration): return time.time() - entry[&quot;time&quot;] &gt; duration def compute_key(function, args, kw): &quot;&quot;&quot; 利用已排序的参数来构建 SHA 哈希键, 并将结果保存在一个全局字典中. 利用 pickle 来建立 hash , 这是冻结所有作为参数传入的对象状态的快捷方式, 以确保所有参数都满足于要求. &quot;&quot;&quot; key = pickle.dumps((function.__name__, args, kw)) return hashlib.sha1(key).hexdigest() def memoize(duration=10): def _memoize(function): def __memoize(*args, **kw): key = compute_key(function, args, kw) # 是否已经拥有它了? if (key in cache and not is_obsolete(cache[key], duration)): print(&quot;We got a winner.&quot;) return cache[key][&quot;value&quot;] # 计算 result = function(*args, **kw) # 保存结果 cache[key] = { &quot;value&quot;: result, &quot;time&quot;: time.time() } return result return __memoize return _memoize @memoize() def func_1(a, b): return a + b print(func_1(2, 2)) # 4 print(func_1(2, 2)) # print , 4 @memoize(1) def func_2(a, b): return a + b print(func_2(2, 2)) # 4 time.sleep(1) print(func_2(2, 2)) # 4 缓存值还可以与函数本身绑定, 以管理其作用域和生命周期, 代替集中化的字典. 但在任何情况下, 更高效的装饰器会使用基于高级缓存算法的专用缓存库. 2.3 代理代理装饰器使用全局代理来标记和注册函数. 例如, 一个根据当前用户来保护代码访问的安全层可以使用集中式检查器和相关的可调用对象要求的权限来实现. class User: def __init__(self, roles): self.roles = roles class Unauthorized(Exception): pass def protect(role): def _protect(function): def __protect(*args, **kw): user = globals().get(&quot;user&quot;) if user is None or role not in user.roles: raise Unauthorized(&quot;I won&apos;t tell you.&quot;) return function(*args, **kw) return __protect return _protect tarek = User((&quot;admin&quot;, &quot;user&quot;)) bill = User((&quot;user&quot;,)) class MySecrets: @protect(&quot;admin&quot;) def waffle_recipe(self): print(&quot;use tons of butter&quot;) these_are = MySecrets() user = tarek these_are.waffle_recipe() # use tons of butter user = bill these_are.waffle_recipe() # __main__.Unauthorized: I won&apos;t tell you. 以上模型常用于 Python Web 框架中(权限验证), 用于定义可发布类的安全性. 例如, Django 提供装饰器来保护函数访问的安全. 2.4 上下文提供者上下文装饰器确保函数可以运行在正确的上下文中, 或者在函数前后运行一些代码, 换句话说, 他设定并复位一个特定的执行环境. 例如, 当一个数据项需要在多个线程之间共享时, 就要用一个锁来保护她避免多次访问, 这个锁可以在装饰器中编写. from threading import RLock lock = RLock() def synchronized(function): def _synchronized(*args, **kw): lock.acquire() try: return function(*args, **kw) finally: lock.release() return _synchronized @synchronized def thread_safe(): # 确保锁定资源 pass 上下装饰器通常会被上下文管理器(with) 替代.]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>python 装饰器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python-上下文管理]]></title>
    <url>%2F2018%2F03%2F15%2FPython-%E4%B8%8A%E4%B8%8B%E6%96%87%E7%AE%A1%E7%90%86%2F</url>
    <content type="text"><![CDATA[使用上下文协议创建和管理上下文 使用 contextlib.contextmanager 创建和管理上下文 上下文的基本使用和嵌套方法 1. 编写实现上下文管理器1.1 作为一个类: 上下文管理协议任何实现了 上下文管理协议的对象都可以用作上下文管理器. 该协议包含两个特殊方法: __enter__(self) : 调用该方法, 任何返回值都会绑定到指定的 as 语句. __exit__(self, exc_type, exc_value, traceback) : 接受代码块中出现错误时填入的 3 个参数. 如果没有错误, 三个都为 None. 出现错误时, __exit__ 不应该重新引发这个错误, 因为这是调用者(caller) 的责任. 但他可以通过返回 True 来避免引发异常. 多数情况下, 这一方法只是执行一些清理工作, 无论代码块中发生什么, 他都不会返回任何内容. 代码示例: class ContextIllustration: def __enter__(self): print(&quot;entering context&quot;) def __exit__(self, exc_type, exc_value, traceback): print(&quot;leveling context&quot;) if exc_type is None: print(&quot;With no ERROR&quot;) else: print(&quot;With an ERROR (%s)&quot; % exc_value) with ContextIllustration(): print(&quot;inside&quot;) # 输出: # entering context # inside # leveling context # With no ERROR with ContextIllustration(): raise RuntimeError(&quot;Raised within &apos;with&apos;&quot;) # 输出: # entering context # leveling context # With an ERROR (Raised within &apos;with&apos;) # Traceback (most recent call last): # File &quot;D:\VBoxShare\Work\Documents\PyProject\PyCookbook\test.py&quot;, line 23, in &lt;module&gt; # raise RuntimeError(&quot;Raised within &apos;with&apos;&quot;) # RuntimeError: Raised within &apos;with&apos; 通过返回 True 来避免触发异常: class ContextIllustration: def __enter__(self): print(&quot;entering context&quot;) def __exit__(self, exc_type, exc_value, traceback): print(&quot;leveling context&quot;) if exc_type is None: print(&quot;With no ERROR&quot;) else: print(&quot;With an ERROR (%s)&quot; % exc_value) return True with ContextIllustration(): raise RuntimeError(&quot;Raised within &apos;with&apos;&quot;) # 输出: # entering context # leveling context # With an ERROR (Raised within &apos;with&apos;) 1.2 作为一个函数: contextlib 模块标准库 contextlib 提供了与上下文管理器一起使用的辅助函数: contextmanager, 他可以在一个函数里同时提供 __enter__ 和 __exit__ 两部分, 中间用 yield 分开(函数变成了生成器). from contextlib import contextmanager thelist = [1, 2, 3] @contextmanager def ListTransaction(thelist): workingcopy = list(thelist) yield workingcopy # 尽在没有出现错误时才会修改原始列表. thelist[:] = workingcopy with ListTransaction(thelist) as l: print(l) print(type(l)) 传递给 yield 的值, 用作 __enter__() 方法的返回值, 调用 __exit__() 方法时, 执行将在 yield 语句后恢复. 如果上下文中出现异常, 他将以异常形式出现在生成器函数中.如有需要可以捕获异常, 以上例子中, 异常被传递出生成器, 并其他地方进行处理. 如果出现任何异常, 被装饰函数需要再次抛出异常, 以便传递异常 from contextlib import contextmanager @contextmanager def context_illustration(): print(&quot;Entering context&quot;) try: yield except Exception as e: print(&quot;Leaving context&quot;) print(&quot;with an ERROR (%s)&quot; % e) # 抛出异常 raise else: print(&quot;Leaving context&quot;) print(&quot;with no error&quot;) with context_illustration(): print(&quot;Entering&quot;) # 输出: # Entering context # Entering # Leaving context # with no error with context_illustration(): raise RuntimeError(&quot;MyError&quot;) # 输出: # Entering context # Traceback (most recent call last): # File &quot;D:\VBoxShare\Work\Documents\PyProject\PyCookbook\test.py&quot;, line 18, in &lt;module&gt; # Leaving context # with an ERROR (MyError) # raise RuntimeError(&quot;MyError&quot;) # RuntimeError: MyError contextlib 还提供其他三个辅助函数: closing(element) : 返回一个上下文管理器, 在退出时, 调用该元素的 close() 方法, 对处理流的类很有用. supress(*exceptions) : 他会压制发生在 with 语句正文中的特定异常. redirect_stdout(new_target) : 将代码内任何代码的 sys.stdout 输出重定向到类文件(file-like)对象的另一个文件. redirect_stderr(new_target) : 将代码内任何代码的 sys.stderr 输出重定向到类文件(file-like)对象的另一个文件. 2. 使用方式 基本使用 with context_manager: # code here ... 上下文变量: 使用 as 语句保存为局部变量 __enter__() 的任何返回值都会绑定到指定的 as 子句. with context_manager as context: # code here ... 多个上下文管理器(嵌套) with A() as a, B() as b: # code here ... 等价于嵌套使用: with A() as a: with B() as b: # code here ...]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>python 标准库</tag>
        <tag>python 上下文</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PyStdLib-optparse]]></title>
    <url>%2F2018%2F03%2F15%2FPyStdLib-optparse%2F</url>
    <content type="text"><![CDATA[使用 optparse 解析命令行参数. 代码示例: import optparse p = optparse.OptionParser() p.add_option(&quot;-t&quot;, action=&quot;store_true&quot;, dest=&quot;tracing&quot;) p.add_option(&quot;-o&quot;, &quot;--outfile&quot;, action=&quot;store&quot;, type=&quot;string&quot;, dest=&quot;outfile&quot;) p.add_option(&quot;-d&quot;, &quot;--debuglevel&quot;, action=&quot;store&quot;, type=&quot;int&quot;, dest=&quot;debug&quot;) p.add_option(&quot;--speed&quot;, action=&quot;store&quot;, type=&quot;choice&quot;, dest=&quot;speed&quot;, choices=[&quot;slow&quot;, &quot;fast&quot;, &quot;ludicrous&quot;]) p.add_option(&quot;--coord&quot;, action=&quot;store&quot;, type=&quot;int&quot;, dest=&quot;coord&quot;, nargs=2) p.add_option(&quot;--novice&quot;, action=&quot;store_const&quot;, const=&quot;novice&quot;, dest=&quot;mode&quot;) p.add_option(&quot;--guru&quot;, action=&quot;store_const&quot;, const=&quot;guru&quot;, dest=&quot;mode&quot;) p.set_defaults(tracing=False, debug=0, speed=&quot;fast&quot;, coord=(0, 0), mode=&quot;novice&quot;) opt, args = p.parse_args() print &quot;tracing: &quot;, opt.tracing print &quot;outfile: &quot;, opt.outfile print &quot;debug : &quot;, opt.debug print &quot;speed : &quot;, opt.speed print &quot;coord : &quot;, opt.coord print &quot;mode : &quot;, opt.mode print &quot;args : &quot;, args]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>python 标准库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django-学习总结]]></title>
    <url>%2F2018%2F03%2F14%2FDjango-%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>web development</tag>
        <tag>django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ansible-学习总结]]></title>
    <url>%2F2018%2F03%2F14%2FAnsible-%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[大纲Ansible 原理配置篇 Ansible 简介 Ansible 任务的执行细节原理2.1 角色与依赖:2.2 工作机制2.3 工作原理 安装配置3.1 安装3.2 配置文件 Ansible 抽象实体4.1 inventory4.2 变量与fact4.3 模块4.4 task/play/role/playbook Ansible 命令5.1 ansible5.2 ansible-doc5.3 ansible-galaxy5.4 ansible-vault5.5 ansible-playbook ansible 优化加速6.1 SSH Multiplexing (ControlPersist)6.2 fact 缓存6.3 pipeline6.4 并发 Ansible Inventory篇 静态 Inventory1.1 inventory 行为参数1.2 ansible.cfg 设置 Inventory 行为参数默认值1.3 群组1.4 主机与群组变量 动态 inventory2.1 动态 inventory 脚本的接口2.2 在运行时添加主机或群组: add_host, group_by2.3 ec2.py &amp; ec2.ini 静态 Inventory 与 动态 Inventory 结合使用 Ansible task/play/role篇 task play role Ansible api 开发篇Ansible 番外篇之 ansible.cfg 配置 defaults 段 ssh_connection 段 paramiko 段 accelerate 段 (不推荐使用) Ansible 番外篇之模块 内置模块1.1 查看模块帮助1.2 查找第三方模块1.3 常用模块 自定义模块2.1 使用 script 自定义 模块2.2 使用 Python 自定义模块. Ansible 番外篇之变量与fact 变量1.1. 定义变量1.2. 显示变量: debug 模块1.3. register 注册变量: 基于 task 的执行结果, 设置变量的值.1.4. set_fact 定义新变量1.5. 内置变量1.6. 在命令行设置变量 fact2.1. setup 模块2.2. 模块返回 fact2.3. 本地 fact 变量优先级: 过滤器: 变量加工处理4.1. default : 设置默认值.4.2. 用于注册变量的过滤器4.3. 用于文件路径的过滤器4.4. 自定义过滤器 lookup: 从多种来源读取配置数据. file pipe env password template csvfile dnstxt redis-kv etcd Ansible 番外篇之流程控制假如 ansible 是一门开发语言. 循环 条件 函数 与 role]]></content>
      <categories>
        <category>DevOps</category>
      </categories>
      <tags>
        <tag>ansible</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[AngularJS-学习总结]]></title>
    <url>%2F2018%2F03%2F14%2FAngularJS-%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"></content>
      <categories>
        <category>JavaScript</category>
      </categories>
      <tags>
        <tag>AngularJS</tag>
        <tag>JS 前端框架</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python-标准库]]></title>
    <url>%2F2018%2F03%2F14%2Fpython-%E6%A0%87%E5%87%86%E5%BA%93%2F</url>
    <content type="text"><![CDATA[python 标准库]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>python 标准库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flask-学习总结]]></title>
    <url>%2F2018%2F03%2F14%2FFlask-%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[Flask 学习总结]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>web development</tag>
        <tag>flask</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sublime-Text-使用总结]]></title>
    <url>%2F2018%2F03%2F14%2FSublime-Text-%E4%BD%BF%E7%94%A8%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[初始化配置设置 Linux 换行符Perference-&gt;Setting-*. 设置对象是 default_line_ending, 这个参数有三 个可用选项： - system : system是根据当前系统情况设置, - windows : windows使用的CRLF, - unix : unix使用的是 LF 设置 tab 为 4 个空格.Preference -&gt; Settings-User // The number of spaces a tab is considered equal to &quot;tab_size&quot;: 4, // Set to true to insert spaces when tab is pressed &quot;translate_tabs_to_spaces&quot;: true, 自动保存Preference -&gt; Settings-User &quot;save_on_focus_lost&quot;: true 手动安装插件插件Preference -&gt; Browse Package 把下载的插件解压到打开的文件夹中, 解压后即可. 去除解压后文件夹中的 `-` 等字符, 重启 sublime 即可. install package controllSUBLIME TEXT 3 import urllib.request,os,hashlib; h = &apos;df21e130d211cfc94d9b0905775a7c0f&apos; + &apos;1e3d39e33b79698005270310898eea76&apos;; pf = &apos;Package Control.sublime-package&apos;; ipp = sublime.installed_packages_path(); urllib.request.install_opener( urllib.request.build_opener( urllib.request.ProxyHandler()) ); by = urllib.request.urlopen( &apos;http://packagecontrol.io/&apos; + pf.replace(&apos; &apos;, &apos;%20&apos;)).read(); dh = hashlib.sha256(by).hexdigest(); print(&apos;Error validating download (got %s instead of %s), please try manual install&apos; % (dh, h)) if dh != h else open(os.path.join( ipp, pf), &apos;wb&apos; ).write(by) SUBLIME TEXT 2 import urllib2,os,hashlib; h = &apos;df21e130d211cfc94d9b0905775a7c0f&apos; + &apos;1e3d39e33b79698005270310898eea76&apos;; pf = &apos;Package Control.sublime-package&apos;; ipp = sublime.installed_packages_path(); os.makedirs( ipp ) if not os.path.exists(ipp) else None; urllib2.install_opener( urllib2.build_opener( urllib2.ProxyHandler()) ); by = urllib2.urlopen( &apos;http://packagecontrol.io/&apos; + pf.replace(&apos; &apos;, &apos;%20&apos;)).read(); dh = hashlib.sha256(by).hexdigest(); open( os.path.join( ipp, pf), &apos;wb&apos; ).write(by) if dh == h else None; print(&apos;Error validating download (got %s instead of %s), please try manual install&apos; % (dh, h) if dh != h else &apos;Please restart Sublime Text to finish installation&apos;) 大块方框,Sublime &gt; Preferences &gt; Package Settings &gt; Anaconda &gt; Settings User {&quot;anaconda_linting&quot;: false} 中文输入法光标跟随Packages Control -&gt; install -&gt; IMESupport 常用插件SublimeLinter : 用于高亮提示用户编写的代码中存在的不规范和错误的写法, 支持 JavaScript、CSS、HTML、Java、PHP、Python、Ruby 等十多种开发语言. SideBarEnhancements : SideBarEnhancements是一款很实用的右键菜单增强插件；在安装该插件前, 在Sublime Text左侧FOLDERS栏中点击右键, 只有寥寥几个简单的功能 Javascript-API-Completions : 支持Javascript、JQuery、Twitter Bootstrap框架、HTML5标签属性提示的插件, 是少数支持sublime text 3的后缀提示的插件, HTML5标签提示sublime text3自带, 不过JQuery提示还是很有用处的, 也可设置要提示的语言. Git : Glue : 会在界面下方显示一个小窗口, 你可以在那里写Shell脚本. 这样一来, 你的编辑器就不仅仅局限于使用Git了 GitGutter &amp; Modific : 这些插件可以高亮相对于上次提交有所变动的行, 换句话说是实时的diff工具 GitGutter : 这是一个小巧有用的插件, 它会告诉你自上次git commit以来已经改变的行. 一个指示器显示在行号的旁边. PlainTasks : 杰出的待办事项表！所有的任务都保持在文件中, 所以可以很方便的把任务和项目绑定在一起. 可以创建项目, 贴标签, 设置日期. 有竞争力的用户界面和快捷键. Lua : Python : AllAutocomplete : 搜索全部打开的标签页 Emmet : HTML 快速补全 markdown : anaconda : Python IDE anaconda 不能与 jedi 同时存在, 会出现 左括号无法写入的情况. GBK support : 支持 GBK 编码 SublimeTmpl : 支持文件模板, git 地址. 默认模板支持及快捷键 ctrl+alt+h html ctrl+alt+j javascript ctrl+alt+c css ctrl+alt+p php ctrl+alt+r ruby ctrl+alt+shift+p python 添加自定义模板文件及快捷键 : 参考 https://segmentfault.com/a/1190000008674119 1. 新建并编辑自定义模板文件 Packages\User\SublimeTmpl\templates\hexomd.tmpl --- title: ${saved_filename} date: ${date} categories: tags: --- 摘要 &lt;!-- more --&gt; 正文 2. sublime 模板文件定义 : Commands-User [ { &quot;caption&quot;: &quot;Tmpl: Create Hexo Markdown&quot;, &quot;command&quot;: &quot;sublime_tmpl&quot;, &quot;args&quot;: {&quot;type&quot;: &quot;hexomd&quot;} } ] 3. 快捷键定义 : KeyBing-User [ { &quot;keys&quot;: [&quot;ctrl+alt+m&quot;], &quot;command&quot;: &quot;sublime_tmpl&quot;, &quot;args&quot;: {&quot;type&quot;: &quot;hexomd&quot;}, &quot;context&quot;: [{&quot;key&quot;: &quot;sublime_tmpl.hexomd&quot;}] } ] 4. 用户设置定义 : Settings-User { # 支持 ${saved_filename} 变量 &quot;enable_file_variables_on_save&quot;: true, } 设置快捷键. 在SublimeText里, 打开Preferences -&gt; Key Bindings - User, 我设置的快捷键：[ { &quot;keys&quot;: [&quot;ctrl+f9&quot;], &quot;command&quot;: &quot;build&quot; }, { &quot;keys&quot;: [&quot;f10&quot;], &quot;command&quot;: &quot;build&quot;, &quot;args&quot;: {&quot;variant&quot;: &quot;Run&quot;} }, { &quot;keys&quot;: [&quot;ctrl+shift+x&quot;], &quot;command&quot;: &quot;toggle_comment&quot;, &quot;args&quot;: { &quot;block&quot;: true } }, ]]]></content>
      <categories>
        <category>Tools</category>
      </categories>
      <tags>
        <tag>sublime</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo+nexT 博客建设指南]]></title>
    <url>%2F2018%2F03%2F14%2FHexo-nexT-%E5%8D%9A%E5%AE%A2%E5%BB%BA%E8%AE%BE%E6%8C%87%E5%8D%97%2F</url>
    <content type="text"><![CDATA[Hexo 搭建 Github 博客开始使用安装 git$ yum install git -y 安装 NodeJS$ git clone https://github.com/creationix/nvm.git $ source nvm/nvm.sh $ nvm install stable 设置 Github注册 github创建 github page创建仓库, 仓库的名字要和你的账号对应, 格式为: USERNAME.github.io 安装 hexo-cli$ chmod 755 /root &amp;&amp; mkdir -m 755 -p /root/.npm/_logs $ npm install -g hexo-cli $ chmod 700 /root $ npm install -g hexo-cli 建站安装配置$ hexo init &lt;floder&gt; $ cd &lt;floder&gt; $ npm install 文件件目录结构 . ├── _config.yml # 网站的配置信息, 可以在此配置大部分参数 ├── package.json # 应用程序的信息. EJS, Stylus 和 Markdown renderer 已默认安装，您可以自由移除。 ├── scaffolds # 模板文件夹, 当新建文章时, Hexo 会根据 scaffold 来建立文件. | # Hexo 的模板是指在新建的 markdown 文件中默认填充的内容, 每次新建一篇文章时都会包含这个修改. ├── source # 存放用户资源. 除 _posts 文件夹外, 开头命名为 _ 的文件/文件夹和隐藏的文件都会被忽略. | | # Markdown 和 HTML 文件会被解析并放到 public 文件夹, 而其他文件会被拷贝过去. | ├── _drafts | └── _posts └── themes # 主题文件夹, Hexo 会根据主题来生成静态内容. # 安装 next theme, 可选. $ mkdir themes/next $ curl -s https://api.github.com/repos/iissnan/hexo-theme-next/releases/latest | grep tarball_url | cut -d &apos;&quot;&apos; -f 4 | wget -i - -O- | tar -zx -C themes/next --strip-components=1 # 修改默认 主题设置, 可选 $ vim _config.yml theme: next # 安装 hexo server $ npm install hexo-server --save # 启动 hexo server $ hexo server --ip=0.0.0.0 写文章与提交部署安装 hexo-deployer-git 部署方式 $ npm install hexo-deployer-git --save # 配置部署方式 $ vim _config.yml deploy: type: git repo: https://github.com/pyfdtic/pyfdtic.github.io.git branch: master 写文章 # hexo new &quot;TITLE&quot; $ vim source/_posts/TITLE.md --- title: first post date: 2018-03-14 17:08:36 categories: - test tags: - tag-a - tag-b - tag-c --- # content 写摘要: --- 这里是摘要 &lt;!-- more --&gt; 这是正文 生成静态文件并部署 $ hexo g -d 密钥认证提交 $ vim _config.yml 其中 repo 配置为 ssh 协议地址 # 语言配置 language: zh-Hans $ 在 github 上配置 ssh 密钥. 配置主题文档 $ vim themes/next/_config.yml # 主页预览显示 auto_excerpt: enable: true length: 250 # 选择不同的主体 #scheme: Muse scheme: Mist # 主页设置 menu: home: / || home about: /about/ || user tags: /tags/ || tags categories: /categories/ || th archives: /archives/ || archive tags/categories 页面 # tags $ hexo new page &quot;tags&quot; $ vim source/tags/index.md --- title: Tags date: 2018-03-14 18:09:40 type: &quot;tags&quot; comments: false --- $ vim themes/next/_config.yml menu: # ... tags: /tags/ || tags # ... # categories $ hexo new page &quot;categories&quot; $ vim source/categories/index.md --- title: Tags date: 2018-03-14 18:09:40 type: &quot;categories&quot; comments: false --- $ vim themes/next/_config.yml menu: # ... categories: /categories/ || th # ... about页面 $ hexo new page &quot;about&quot; 谷歌/百度统计 $ vim _config.yml google_analytics: UA-[numbers] baidu_analytics: your-analytics-id 站内搜索: $ npm install hexo-generator-searchdb --save $ vim _config.yml search: path: search.xml field: post format: html limit: 10000 $ vim themes/next/_config.yml local_search: enable: true # if auto, trigger search by changing input # if manual, trigger search by pressing enter key or search button trigger: auto # show top n results per article, show all results by setting to -1 top_n_per_article: -1 站点地图: $ npm install hexo-generator-sitemap $ vim themes/next/_config.yml menu: # ... sitemap: /sitemap.xml || sitemap $ 在 google Search console 提交 siteamp 地图. 配置资源文件夹 $ mkdir source/images # 在文章中引用 ![](/images/image.jpg) 主题配置参考站点配置 # ============================================================================= # NexT Theme configuration # ============================================================================= avatar: https://avatars1.githubusercontent.com/u/32269?v=3&amp;s=460 # Duoshuo duoshuo_shortname: notes-iissnan # Disqus disqus_shortname: # Social links social: GitHub: https://github.com/iissnan Twitter: https://twitter.com/iissnan Weibo: http://weibo.com/iissnan DouBan: http://douban.com/people/iissnan ZhiHu: http://www.zhihu.com/people/iissnan # Creative Commons 4.0 International License. # http://creativecommons.org/ # Available: by | by-nc | by-nc-nd | by-nc-sa | by-nd | by-sa | zero creative_commons: by-nc-sa # Google Webmaster tools verification setting # See: https://www.google.com/webmasters/ google_site_verification: VvyjvVXcJQa0QklHipu6pwm2PJGnnchIqX7s5JbbT_0 # Google Analytics # Google分析ID google_analytics: # 百度分析ID baidu_analytics: 50c15455e37f70aea674ff4a663eef27 # Specify the date when the site was setup since: 2011 # ============================================================================= # End NexT Theme configuration # ============================================================================= 主题配置文件 menu: home: / categories: /categories archives: /archives tags: /tags #about: /about # Place your favicon.ico to /source directory. favicon: /favicon.ico # Set default keywords (Use a comma to separate) keywords: &quot;Hexo,next&quot; # Set rss to false to disable feed link. # Leave rss as empty to use site&apos;s feed link. # Set rss to specific value if you have burned your feed already. rss: # Icon fonts # Place your font into next/source/fonts, specify directory-name and font-name here # Avialable: default | linecons | fifty-shades | feather #icon_font: default #icon_font: fifty-shades #icon_font: feather icon_font: linecons # Code Highlight theme # Available value: normal | night | night eighties | night blue | night bright # https://github.com/chriskempson/tomorrow-theme highlight_theme: normal # MathJax Support mathjax: # Schemes scheme: Mist # Automatically scroll page to section which is under &lt;!-- more --&gt; mark. scroll_to_more: true # Automatically add list number to toc. toc_list_number: true ## DO NOT EDIT THE FOLLOWING SETTINGS ## UNLESS YOU KNOW WHAT YOU ARE DOING # Use velocity to animate everything. use_motion: true # Fancybox fancybox: true # Static files vendors: vendors css: css images: images # Theme version version: 0.4.2 删除文章$ hexo clean $ hexo g -d 配置 _config.yml网站 参数 描述 title 网站标题 subtitle 网站副标题 description 网站描述, 网站 SEO author 作者. 显示文章的作者 language 网站使用的语言 timezone 网站时区. Hexo 默认使用浏览器时区 网址 参数 描述 默认值 url 网址 - root 网站根目录 - permalink 文章的永久链接 :year/:mouth/:day/:title permalink_defaults 永久链接中各部分的默认值 网站存放在子目录, 如果网站存放在子目录中, 如 http://yoursite.com/blog , 则需要把 url 设为 ‘http://yoursite.com/blog&#39;, 并把 root 设为 /blog/; 目录 参数 描述 默认值 source_dir 资源文件夹, 用于存放内容 source public_dir 公共文件夹, 用于存放生成的站点文件 public tag_dir 标签文件夹 tags archive_dir 归档文件夹 archives category_dir 分类文件夹 categories code_dir include code 文件夹 downloads/code i18n_dir 国际化(i18n)文件夹 :lang skip_render 跳过指定文件的渲染, 可使用 glob 表达式来匹配路径 如果刚接触 hexo , 则没必要设置以上各值. 文章 参数 描述 默认值 new_post_name 新文章的文件名称 :title.md default_layout 预设布局 post auto_spacing 在中文和英文之间加入空格 false titlecase 把标题转换为 title case false external_link 在新标签中打开链接 true filename_case 把文件名称转换为 (1) 小写或 (2) 大写 0 render_drafts 显示草稿 false post_asset_folder 启动 Asset 文件夹 false relative_link 把链接改为与根目录的相对位址 false future 显示未来的文章 true highlight 代码块的设置 默认情况下，Hexo生成的超链接都是绝对地址. 建议使用绝对地址. 分类 &amp; 标签 参数 描述 默认值 default_category 默认分类 uncategorized category_map 分类别名 tag_map 标签别名 日期/时间格式Hexo 使用 Moment.js 来解析和显示时间. 参数 描述 默认值 date_format 日期格式 YYYY-MM-DD time_format 时间格式 H:mm:ss 分页 参数 描述 默认值 per_page 每页显示的文章数量(0= 关闭分页功能) 10 pagination_dir 分页目录 page 扩展 参数 描述 默认值 theme 当前主题名称, 值为 false 时禁用主题 deploy 部署部分的设置 指令$ hexo SUB_CMD PARAM init [floder] : 新建一个网站. floder 为空时, 为当前文件夹. new [layout] &lt;title&gt; : 新建一篇文章, 如果没有设置`layout`的话, 默认使用 `_config.yml` 中的 `default_layout` 参数代替. 如果标签包含空格, 请使用引号. generate : 生成静态文件. 可以简写为 &quot;hexo g&quot; --d, --deploy : 文件生成后, 立即部署网站 -w, --watch : 监视文件变动. publish [layout] &lt;filename&gt; : 发表草稿 server : 启动服务器, 默认为 &quot;http://localhost:4000/&quot; -p, --port : 指定端口 -s, --static : 只使用静态文件, -l, --log : 启动日志记录, 使用覆盖记录格式. deploy : 部署网站. 可以简写为 &quot;hexo d&quot; -g, --generate : 部署之前预先生成静态文件. render &lt;file1&gt; [file2] ... : 渲染文件. -o, --output : 设置输出路径. migrate &lt;type&gt; : 从其他博客系统迁移. clean : 清除缓存文件(db.json) 和 已生成的静态文件(public). 在某些情况下(尤其是更换主题后), 如果发现对网站的更改无论如何不生效, 可能需要运行该命令. list &lt;type&gt; : 列出网站资料 version : 显示 hexo 版本 --safe : 在安全模式下运行, 不会载入插件和脚本. 当安装新插件遇到问题时, 可以尝试以安全模式重新执行. --debug : 在终端中显示调试信息, 并记录到 debug.log. --silent : 隐藏终端信息 --config custom.yml : 自定义配置文件路径, 执行后将不再使用 _config.yml --draft : 显示 source/_drafts 文件夹中的草稿万丈. --cwd /path/to/cwd : 自定义当前工作目录. 基本操作写作新建文章新建一篇文章: $ hexo new [layout] &lt;title&gt; 可以在 layout 中指定文章的布局(layout), 默认为 post, 可以通过修改 _config.yml 中的 default_layout 参数来指定默认布局. 文章布局Hexo 有三种默认布局: post, page, draft. 他们分别对应不同的路径, 用户自定义的其他布局和post相同, 都将存储在source/_posts 文件夹. 布局 路径 post source/_posts page source draft source/_drafts 如果你不希望你的文章被处理, 可以将 Front-Matter 中的 layout: 设置为 false 草稿草稿(draft) 默认不会显示在页面中, 可以在执行时加上 --draft 参数, 或是把 render_drafts 参数设置为 true 来预览草稿. draft 为草稿布局, 保存与 source/_drafts 目录, 可以通过 publish 命令将草稿移动到source/_posts 文件夹, publish 与 new 使用方式十分类似. $ hexo publish [layout] &lt;title&gt; 文件名称Hexo 默认以标题作为文件名称, 可以编辑 new_post_name 参数来改变默认的文件名称.如 :year-:month-:day-:title.md. 变量 描述 :title 标题(小写, 空格将被替换为短杠) :year 建立年份, 如 2016 :mouth 建立月份, 前导有零, 如 04 :i_mouth 建立月份, 前导无零, 如 4 :day 建立的日期, 前导有零, 如 07 :i_day 建立的日期, 前导无零, 如 7 模板(scaffold)使用方法在新建文章时, Hexo 会根据 scaffolds 文件夹内向对应的文件来建立新文件. 如: # hexo 在 scaffolds 文件夹中寻找 photo.md , # 并根据其内容建立文章. $ hexo new photo &quot;My Gallery&quot; 模板中的可用变量 变量 描述 layout 布局 title 标题 date 文件建立日期 Front-matter使用格式及预定义参数Front-matter 是文件最上方以 --- 分割的区域, 用于指定个别文件的变量. title: Hello World date: 2013/7/12 20:46:25 --- 预定义参数列表如下: 参数 描述 默认值 layout 布局 title 标题 date 建立日期 文件建立日期 updated 更新日期 文件跟新日期 comments 开启文章评论功能 true tags 标签(不适用于分页) categories 分类(不适用于分页) permalink 覆盖文章网址 分类和标签只有文章支持分类和标签, 可以在 Front-matter 中设置. 分类: 分类有顺序性和层次性, 如 Foo,Bar 不等于 Bar, Foo 标签: 标签没有顺序和层次 示例: categories: - Diary tags: - PS3 - Games WordPress 支持对一篇文章设置多个分类, 而且这些分类可以是同级的, 也可以是父子分类. 但 Hexo 不支持指定多个同级分类. 如下的分类, Life 将成为 Diary 的子分类. categories - Diary - Life JSON Front-matter可以使用 JSON 来编写 Front-matter, 只需将 --- 替换为 ;;; 即可: &quot;title&quot;: &quot;Hello World&quot; &quot;date&quot;: &quot;2013/7/12 20:46:25&quot; ;;; 标签插件(Tag Plugins)标签插件和 Front-matter 中的标签不同, 标签插件是用于在文章中快速插入特定内容的插件 引用块在文章中插入引言, 可包含作者, 来源 和 标题. 格式 {% blockquote [author[, source]] [link] [source_link_title] %} CONTENT {% endblockquote %} 示例 # 引用网络上的文章 {% blockquote Seth Godin http://sethgodin.typepad.com/seths_blog/2009/07/welcome-to-island-marketing.html Welcome to Island Marketing %} Every interaction is both precious and an opportunity to delight. {% endblockquote %} # 引用书上的句子 {% blockquote David Levithan, Wide Awake %} Do not just seek happiness for yourself. Seek happiness for all. Through kindness. Through mercy. {% endblockquote %} 代码块在文章中插入代码. 格式 {% codeblock [title] [lang:language] [url] [link text] %} CODE_SNIPPET {% endcodeblock %} 示例 # 附加说明和网址 {% codeblock _.compact http://underscorejs.org/#compact Underscore.js %} _.compact([0, 1, false, 2, '', 3]); => [1, 2, 3] {% endcodeblock %} # 指定语言 {% codeblock lang:objc %} [rectangle setX: 10 y: 10 width: 20 height: 20]; {% endcodeblock %} 反引号代码块使用三个反引号类包裹的代码块: ``` [language] [title] [url] [link text] code snippet ``` Pull Quote{% pullquote [class] %} content {% endpullquote %} jsFiddle{% jsfiddle shorttag [tabs] [skin] [width] [height] %} Gist{% gist gist_id [filename] %} 3iframe{% iframe url [width] [height] %} Image{% img [class names] /path/to/image [width] [height] [title text [alt text]] %} Link在文章中插入链接, 并自动给外部链接添加 target=&quot;_blank&quot; {% link text url [external] [title] %} Include Code插入source文件夹内的代码文件. {% include_code [title] [lang:language] path/to/file %} Youtube插入 Youtube 视频. {% youtube video_id %} Vimeo插入 vimeo 视频 {% vimeo video_id %} 引用文章引用其他文章的链接. {% post_path slug %} {% post_link slug [title] %} 引用资源引用文章的资源 {% asset_path slug %} {% asset_img slug [title] %} {% asset_link slug [title] %} Raw如果希望在文章中插入 Swig 标签, 可以尝试使用 Raw 标签, 以免发生解析异常. {% raw %} content {% endraw %} 资源文件夹资源Asset代表 source 文件夹中除了文章以外的所有文件, 如图片,CSS,JS文件等. 如在 source/images 文件夹中的图片, 可以使用类似于![](/images/NAME.jpg) 方法访问他们. 文章资源文件夹更加组织化的管理资源, 可以通过修改 config.yml 文件中的 post_asset_folder 选项设为 true 来打开. post_asset_folder: true 打开资源文件管理功能之后, Hexo 将会在每一次通过 hexo new [layout] &lt;title&gt; 命令创建新文章时自动创建一个文件夹. 这个资源文件夹间会有与这个 markdown 文件一样的名字. 将所有与该文章有关的资源放在这个关联文件夹中之后, 可以通过相对路径来引用这些资源, 这样就得到了一个更简单而且方便的得多的工作流. 相对路径引用的标签插件通过常规的 markdown 语法和相对路径来引用图片和其他资源可能会导致他们在存档页和主页上显示不正常. 可以使用如下方式引用资源, 解决这个问题: {% asset_path slug %} {% asset_img slug [title] %} {% asset_link slug [title] %} 如, 当打开文章资源文件夹功能后, 资源文件夹中有一个 example.jpg 图片, 正确的引用该图片的方式是使用如下的标签插件, 而不是 markdown, 该图片将会同时出现在文章和主页及归档页中: {% asset_img example.jpg This is an example image %} 数据文件有时, 可能需要在主题中使用某些资料, 而这些资料并不在文章内, 并且是需要重复使用的, 那么可以使用 Hexo 3.0 新增的 数据文件功能, 此功能会载入 source/_data 内的 YAML 或 JSON 文件, 以方便在网站中复用这些文件. # source/_date/menu.yml Home: / Gallery: /gallery/ Archives: /archives/ # 在模板中引用这些资料: &lt;% for (var link in site.data.menu) { %&gt; &lt;a href=&quot;&lt;%= site.data.menu[link] %&gt;&quot;&gt; &lt;%= link %&gt; &lt;/a&gt; &lt;% } %&gt; # 渲染结果 &lt;a href=&quot;/&quot;&gt; Home &lt;/a&gt; &lt;a href=&quot;/gallery/&quot;&gt; Gallery &lt;/a&gt; &lt;a href=&quot;/archives&quot;&gt; Archives &lt;/a&gt; 服务器hexo-serverHexo 3.0 把服务器独立成了个别模块, 必须先安装 hexo-server 才能使用. # 安装 $ npm install hexo-server --save # 启动服务器, 默认 http://localhost:4000 $ hexo server [-p PORT] [-i IP_ADDRESS] [-s] -s : 静态模式, 服务器只处理 public 文件夹内的文件, 而不会处理文件变动, 在执行时, 应该先自行执行 hexo generate, 常用于生产环境. -i IP_ADDRESS : 指定IP地址, 默认为 0.0.0.0 . -p PORT : 指定监听端口. PowPow 是 Mac 系统上的零配置 Rack 服务器, 他也可以作为一个简单易用的静态文件服务器来使用. # 安装 $ curl get.pow.cx | sh # 设置: 在 ~/.pow 文件夹建立链接(symlink) $ cd ~/.pow $ ln -s /path/to/myapp # 网站将在 http://myapp.dev 下运行, 网址根据链接名称而定. 生成器生成文件:$ hexo generate [--watch] --watch : 监视文件变动并立即重新生成静态文件. 在生成时对比文件的 SHA1 , 只有变动的文件才会写入. 完成后部署如下两个命令功能相同, 让 Hexo 在生成完毕后自动部署网站. $ hexo generate --deploy $ hexo g -d # 上述命令的简写 $ hexo deploy --generate $ hexo d -g # 上述命令的简写 部署部署步骤 $ vim _config.yml deploy: type: git $ hexo deploy git 部署# 安装 hexo-deployer-git $ npm install hexo-deployer-git --save # 修改 _config.yml deploy: type: git repo: &lt;REPOSITORY URL&gt; branch: &lt;GIT BRANCH&gt; message: &lt;自定义提交信息&gt; # 默认为 Site updated: {{ now('YYYY-MM-DD HH:mm:ss') }} # 部署 $ hexo deploy Heroku 部署# 安装 hexo-deployer-heroku $ npm install hexo-deployer-heroku --save # 修改 _config.yml deploy: type: heroku repo: &lt;REPOSITORY URL&gt; message: &lt;自定义提交信息&gt; # 默认为 Site updated: {{ now('YYYY-MM-DD HH:mm:ss') }} # 部署 $ hexo deploy 自定义永久链接(Permalink)https://hexo.io/zh-cn/docs/permalinks.html 主题修改主题 在 themes 文件夹内, 创建一个任意名称的文件夹, 修改 _config.yml 内的 theme 设定, 即可切换主体题 主题目录结构. ├── _config.yml ├── languages ├── layout ├── scripts └── source _config.yml主体的配置文件, 修改时会自动更新, 无需重启服务器. languages语言文件夹, 参见国际化 layout布局文件夹, 用于存放主题的模板文件, 决定网站内容的呈现方式. Hexo 内建 Swig 模板引擎, 可以另外安装插件来获得 EJS, Haml, Jade 支持, Hexo 根据模板文件的扩展名来决定所使用的模板引擎. scripts脚本文件夹, 在启动时, Hexo 会自定载入此文件夹内的 JavaScript 文件. source资源文件夹, 除了模板以外的 Asset, 如 CSS , JavaScript 文件等, 都应该放在这个文件夹中. 文件或文件夹前缀为 _ (下划线) 或 隐藏的文件会被忽略. 如果文件可以被渲染的话, 会经过解析然后存储到 public 文件夹, 否则会直接拷贝到 public 文件夹. 模板https://hexo.io/zh-cn/docs/templates.html 变量https://hexo.io/zh-cn/docs/variables.html 辅助函数https://hexo.io/zh-cn/docs/helpers.html#toc 国际化(i18n)https://hexo.io/zh-cn/docs/internationalization.html 插件https://hexo.io/zh-cn/docs/plugins.html nexThttp://theme-next.iissnan.com/getting-started.html#third-party-services 参考文档hexo-theme-nexthexo-wiki hexo 文档 - 中文hexo 文档 - 英文nexT 主题配置文档]]></content>
      <categories>
        <category>Tools</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>nexT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[markdown 语法总结]]></title>
    <url>%2F2018%2F03%2F14%2Fmarkdown-%E8%AF%AD%E6%B3%95%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[段落标题 在行首插入 1 - 6 个 # ,对应 标题1 - 标题6 ## 会在标题下面插入一个横线, 作为分割. 区块引用 普通区块引用 嵌套区块引用 列表 有序列表 数字 + . 无序列表 * + - , 作用相同, 无差别 代码 1.代码块 `缩进4个空格, 或一个制表符, 代码块会一直持续到没有缩进的哪一行, 或者文件结尾` 2.小段代码 `反引号包含小段代码` 分割线 三个以上的星号,减号, 来建立一个分割线, 行内不能有其他东西 链接 普通链接 行内式 [Name](http://www.baidu.com &quot;Title&quot;) 引用式 定义: [id]: http://example.com &quot;Optionnal Title&quot; 引用: [Name][id] 图片 行内式 ![Alt text](/path/to/img.jpg &quot;Optional title&quot;) 引用式 定义: [id]: url/to/image &quot;Optional title attribute&quot; 引用: ![Alt text][id] Markdown 无法指定图片的宽高, 如果需要请使用 自动链接 针对URL : &lt;http://www.baidu.com&gt; 针对邮箱 : &lt;admin@example.com&gt; 强调 斜体 *WORD* 加粗 **WORD** 转义 : 在一些符号前面加上 反斜杠 来插入特殊符号 9.表格基本 | 表头 | 表头 | 表头 | 表头 | | -- | --- | --- | --- | | 内容 | 内容 | 内容 | 内容 | 表格对齐 | :-- | ---&gt; 左对齐 | ---: | ---&gt; 右对齐 | :---:| ---&gt; 居中对齐 颜色 `&lt;font color=&quot;blue&quot;&gt;GREEN&lt;/font&gt;` GREEN 删除线 ~~这里是删除内容~~ 效果 : 这里是删除内容 区块元素 段落 1. 类 Setext 格式 : 底线 形式 最高阶标题 : ==== 第二阶标题 : ---- 示例 : This is H1 ========== This is H2 ---------- 2. 类 atx 格式 : # 形式 在行首插入 1 - 6 个 # , 对应标题1 - 标题6 示例 : # This is H1 ## This is H2 ### This is H3 ** 可以选择性的 [闭合] 类 atx 样式的标题 : 在行尾加上 # , 而且行尾的 # 数量也无需同开头一样. 区块引用 Blockquotes 普通区块引用 : &gt; &gt; This is a blockquote with two paragraphs. Lorem ipsum dolor sit amet, &gt; consectetuer adipiscing elit. Aliquam hendrerit mi posuere lectus. &gt; Vestibulum enim wisi, viverra nec, fringilla in, laoreet vitae, risus. 嵌套区块引用 : 根据层次加上不同数量的 &gt; &gt; This is a blockquote with two paragraphs. Lorem ipsum dolor sit amet, &gt;&gt; consectetuer adipiscing elit. Aliquam hendrerit mi posuere lectus. &gt;&gt; Vestibulum enim wisi, viverra nec, fringilla in, laoreet vitae, risus. 引用的区域也可以使用其他的 Markdown 语法 : 包括标题,列表,代码区块等. 列表 有序列表 : 数字 + . 无序列表 : * + - , 作用相同, 无差别. ** 层次化表示, 需要缩进. ** 转义 : \ 如 : 1991\.12\.12 代码区块 缩进4个空格 , 或者 1 个制表符. 代码区块会一直持续到没有缩进的那一行, 或是文件结尾. ** 代码区块中, 一般 Markdown 语法不会被转换. 代码 小段代码 `CODE` `` CODE `` ** 多个反引号时, 可以在代码中使用 反引号本身. ** 代码区段的起始和结束端都可以放入一个空白，起始端后面一个，结束端前面一个，这样你就可以在区段的一开始就插入反引号 ** 在代码区段内，&amp; 和尖括号都会被自动地转成 HTML 实体，这使得插入 HTML 原始码变得很容易 分割线 : 三个以上的星号,减号,底线来建立一个分割线, 行内不能有其他东西. 型号或减号之间可以插入空格. *** * * * --- - - - - - 链接 链接文字 : [文字] 行内式 [Name](http://www.baidu.com &quot;Title&quot;) 相对路径 [logo](/static/logo.jpg &quot;logo&quot;) 参考式 : 先定义, 后引用 定义 : 在文档的任意处, 把这个标记的链接内容定义出来： [id]: http://example.com &quot;Optionnal Title&quot; 引用 : 不区分大小写 [Name][id] 示例 : [foo]: http://example.com/ &quot;Optional Title Here&quot; [foo]: http://example.com/ &apos;Optional Title Here&apos; [foo]: http://example.com/ (Optional Title Here) [id]: &lt;http://example.com/&gt; &quot;Optional Title Here&quot; [link text][a] [link text][A] 强调 : *WORDS* : &lt;em&gt; _WORDS_ : &lt;em&gt; **WORD** : &lt;strong&gt; __WORD__ : &lt;strong&gt; \* : 转义 \_ : 转义 ** 如果 * 和 _ 两边都有空白的话，它们就只会被当成普通的符号。 图片 行内式 ![Alt text](/path/to/img.jpg) ![Alt text](/path/to/img.jpg &quot;Optional title&quot;) 参考式 ![Alt text][id] [id]: url/to/image &quot;Optional title attribute&quot; ** Markdown 无法指定图片的 宽高, 如果需要可以使用 &lt;img&gt; 标签. 自动链接 : 针对 URL 和 Email 地址 &lt;http://example.com/&gt; &lt;address@example.com&gt; 反斜杠 : 转义 Markdown 支持一下这些符号前面加上 反斜杠 来帮助插入普通的符号. \ 反斜线 ` 反引号 * 星号 _ 底线 {} 花括号 [] 方括号 () 括弧 # 井字号 + 加号 - 减号 . 英文句点 ! 惊叹号 免费编辑器 Windows 平台 MarkdownPad MarkPad Linux 平台 ReText Mac 平台 Mou 在线编辑器 Markable.in Dillinger.io 浏览器插件 MaDe (Chrome) 高级应用 Sublime Text 2 + MarkdownEditing / 教程]]></content>
      <categories>
        <category>tools</category>
      </categories>
      <tags>
        <tag>markdown</tag>
        <tag>标记语言</tag>
      </tags>
  </entry>
</search>
