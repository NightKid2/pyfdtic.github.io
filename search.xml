<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[算法复杂度速查表]]></title>
    <url>%2F2018%2F03%2F15%2F%E7%AE%97%E6%B3%95%E5%A4%8D%E6%9D%82%E5%BA%A6%E9%80%9F%E6%9F%A5%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[摘要 参考链接 图例绝佳不错一般不佳糟糕 数据结构操作数据结构时间复杂度空间复杂度&nbsp;平均最差最差&nbsp;访问搜索插入删除访问搜索插入删除&nbsp;ArrayO(1)O(n)O(n)O(n)O(1)O(n)O(n)O(n)O(n)StackO(n)O(n)O(1)O(1)O(n)O(n)O(1)O(1)O(n)Singly-Linked ListO(n)O(n)O(1)O(1)O(n)O(n)O(1)O(1)O(n)Doubly-Linked ListO(n)O(n)O(1)O(1)O(n)O(n)O(1)O(1)O(n)Skip ListO(log(n))O(log(n))O(log(n))O(log(n))O(n)O(n)O(n)O(n)O(n log(n))Hash Table-O(1)O(1)O(1)-O(n)O(n)O(n)O(n)Binary Search TreeO(log(n))O(log(n))O(log(n))O(log(n))O(n)O(n)O(n)O(n)O(n)Cartesian Tree-O(log(n))O(log(n))O(log(n))-O(n)O(n)O(n)O(n)B-TreeO(log(n))O(log(n))O(log(n))O(log(n))O(log(n))O(log(n))O(log(n))O(log(n))O(n)Red-Black TreeO(log(n))O(log(n))O(log(n))O(log(n))O(log(n))O(log(n))O(log(n))O(log(n))O(n)Splay Tree-O(log(n))O(log(n))O(log(n))-O(log(n))O(log(n))O(log(n))O(n)AVL TreeO(log(n))O(log(n))O(log(n))O(log(n))O(log(n))O(log(n))O(log(n))O(log(n))O(n) 数组排序算法算法时间复杂度空间复杂度&nbsp;最佳平均最差最差QuicksortO(n log(n))O(n log(n))O(n^2)O(log(n))MergesortO(n log(n))O(n log(n))O(n log(n))O(n)TimsortO(n)O(n log(n))O(n log(n))O(n)HeapsortO(n log(n))O(n log(n))O(n log(n))O(1)Bubble SortO(n)O(n^2)O(n^2)O(1)Insertion SortO(n)O(n^2)O(n^2)O(1)Selection SortO(n^2)O(n^2)O(n^2)O(1)Shell SortO(n)O((nlog(n))^2)O((nlog(n))^2)O(1)Bucket SortO(n+k)O(n+k)O(n^2)O(n)Radix SortO(nk)O(nk)O(nk)O(n+k) 图操作节点 / 边界管理存储增加顶点增加边界移除顶点移除边界查询Adjacency listO(|V|+|E|)O(1)O(1)O(|V| + |E|)O(|E|)O(|V|)Incidence listO(|V|+|E|)O(1)O(1)O(|E|)O(|E|)O(|E|)Adjacency matrixO(|V|^2)O(|V|^2)O(1)O(|V|^2)O(1)O(1)Incidence matrixO(|V| &sdot; |E|)O(|V| &sdot; |E|)O(|V| &sdot; |E|)O(|V| &sdot; |E|)O(|V| &sdot; |E|)O(|E|) 堆操作类型时间复杂度&nbsp;Heapify查找最大值分离最大值提升键插入删除合并Linked List (sorted)-O(1)O(1)O(n)O(n)O(1)O(m+n)Linked List (unsorted)-O(n)O(n)O(1)O(1)O(1)O(1)Binary HeapO(n)O(1)O(log(n))O(log(n))O(log(n))O(log(n))O(m+n)Binomial Heap-O(1)O(log(n))O(log(n))O(1)O(log(n))O(log(n))Fibonacci Heap-O(1)O(log(n))O(1)O(1)O(log(n))O(1) Big-O 复杂度图表]]></content>
  </entry>
  <entry>
    <title><![CDATA[创业的本质]]></title>
    <url>%2F2018%2F03%2F15%2F%E6%9D%82%E8%AE%B0-%E5%88%9B%E4%B8%9A%E7%9A%84%E6%9C%AC%E8%B4%A8%2F</url>
    <content type="text"><![CDATA[我从没有听到过他们的一丝抱怨，我看到的永远是他们冲在一线解决客户问题的身影。这种乐观的心态，这种对荣辱毫不计较的精神，才是真正的创业者精神。 对创业初心的坚持，就是对使命的坚持。使命一定是解决了他人的问题，而不是解决了自己的问题。解决他人的问题就是「利他」，商业的本质不是交换，而是「利他」。一定是在利他的基础上，才能产生交换的需求。这就是为什么多数成功公司的企业文化里，都会强调「利他」的原因。需要有对应的企业文化，来催生对应的组织机制，以完成相应的商业目标，这是一环扣一环的。很多管理者从书上借鉴了一些做法，但并没有理解这其中的深层次联系。如果理解了企业的根本是客户价值，客户价值的本质是利他，从而鼓励所有员工成为「利他」的人，企业文化才不会流于形式和口号，才能真正成为基业长青的基石。 解决他人的问题就是「利他」，商业的本质不是交换，而是「利他」。一定是在利他的基础上，才能产生交换的需求。 我会狂妄的宣称要去颠覆世界，现在看来，世界根本不需要被颠覆，也很难在短时间内颠覆一个行业或一个市场。世界需要的是变得更美好，这是创业公司应该追求的。 Make the world a batter place. 不要用战术上的勤奋，掩盖战略上的懒惰。 创业不是一将功成万骨枯，创业是一个团队的成功。 来源]]></content>
      <categories>
        <category>杂记</category>
      </categories>
      <tags>
        <tag>创业</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[fpm 制作 rpm 包]]></title>
    <url>%2F2018%2F03%2F15%2FOS-Linux-fpm-%E6%89%93%E5%8C%85%2F</url>
    <content type="text"><![CDATA[使用 fpm 打包 rpm 包. 支持的 源类型包 ① dir : 将目录打包成所需要的类型, 可用于源码编译安装软件包 ② rpm : 对 rpm 包进行转换 ③ gem : 对 rubygem 包进行转换 ④ python : 将 python 模块打包成响应的类型 支持的 目标类型包 ① rpm : 转换为 rpm 包 ② deb : 转换为 deb 包 ③ solaris : 转换为 solaris 包 ④ puppet : 转换为 puppet 模块 FPM 安装 及 使用帮助 : FPM 基于 ruby , 需要首先安装 ruby 环境. ruby &gt; 1.8.5 $ yum install ruby rubygems ruby-devel gcc make libffi-devel -y $ yum install rpm-build -y # fpm 依赖 rpmbuild $ gem sources list $ gem sources --remove https://rubygems.org/ $ gem sources --add https://ruby.taobao.org $ gem install fpm # for centos7 $ gem install json -v 1.8.3 # for centos6 $ gem install fpm -v 1.3.3 # for centos6 $ fpm --help -s 指定源类型 -t 指定目标类型，即想要制作为什么包 -n 指定包的名字 -v 指定包的版本号 -C 指定打包的相对路径 Change directory to here before searching forfiles -d 指定依赖于哪些包 -f 第二次打包时目录下如果有同名安装包存在，则覆盖它 -p 输出的安装包的目录，不想放在当前目录下就需要指定 --post-install 软件包安装完成之后所要运行的脚本；同--after-install --pre-install 软件包安装完成之前所要运行的脚本；同--before-install --post-uninstall 软件包卸载完成之后所要运行的脚本；同--after-remove --pre-uninstall 软件包卸载完成之前所要运行的脚本；同--before-remove --description 示例: 定制 nginx rpm 包 $ yum -y install pcre-devel openssl-devel libzip $ useradd nginx -M -s /sbin/nologin $ tar xf nginx_1.10.tar.gz $ cd nginx_1.10 $ ./configure --prefix=/opt/nginx --user=nginx --group=nginx --with-http_ssl_module --with-http_stub_status_module $ make &amp;&amp; make install $ echo &quot;nginx-1.10.2&quot; &gt; /opt/nginx/version $ vim /tmp/nginx_rpm.sh #!/bin/bash useradd nginx -M -s /sbin/nologin $ fpm -s dir -t rpm -n nginx -v 1.10.2 -d &apos;pcre-devel,openssl-devel,libzip&apos; --post-install /tmp/nginx_rpm.sh -f /opt/nginx # 注意此处的 绝对路径. $ rpm -qpl nginx-1.6.2-1.x86_64.rpm # 查看软件包内容. 有用的命令.$ yum provides *bin/prove provides Find what package provides the given value resolvedep Determine which package provides the given dependency $ tar -tvf new.tgz # 查看包的内容, 不解压包. $ make install DESTDIR=/tmp/installdir/ $ getent passwd root # 查看手否存在用户root root:x:0:0:root:/root:/bin/bash $ rpm -qp --scripts tengine-2.1.0-1.el6.x86_64.rpm # 查看 rpm 保存的脚本信息]]></content>
      <categories>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>rpm</tag>
        <tag>fpm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[AWS-S3]]></title>
    <url>%2F2018%2F03%2F15%2FAWS-S3%2F</url>
    <content type="text"><![CDATA[AWS 的数据存储服务 S3使用 S3 创建公司内部的文件服务器, 保存员工私人/共享文件, 并以类似 Dropbox 的方式双向同步. S3 介绍S3 是 AWS 最早发布的诸多服务之一, 用作可信存储. 可信 : 在指定年度内, 为对象提供 99.999999999% 的持久性和 高达 99.99% 的可用性. S3 提供如下特性 : 跨区域复制 : 只需要简单的配置，存储于S3中的数据会自动复制到选定的不同区域中。当你的数据对象的收集分散在不同的区域，而处理集中在某些区域时非常有用。 事件通知 : 当数据对象上传到 Amazon S3 中或从中删除的时候会发送事件通知。事件通知可使用 SQS 或 SNS 进行传送，也可以直接发送到 AWS Lambda 进行处理。 版本控制 : 数据对象可以启用版本控制，这样你就可以很方便地进行回滚。对于应用开发者来说，这是个特别有用的特性。 加密 S3的访问本身是支持 SSL（HTTPS）的，保证传输的安全，对于数据本身，你可以通过Server side encryption（AES256）来加密存储在S3的数据。 访问管理 通过 IAM/VPC 可以控制S3的访问粒度，你甚至可以控制一个bucket（S3对数据的管理单元，一个bucket类似于一组数据的根目录）里面的每个folder，甚至每个文件的访问权限。 可编程 可以使用 AWS SDK 进行客户端或者服务端的开发。 成本监控和控制 S3 有几项管理和控制成本的功能，包括管理成本分配的添加存储桶标签和接收账单警报的 Amazon Cloud Watch 集成。 灵活的存储选项 S3 Standard， Standard–Infrequent Access 选项可用于非频繁访问数据，存储的价格大概是 Standard 的 2/5。 Glacier : 用于存储冷数据（如N年前的Log），价格在 Standard 的 1/4，缺点是需要几个小时来恢复数据。 S3 操作方式consoleAWS CLI创建 bucket aws s3api create-bucket --bucket &lt;name&gt; 删除 bucket aws s3api delete-bucket --bucket &lt;name&gt; 像使用一般文件系统一样操作 S3 aws s3 ls aws s3 cp aws s3 rm 本地文件 与 S3 上文件同步 : aws s3 sync ./local_dir s3://my_bucket/my_dir AWS SDK使用的一般流程 : ① 创建 AWS Connection (需要 access key) ② 使用 connection 创建 S3 对象 ③ 使用 S3 API 进行各种操作. 使用 S3 的典型场景存储用户上传的文件, 如照片,视频等静态内容单做一个 k-v 存储, 承担简单的数据库服务功能数据备份静态网站的托管 : 可以对一个 bucket 使能 Web Hosting.参考]]></content>
      <categories>
        <category>AWS</category>
      </categories>
      <tags>
        <tag>S3</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[AWS-IAM权限控制]]></title>
    <url>%2F2018%2F03%2F15%2FAWS-IAM%E6%9D%83%E9%99%90%E6%8E%A7%E5%88%B6%2F</url>
    <content type="text"><![CDATA[AWS 的认证与权限管理服务 IAM 与 权限访问控制机制IAM , Identity and Access Management 基本概念ARN, Amazon Resource Name :在 AWS 里, 创建的任何资源都有其全局唯一的 ARN, 是访问控制可以到达的最小粒度. users, 用户groups, 组将用户添加到一个群组中, 可以自动获得这个群组的所有权限. roles, 角色没有任何访问凭证(密码或密钥), 他一般被赋予某个资源(包括用户), 那时起临时具有某些权限. 角色的密钥是动态创建的, 更新和失效都无需特别处理. permissions, 权限,权限可以赋给 用户,组, roles, 权限通过 policy document 描述, policy, 是描述权限的一段 JSON 文本. 示例如下{ &quot;Version&quot;: &quot;2012-10-17&quot;, &quot;Statement&quot;: [ { &quot;Effect&quot;: &quot;Allow&quot;, &quot;Action&quot;: &quot;*&quot;, &quot;Resource&quot;: &quot;*&quot; } ] } 用户或者群组只有添加了相关的 policy才会有响应的权限. policy 是 IAM 的核心内容. 是 JSON 格式来描述, 主要包含 Statement, 也就是 policy 拥有的权限的陈诉. 一言以蔽之: 谁在什么条件下能对哪些资源的哪些操作进行处理。 policy 的 PARCE 原则 : Principal : 谁 – 单独创建的 policy 是不需要指明 Principal 的, 当该 policy 被赋给用户,组或者 roles 时, principal 自动创建. Action : 那些操作 Resource : 那些资源 Condition : 什么条件 Effect : 如何处理 (Allow/Deny) 示例 { &quot;Version&quot;: &quot;2012-10-17&quot;, &quot;Statement&quot;: [ { &quot;Effect&quot;: &quot;Allow&quot;, &quot;Action&quot;: [ &quot;s3:Get*&quot;, &quot;s3:List*&quot; ], &quot;Resource&quot;: &quot;*&quot; } ] } 在这个policy里，Principal和Condition都没有出现。如果对资源的访问没有任何附加条件，是不需要Condition的；而这条policy的使用者是用户相关的principal（users, groups, roles），当其被添加到某个用户身上时，自然获得了principal的属性，所以这里不必指明，也不能指明。 Resource policy，它们不能单独创建，只能依附在某个资源之上（所以也叫inline policy），这时候，需要指明Principal。 示例如下 : { &quot;Version&quot;: &quot;2012-10-17&quot;, &quot;Statement&quot;: [ { &quot;Effect&quot;: &quot;Allow&quot;, &quot;Principal&quot;: &quot;*&quot;, &quot;Action&quot;: &quot;s3:GetObject&quot;, &quot;Resource&quot;: &quot;arn:aws:s3:::corp-fs-web-bucket/*&quot; } ] } 当我希望对一个S3 bucket使能Web hosting时，这个bucket里面的对象自然是要允许外界访问的，所以需要如下的inline policy： Condition 示例 &quot;Condition&quot;: { &quot;IPAddress&quot;: {&quot;aws:SourceIP&quot;: [&quot;10.0.0.0/8&quot;, &quot;4.4.4.4/32&quot;]}, &quot;StringEquals&quot;: {&quot;ec2:ResourceTag/department&quot;: &quot;dev&quot;} } &quot;Condition&quot;: { &quot;StringLike&quot;: { &quot;s3:prefix&quot;: [ &quot;tyrchen/*&quot; ] } } ** 在一条Condition下并列的若干个条件间是and的关系，这里IPAddress和StringEquals这两个条件必须同时成立； ** 在某个条件内部则是or的关系，这里10.0.0.0/8和4.4.4.4/32任意一个源IP都可以访问。 Policy 执行规则 : 默认情况下，一切资源的一切行为的访问都是Deny 如果在policy里显式Deny，则最终的结果是Deny 否则，如果在policy里是Allow，则最终结果是Allow 否则，最终结果是Deny]]></content>
      <categories>
        <category>AWS</category>
      </categories>
      <tags>
        <tag>IAM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Lua学习笔记]]></title>
    <url>%2F2018%2F03%2F15%2FLua%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[Lua 是一种高性能, 解释型, 面向对象的语句, 广泛用于各种项目的内嵌语言, 如 redis, nginx, scrapy, 愤怒的小鸟, 魔兽世界等等. 本文主要介绍 Lua 的语法. 1. 数据类型lua 是一个动态类型语言,一个变量可以存储类型的值.Lua 常用数据类型: 空(nil): 空类型只包含一个值,即nil . nil表示空, 没有赋值的变量或标的字段都是 nil. 布尔(boolean): 布尔类型包含 True 和 False 两个值. 数字(number): 整数合浮点数是都是使用数字类型存储. 字符串(string): 字符串类型可以存储字符串,且与Redis的键值一样都是二进制安全的.字符串可以使用单引号或双引号表示,两个符号是相同的. 字符串可以包含转义字符,如 ‘\n’,’\r’ 等. 表(table): 表类型是Lua 语言中唯一的数据结构,既可以当数组,又可以当字典,十分灵活. 函数(function): 函数是Lua中的一等值(first-class value),可以存储在变量中,作为函数的参数或返回结果. 2. 变量Lua 变量分为全局变量和局部变量. 全局变量无需声明就可以直接使用,默认值是 nil . &gt; print(b) a = 1 -- 为全局变量a赋值 a = nil -- 删除全局变量的方法是将其复制为 nil . 全局变量没有声明与未声明之分,只有非 nil 和 nil 的区别. print(b) -- 无需声明即可使用，默认值是nil 声明局部变量的方式为 “local 变量名” : local c --声明一个局部变量c，默认值是nil local d = 1 --声明一个局部变量d并赋值为1 local e, f --可以同时声明多个局部变量 * 局部变量的作用域为从声明开始到所在层的语句块的结尾. 声明一个存储函数的局部变量的方法为 : local say_hi = function () print &apos;hi&apos; end 变量名必须是非数字开头,只能包含字母,数字和下划线,区分大小写. 变量名不能与Lua的保留关键字相同, 保留关键字如下: and break do else elseif end false for function if in local nil not or repeat return then true until while 3. 注释 单行: -- 开始, 到行尾结束. 多行: --[[ ... ]] . 4. 赋值多重赋值 : local a, b = 1, 2 -- a的值是1，b的值是2 local c, d = 1, 2, 3 -- c的值是1，d的值是2，3被舍弃了 local e, f = 1 -- e的值是1，f的值是nil 在执行多重赋值时,Lua会先计算所有表达式的值,比如: local a = {1, 2, 3} local i = 1 i, a[i] = i + 1, 5 -- i = 2 ; a = {5,2,3} , lua 索引从 1 开始. lua 中的函数也可以返回多个值 5. 操作符5.1 数学操作符 :常见的+、-、*、/、%（取模）、-（一元操作符，取负）和幂运算符号^。 数学操作符的操作数如果是字符串,则会自动转换为数字. print(&apos;1&apos; + 1) -- 2 print(&apos;10&apos; * 2) -- 20 5.2 比较操作符 : == : 比较两个操作数的类型和值是否相等 ~= : 与 == 结果相反 &lt;,&gt;,&lt;=,&gt;= : 大于,小于,小于等于,大于等于. 比较操作符的结果一定是布尔类型 ; 比较操作符,不会对两边的操作数进行自动类型转换. 5.3 逻辑操作符 : not : 根据操作数的真和假返回false 和 true and : a and b, 如果a 是真,则返回 b , 否则返回 a . or : a or b , 如果a 是假,则返回 a , 否则返回 b . 只要操作数不是 nil 或 false ,逻辑操作符都认为操作数是真. 特别注意 0 或 空字符串也被当做真. Lua 逻辑操作符支持短路，也就是说对于 false and foo() ，lua 不会调用foo函数，or 类似。 5.4 连接操作符.... 用来连接两个字符串. 连接操作符会自动把数字类型的抓换成字符串类型. 5.5 取长度操作符.是lua5.1 新增的操作符, # ,用来获取字符串或表的长度. &gt; print(#&apos;hello&apos;) -- 5 5.6 运算符的优先级:^ not # -(一元) * / % + - .. &lt; &gt; &lt;= &gt;= ~= == and or 6. if 语句语法 : if 条件表达式 then 语句块 elseif 条件表达式 then 语句块 else 语句块 end 注意 : Lua 中只有 nil 和 false 才是假, 其余值,包括0 和空字符串,都被认为是真值. Lua 每个语句都可以 ; 结尾 ,但是一般来说编写 Lua 是会省略 ; , Lua 并不强制要求缩进,所有语句也可以写在一行中, 但为了增强可读性,建议在注意缩进. &gt; a = 1 b = 2 if a then b = 3 else b = 4 end 7. 循环语句7.1 while 循环while 条件表达式 do 语句块 end 7.2 repeat 循环repeat 语句块 until 条件表达式 7.3 for 循环形式一 :for 循环中的 i 是局部变量, 作用域为 for 循环体内. 虽然没有使用 local 声明,但它不是全局变量. for 变量=初值,终值,步长 do -- 步长可省略,默认为 1 语句块 end 示例 # 计算 1 ~ 100 之和 local sum = 0 for i = 1 ,100 do sum = sum + 1 end 形式二 :for 变量1 ,变量2, ... , 变量N in 迭代器 do 语句块 end 8. 表类型表是Lua中唯一的数据结构,可以理解为关联数组, 任何类型的值(除了空类型)都可以作为表的索引. a = {} --将变量a赋值为一个空表 a[&apos;field&apos;] = &apos;value&apos; --将field字段赋值value print(a.field) --打印内容为&apos;value&apos;，a.field是a[&apos;field&apos;]的语法糖。 people = { --也可以这样定义 name = &apos;tom&apos;, age = 29 } 当索引为整数的时候表和传统的数组一样，例如： a = {} a[1] = &apos;Tom&apos; a[2] = &apos;Jeff&apos; 可以写成下面这样： a = {&apos;Tom&apos;, &apos;Jeff&apos;} print(a[1]) --打印的内容为&apos;Tom&apos; 可以使用通用形式的for语句遍历数组,例如: for index,value in ipairs(a) do -- index 迭代数组a 的索引 ; value 迭代数组a 的值. print(index) print(value) end -- ipairs 是Lua 内置的函数,实现类似迭代器的功能. 数字形式的for语句 for i=1,#a do print(i) print(a[i]) end pair : 迭代器,用来遍历非数组的表值. person = { name = &apos;Tom&apos;, age = 29 } for index,value in pairs(person) do print(index) print(value) end pairs 与 ipairs 的区别在于前者会遍历所有值不为 nil 的索引, 而后者只会从索引 1 开始递增遍历到最后一个值不为 nil 的整数索引. 9. 函数一般形式: function(参数列表) 函数体 end 可以将函数赋值给一个局部变量, 比如: local square = function(num) return num*num end ** 因为在赋值前声明了局部变量 square, 所以可以在函数内部引用自身(实现递归). 函数参数 : 如果实参的个数小于形参的个数,则没有匹配到的形参的值为 nil . 相对应的,如果实参的个数大于形参的个数,则多出的实参会被忽略. 如果希望捕获多出的参数(即实现可变参数个数),可以让最后一个形参为 ... . local function square(...) local argv = {...} for i = 1,#argv do argv[i] = argv[i] * argv[i] end return unpack(argv) -- unpack 函数用来返回 表 中的元素. 相当于return argv[1], argv[2], argv[3] end a,b,c = square(1,2,3) print(a) -- 1 print(b) -- 4 print(c) -- 9 在 Lua 中, return 和 break (用于跳出循环) 语句必须是语句块中的最后一条语句, 简单的说在这两条语句之后只能是 end,else 或 until 三者之一. 如果希望在语句块中间使用这两条语句,可以认为的使用 do 和 end 将其包围. 10. 标准库 http://www.lua.org/manual/5.1/manual.html#5Lua 的标准库中提供了很多使用的函数, 比如 ipairs,pairs,tonumber,tostring,unpack 都属于标准库中的Base库. Redis 支持大部分Lua标准库,如下所示: 库名 说明 Base 一些基础函数 String 用于字符串操作的函数 Table 用于表操作的函数 Math 数学计算函数 Debug 调试函数 10.1 String库 : 可以通过字符串类型的变量以面向对象的形式访问, 如 string.len(string_var) 可以写成 string_var:len() 获取字符串长度 : string.len(string) 作用与操作符 “#” 类似 &gt; print(string.len(&apos;hello&apos;)) -- 5 &gt; print(#&apos;hello&apos;) -- 5 转换大小写 string.upper(string) string.lower(string) 获取子字符串 string.sub() 可以获取一个字符串从索引 start 开始到 end 结束的子字符串,索引从1 开始. 索引也可以是负数, -1 代表最后一个元素 . string.sub(string start[,end ]) -- end 默认为 -1. &gt; print(string.sub(&apos;hello&apos;,1)) -- hello &gt; print(string.sub(&apos;hello&apos;,2)) -- ello &gt; print(string.sub(&apos;hello&apos;,2,-2)) -- ell 10.2 Table库 : 其中大部分函数都需要表的形式是数组形式. 将数组转换为字符串 table.concat(table [,sep [,i [,j]]]) # sep : 以 sep 指定的参数分割, 默认为空. # i , j : 用来限制要转换的表元素的索引范围. 默认分别为 1 和 表的长度. 不支持负索引. print(table.concat({1,2,3})) –123print(table.concat({1,2,3},’,’,2)) –2,3print(table.concat({1,2,3},’,’,2,2)) –2 向数组中插入元素 table.insert(table ,[pos,] value) # 在指定索引位置 pos 插入元素 value, 并将后面的元素顺序后移. 默认 pos 值是数组长度加 1 , 即在数组尾部插入. &gt; a = {1,2,4} &gt; table.insert(a,3,3) # {1,2,3,4} &gt; table.insert(a,5) # {1,2,3,4,5} &gt; print(table.concat(a,&apos;,&apos;)) 1,2,3,4,5 从数组中弹出一个元素 table.remove(table,[,pos]) # 从指定的索引删除一个元素,并将后面的元素前移,返回删除元素值. 默认 pos 的值是数组的长度,即从数组尾部弹出一个元素. &gt; table.remove(a) --{1,2,3,4} &gt; table.remove(a,1) --{2,3,4} &gt; print(table.caoncat(a,&apos;,&apos;)) 2,3,4 10.3 Math库 : 提供常用的数学运算函数, 如果参数是字符串会自动尝试转换成数字.math.abs(x) # 绝对值 math.sin(x) # 三角函数sin math.cos(x) # 三角函数cos math.tan(x) # 三角函数tan math.ceil(x) # 进一取整, 1.2 取整后是 2 math.floor(x) # 向下取整, 1.8 取整后是 1 math.max(x,...) # 获得参数中的最大的值 math.min(x,...) # 获取参数中的最小的值 math.pow(x,y) # 获取 xy 的值 math.sqrt(x) # 获取 x 的平方根 math.random([m,[,n]]) # 生成随机数,没有参数 返回 [0,1]的实数, 参数 m 返回范围在 [1,m] 的整数, 同时提供 m n 返回范围在 [m,n] 的整数. math.randomseed(x) # 设置随机数种子, 同一种子生成的随机数相同. 11. 其他库Redis 还通过 cjson库 和 cmsgpack库 提供了对 JSON 和 MessagePack的支持. Redis自动加载了这两个库,在脚本中可以分别通过 cjson 和 cmsgpack 两个全局变量来访问对应的库. local people = { name = &apos;Tom&apos;, age = 29 } -- 使用 cjson 序列化成字符串 local json_people_str = cjson.encode(people) -- 使用 cmsgpack 序列化成字符串 local msgpack_people_str = cmsgpack.pack(people) -- 使用 cjson 将序列化后的字符串还原成表 local json_people_obj = cjson.decode(people) print(json_people_obj.name) -- 使用 cmshpack 将序列化后的字符串还原成表 local msgpack_people_obj = cmsgpack.unpack(people) print(msgpack_people_obj.name)]]></content>
      <categories>
        <category>Lua</category>
      </categories>
      <tags>
        <tag>lua</tag>
        <tag>redis</tag>
        <tag>scrapy</tag>
        <tag>nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scrapy]]></title>
    <url>%2F2018%2F03%2F15%2FScrapy%2F</url>
    <content type="text"><![CDATA[摘要 一. Scrapy 架构组件 P13 组件 描述 类型 ENGINE 引擎, SCHEDULER 调度器, DOWNLOADER 下载器, SPIDER 爬虫, MIDDLEWARE 中间件, ITEM PIPELINE 数据管道, 框架中的数据流 P14 对象 描述 REQUEST Scrapy 中的 HTTP 请求对象 RESPONSE Scrapy 中的 HTTP 响应对象 ITEM 从页面中爬去的一项数据 Request(url[, callback, method=&#39;GET&#39;, headers, body, cookies, meta, encoding=&#39;utf-8&#39;, priority=0, dont_filter=False, errback]) Response 对象 HtmlResponse TextResponse XmlResponse 二. Spiderstart_urls.parse / start_requests.callback --&gt; REQUEST --&gt; DOWNLOADER --&gt; RESPONSE --&gt; Selector/SelectorList[xpath/css][extract/re/extract_first/re_first] 三. Selector 提取数据 构造 Selector html = &quot; &lt;html lang=&quot;en&quot;&gt; &lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;Title&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h1&gt;Hello world!&lt;/h1&gt; &lt;p&gt;This is a line.&lt;/p&gt; &lt;/body&gt; &lt;/html&gt;&quot; # 使用文本字符串构造 from scrapy.selector import Selector selector = Selector(text=html) # 使用 Response 对象构造 from scrapy.http import HtmlResponse response = HtmlResponse(url=&quot;http://www.example.com&quot;, body=body, encoding=&apos;utf-8&apos;) selector = Selector(response=response) RESPONSE 内置 selector : 在第一次访问一个 Response 对象的 selector 属性时, Response 对象内部会以自身为参数, 自动创建 Selector 对象, 并将该 Selector 对象缓存, 以便下次使用. class TextResponse(Response): def __init__(self, *args, **kwargs): ... self._cached_selector = None ... @property def selector(self): from scrapy.selector import Selector if self._cached_selector is None: self._cached_selector = Selector(self) return self._cached_selector ... def xpath(self, query, **kwargs): return self.selector.xpath(query, **kwargs) def css(self, query): return self.selector.css(query) 选中数据 XPATH/CSS 提取数据 extract()/re()/extract_first()/re_first() 四. Item 封装数据Scrapy 提供一下两个类, 用户可以使用它们自定义数据类(如书籍信息), 封装爬取到的数据. Item 基类 : 自定义数据类的基类, 支持字典接口(访问 自定义数据类 中的字段与访问字典类似, 支持 get() 方法). Field 类 : 用来描述自定义数据类包含哪些字段(如name, price 等). 1. 自定义数据类, 只需继承 Item, 并创建一系列 Field 对象的类属性即可, 类似于 ORM 创建的 Model.from scrapy import Item, Field class BookItem(Item): name = Field() price = Field() class ForeignBookItem(BookItem): &quot;&quot;&quot; 扩展 BookItem 类 &quot;&quot;&quot; translator = Field() 2. Field 元数据class BookItem(Item): name = Field(a=123, b=[1,2,3]) price = Field(a=lambda x: x+2) # Field 是 Python 字典的子类, 可以通过键获取 Field 对象中的元数据. b = BookItem(name=100, price=101) b[&apos;name&apos;] # 100 b[&apos;price&apos;] # 101 b.fields # {&apos;name&apos;: {&apos;a&apos;: 123, &apos;b&apos;: [1, 2, 3]}, &apos;price&apos;: {&apos;a&apos;: &lt;function __main__.&lt;lambda&gt;&gt;}} 代码示例class BookItem(Item): ... # 当 authors 是一个列表而不是一个字符串时, 串行化为一个字符串. authors = Field(serializer=lambda x: &quot;|&quot;.join(x)) ... 以上例子中, 元数据键 serializer 时 CSVItemExportr 规定好的, 他会用该键获取元数据, 即一个串行化函数对象, 并使用这个串行化函数将 authors 字符串行化成一个字符串, 具体代码参考 scrapy/exporters.py 文件. 五. Item Pipeline 处理数据在 Scrapy 中, Item Pipeline 是处理数据的组件, 一个 Item Pipeline 就是一个包含特定接口的类, 通常只负责一种功能的数据处理, 在一个项目中可以同时启动多个 Item Pipeline, 他们按指定次序级联起来, 形成一条数据处理流水线. Item Pipeline 典型应用 数据请求 验证数据有效性 过滤重复数据 将数据存入数据库. 1. 编写 pipelineclass PriceConverterPipeline(object): # 英镑兑换人民币汇率 exchange_rate = 8.5309 def process_item(self, item, spider): # 提取 item 的 price 字段(如 £ 53.74), 去掉 £ 符号, 转换为 float 类型, 乘以汇率 price = float(item[&quot;price&quot;][1:]) * self.exchange_rate # 保留两位小数赋值, item[&quot;price&quot;] = &quot;¥ %.2f&quot; % price return item 一个 Item Pipeline 不需要继承特定基类, 只需要实现某些特定的方法. process_item(self, item, spider) : 该方法必须实现. 该方法用来处理每一项由 Spider 爬取到的数据, 其中 item 为爬取到的一项数据, Spider 为爬取此项数据的 Spider 对象. process_item 是 Item Pipeline 的核心, process_item 返回的一项数据, 会传递给下一级 Item Pipeline(如果有) 继续处理. 如果 process_item 在处理某项 item 时抛出 DropItem(scrapy.exceptions.DropItem) 异常, 该项 item 便会被抛弃, 不会传递到下一级 Item Pipeline, 也不会导出到文件. 通常, 在检测到无效数据, 或者希望过滤的数据时, 抛出该异常. open_spider(self, spider) Spider 打开时(处理数据前), 回调该方法, 通常该方法用于在 开始处理数据之前完成某些初始化的工作, 如连接数据库. close_spider(self, spider) Spider 关闭时(处理数据后), 回调该方法, 通常该方法用于在 处理完所有数据之后完成某些清理工作, 如关闭数据库链接. from_crawler(cls, crawler) 创建 Item Pipeline 对象时回调该类方法. 通常, 在该方法中通过 crawler.settings 读取配置, 根据配置创建 Item Pipeline 对象. 如果一个 Item Pipeline 定义了 from_crawler 方法, Scrapy 就会调用该方法来创建 Item Pipeline 对象. 该方法的两个参数: cls : Item Pipeline 类的对象, crawler : crawler 是 Scrapy 中的一个核心对象, 可以通过 crawler.settings 属性访问配置文件. 2. 启用 Item Pipeline.$ cat settings.py ITEM_PIPELINES = { &apos;example.pipelines.PriceConverterPipeline&apos;: 300, } ITEM_PIPELINES 是一个字典, 其中每一项 Item Pipeline 类的导入路径, 值是一个 0~1000 的数字, 同时启用多个 Item Pipeline 时, Scrapy 根据这些数值决定各 Item Pipeline 处理数据的先后次序, 数值小的优先级高. 3. 代码示例3.1 Item Pipeline : 过滤重复数据class DuplicatesPipeline(object): def __init__(self): self.book_set = set() def process_item(self, item, spider): name = item[&quot;name&quot;] if name in self.book_set: raise DropItem(&quot;Duplicate book found: %s&quot; % item) self.book_set.add(name) return item 3.2 Item Pipeline : 将数据存储 MongoDB# pipelines.py class MongoDBPipeline(object): @classmethod def from_crawler(cls, crawler): &quot;&quot;&quot; 使用 settings.py 配置文件, 配置 MongoDB 数据库, 而不是硬编码 如果一个 Item Pipeline 定义了 from_crawler 方法, Scrapy 就会调用该方法来创建 Item Pipeline 对象. &quot;&quot;&quot; cls.DB_URI = crawler.settings.get(&quot;MONGO_DB_URI&quot;, &quot;mongodb://localhost:27017/&quot;) cls.DB_NAME = crawler.settings.get(&quot;MONGO_DB_NBAME&quot;, &quot;scrapy_data&quot;) return cls() def open_spider(self, spider): &quot;&quot;&quot;在开始处理数据之前, 链接数据库&quot;&quot;&quot; self.client = pymongo.MongoClient(self.DB_URI) self.db = self.client[self.DB_NAME] def close_spider(self, spider): &quot;&quot;&quot;数据处理完成之后, 关闭数据库链接&quot;&quot;&quot; self.client.close() def process_item(self, item, spider): &quot;&quot;&quot;将 item 数据 写入 MongoDB&quot;&quot;&quot; collection = self.db[spider.name] post = dict(item) if isinstance(item, Item) else item collection.insert_one(post) return item # settings.py MONGO_DB_URI = &quot;mongodb://192.168.1.1:27017/&quot; MONGO_DB_NBAME = &quot;my_scrapy_data&quot; ITEM_PIPELINES = { &quot;toscrapt_book.pipelines.MongoDBPipeline&quot;: 403, } 3.3 Item Pipeline : 将数据存储 Mysql版本一 : # pipelines.py class MySQLPipeline(object): def open_spider(self, spider): db = spider.settings.get(&quot;MYSQL_DB_NAME&quot;, &quot;scrapy_data&quot;) host = spider.settings.get(&quot;MYSQL_HOST&quot;, &quot;locahost&quot;) port = spider.settings.get(&quot;MYSQL_PORT&quot;, &quot;3306&quot;) user = spider.settings.get(&quot;MYSQL_USER&quot;, &quot;root&quot;) password = spider.settings.get(&quot;MYSQL_PASSWORD&quot;, &quot;123456&quot;) self.db_conn = MySQLdb.connect(host=host, port=port, db=db, user=user, passwd=password, charset=&quot;utf-8&quot;) self.db_cur = self.db_conn.cursor() def closer_spider(self, spider): self.db_conn.commit() self.db_conn.close() def procecss_item(self, item, spider): self.insert_db(item) return item def insert_db(self, item): values = ( item[&quot;name&quot;], item[&quot;price&quot;], item[&quot;rating&quot;], item[&quot;number&quot;] ) sql = &quot;INSERT INTO books VALUES (%s, %s, %s, %s, )&quot; self.db_cur.execute(sql, values) # settings.py MYSQL_DB_NAME = &quot;scrapy_data&quot; MYSQL_HOST = &quot;locahost&quot; MYSQL_PORT = &quot;3306&quot; MYSQL_USER = &quot;root&quot; MYSQL_PASSWORD = &quot;123456&quot; ITEM_PIPELINES = { &quot;toscrapt_book.pipelines.MySQLPipeline&quot;: 403, } 版本二: Scrapy 框架本身使用 Twisted 编写, Twisted 是一个事件驱动型的异步网络框架, 鼓励用户编写异步代码, Twisted 中提供了以异步方式多线程访问数据库的模块 adbapi, 使用该模块可以显著提高程序访问数据库的效率. # pipelines.py from twisted.enterprise import adbapi class MySQLAsyncPipeline(object): def open_spider(self, spider): db = spider.settings.get(&quot;MYSQL_DB_NAME&quot;, &quot;scrapy_data&quot;) host = spider.settings.get(&quot;MYSQL_HOST&quot;, &quot;locahost&quot;) port = spider.settings.get(&quot;MYSQL_PORT&quot;, &quot;3306&quot;) user = spider.settings.get(&quot;MYSQL_USER&quot;, &quot;root&quot;) password = spider.settings.get(&quot;MYSQL_PASSWORD&quot;, &quot;123456&quot;) # adbapi.ConnectionPool 可以创建一个数据库连接池对象, 其中包含多个链接对象, 每个链接对象在单独的线程中工作. # adbapi 只提供异步访问数据库的框架, 其内部依然使用 MySQLdb, sqlite3 这样的库访问数据库. self.dbpool = adbapi.ConnectionPool(&quot;MySQLdb&quot;, host=host, database=db, user=user, password=password, charset=&quot;utf-8&quot;) def close_spider(self, spider): self.dbpool.close() def process_item(self, item, spider): # 以异步方式调用 insert_db 方法, 执行完 insert_db 方法之后, 链接对象自动调用 commit 方法. self.dbpool.runInteraction(self.insert_db, item) return item def insert_db(self, tx, item): # tx 是一个 Transaction 对象, 其接口与 Cursor 对象类似, 可以调用 execute 执行 SQL 语句. values = ( item[&quot;name&quot;], item[&quot;price&quot;], item[&quot;rating&quot;], item[&quot;number&quot;] ) sql = &quot;INSERT INTO books VALUES (%s, %s, %s, %s, )&quot; tx.execute(sql, values) # settings.py MYSQL_DB_NAME = &quot;scrapy_data&quot; MYSQL_HOST = &quot;locahost&quot; MYSQL_PORT = &quot;3306&quot; MYSQL_USER = &quot;root&quot; MYSQL_PASSWORD = &quot;123456&quot; ITEM_PIPELINES = { &quot;toscrapt_book.pipelines.MySQLAsyncPipeline&quot;: 403, } 3.4 Item Pipeline : 将数据存储 Redis# pipelines.py import redis from scrapy import Item class RedisPipeline(object): def open_spider(self, spider): db_host = spider.settings.get(&quot;REDIS_HOST&quot;, &quot;localhost&quot;) db_port = spider.settings.get(&quot;REDIS_PORT&quot;, &quot;6379&quot;) db_index = spider.settings.get(&quot;REDIS_DB_INDEX&quot;, 0) self.db_conn = redis.StrictRedis(host=db_host, port=db_port, db=db_index) self.item_i = 0 def close_spider(self, spider): self.db_conn.connection_pool.disconnect() def process_item(self, item, spider): self.insert_db(item) return item def insert_db(self, item): if isinstance(item, Item): item = dict(item) self.item_i += 1 self.db_conn.hmset(&quot;book:%s&quot; % self.item_i, item) # settings.py &quot;REDIS_HOST&quot; = &quot;localhost&quot; &quot;REDIS_PORT&quot; = 6379 &quot;REDIS_DB_INDEX&quot; = 0 ITEM_PIPELINES = { &quot;toscrapt_book.pipelines.RedisPipeline&quot;: 403, } 六. LinkExtractor 提取链接提取页面中的链接, 有两种方法: selector : 提取少量链接时, 或提取规则比较简单. LinkExtractor : 专门用于提取链接的类 LinkExtractor. 1. 使用示例:# url from selector # next_url = response.css(&quot;ul.pager li.next a::attr(href)&quot;).extract_first() # if next_url: # next_url = response.urljoin(next_url) # yield scrapy.Request(next_url, callback=self.parse) # url from LinkeExtractor le = LinkExtractor(restrict_css=&quot;ul.pager li.next&quot;) links = le.extract_links(response) # 返回一个列表, 其中每一个元素都是一个 Link 对象, Link 对象的 url 属性便是链接页面的 绝对 url 地址. if links: next_url = links[0].url yield scrapy.Request(next_url, callback=self.parse) 2. 链接提取规则:LinkExtractor 构造器的所有参数都有默认值, 如果构造对象时, 不传递任何参数(使用默认值), 则提取页面中的所有链接. allow : 接受一个正则表达式或一个正则表达式列表, 提取绝对 url 与 正则表达式匹配的链接, 如果该参数为空(默认), 则提取全部链接. &gt;&gt;&gt; pattern = &apos;/intro/.+\.html$&apos; &gt;&gt;&gt; le = LinkExtractor(allow=pattern) &gt;&gt;&gt; links = le.extract_links(response) deny : 接受一个正则表达式或一个正则表达式列表, 与 allow 相反, 排除绝对 url 与正则表示匹配的链接. &gt;&gt;&gt; pattern = &apos;^http://example.com&apos; &gt;&gt;&gt; le = LinkExtractor(deny=pattern) &gt;&gt;&gt; links = le.extract_links(response) allow_domains : 接受一个域名或一个域名列表, 提取到指定域的链接. &gt;&gt;&gt; domains = [&quot;github.com&quot;, &quot;stackoverflow.com&quot;] &gt;&gt;&gt; le = LinkExtractor(allow_domains=domains) &gt;&gt;&gt; links = le.extract_links(response) deny_domains : 接受一个域名或一个域名列表, 与 allow_domains 相反, 排除到指定域的链接. &gt;&gt;&gt; domains = [&quot;github.com&quot;, &quot;stackoverflow.com&quot;] &gt;&gt;&gt; le = LinkExtractor(deny_domains=domains) &gt;&gt;&gt; links = le.extract_links(response) restrict_xpaths : 接受一个 Xpath 表达式或一个 Xpath 表达式列表, 提取 XPath 表达式选中区域下的的链接. &gt;&gt;&gt; le = LinkExtractor(restrict_xpaths=&quot;//div[@id=&apos;top&apos;]&quot;) &gt;&gt;&gt; links = le.extract_links(response) restrict_css : 接受一个 CSS 选择器或一个 CSS 选择器列表, 提取 CSS 选择器选中区域下的的链接. &gt;&gt;&gt; le = LinkExtractor(restrict_css=&quot;div#bottom&quot;) &gt;&gt;&gt; links = le.extract_links(response) tags : 接受一个标签(字符串)或一个标签列表, 提取指定标签内的链接, 默认为 [&quot;a&quot;, &quot;area&quot;] attrs : 接受一个属性(字符串)或一个属性列表, 提取指定属性内的链接, 默认为 [&quot;href&quot;] # &lt;script type=&quot;text/javascript&quot; src=&quot;/js/app.js&quot; /&gt; &gt;&gt;&gt; le = LinkExtractor(tags=&quot;script&quot;, attrs=&quot;src&quot;) &gt;&gt;&gt; links = le.extract_links(response) process_value : 接受一个形如 func(value) 的回调函数. 如果传递了该参数, LinkExtractor 将调用该回调函数对提取的每一个链接(如 a 的 href) 进行处理, 回调函数正常情况下应该返回一个字符串(处理结果), 想要抛弃所处理的连接时, 返回 None. # &lt;a href=&quot;javascript:goToPage(&apos;/doc.html&apos;); return false&quot;&gt;文档&lt;/a&gt; &gt;&gt;&gt; import re &gt;&gt;&gt; def process(value): m = re.search(&quot;javascript:goToPage\(&apos;(.*?)&apos;&quot;, value) # 如果匹配, 就提取其中 url 并返回, 不匹配则返回原值. if m: value = m.group() return value &gt;&gt;&gt; le = LinkExtractor(process_value=process) &gt;&gt;&gt; links = le.extract_links(response) 七. Exporter 导出数据在 Scrapy 中, 负责导出数据的组件被称为 Exporter(导出器), Scrapy 内部实现了多个 Exporter , 每个 Exporter 实现一种数据格式的导出. 支持的数据导出格式如下(括号内为对应的 Exporter): JSON (JsonItemExporter) JSON lines (JsonLinesItemExporter) CSV (CsvItemExporter) XML (XmlItemExporter) Pickle (PickleItemExporter) Marshal (MarshalItemExporter) Scrapy 爬虫会议 -t 参数中的数据格式字符串(如 csv, json, xml) 为键, 在配置字典 FEED_EXPORTERS 中搜索 Exporter, FEED_EXPORTERS 的内容由以下两个字典的内容合并而成: 默认配置文件中的 FEED_EXPORTERS_BASE, 为 Scrapy 内部支持的导出数据格式, 位于 scrapy.settings.default_settings 用户配置文件中的 FEED_EXPORTERS, 为用户自定义的导出数据格式, 在配置文件 settings.py 中. FEED_EXPORTERS = {&quot;excel&quot;: &quot;my_propject.my_exporters.ExcelItemExporter&quot;} 1. 导出数据方式: 命令行参数 $ scrapy crawl CRAWLER -t FORMAT -o /path/to/save.file $ scrapy crawl books -t csv -o books.data $ scrapy crawl books -t xml -o books.data $ scrapy crawl books -t json -o books.data $ scrapy craw books -o books.csv # Scrapy 可以通过文件后缀名, 推断出文件格式, 从而省去 -t 参数. 如 `-o books.json` -o /path/to/file 支持变量: 如 scrapy crawl books -o &#39;export_data/%(name)s/%(time)s.csv&#39; %(name)s : Spider 的名字 %(time)s : 文件创建时间 通过配置文件指定. 常用选项如下: FEED_URL : 导出文件路径 FEED_URL = &apos;export_data/%(name)s/%(csv)s.data&apos; FEED_FORMAT : 导出数据格式 FEED_FORMAT = &apos;csv&apos; FEED_EXPORT_ENCODING : 导出文件编码, 默认 json 使用数字编码, 其他使用 utf-8 编码 FEED_EXPORT_ENCODING = &quot;gbk&quot; FEED_EXPORT_FIELDS : 导出数据包含的字段(默认情况下, 导出所有字段), 并指定导出顺序. FEED_EXPORT_FIELDS = [&quot;name&quot;, &quot;author&quot;, &quot;price&quot;] FEED_EXPORTERS : 用户自定义的 Exporter 字典, 添加新的导出数据格式时使用. FEED_EXPORTERS = {&quot;excel&quot;: &quot;my_project.my_exporters.ExcelItemExporter&quot;} 2. 自定义数据导出格式import six from scrapy.utils.serialize import ScrapyJSONEncoder import xlwt class BaseItemExporter(object): def __init__(self, **kwargs): self._configure(kwargs) def _configure(self, options, dont_fail=False): self.encoding = options.pop(&quot;encoding&quot;, None) self.fields_to_export = options.pop(&quot;field_to_export&quot;, None) self.export_empty_fields = options.pop(&quot;export_empty_fields&quot;, False) if not dont_fail and options: raise TypeError(&quot;Unexpected options: %s&quot; % &apos;,&apos;.join(options.keys())) def export_item(self, item): &quot;&quot;&quot; 负责导出爬去到的每一项数据, 参数 item 为一项爬取到的数据, 每个子类必须实现该方法. :param item: :return: &quot;&quot;&quot; raise NotImplementedError def serialize_field(self, field, name, value): serializer = field.get(&quot;serializer&quot;, lambda x: x) return serializer(value) def start_exporting(self): &quot;&quot;&quot; 在导出开始时被调用, 可在该方法中执行某些初始化操作. :return: &quot;&quot;&quot; pass def finish_exporting(self): &quot;&quot;&quot; 在导出完成时被调用, 可在该方法中执行某些清理工作. :return: &quot;&quot;&quot; pass def _get_serialized_field(self, item, default_value=None, include_empty=None): &quot;&quot;&quot; Return the fields to export as an iterable of tuples (name, serialized_value) :param item: :param default_value: :param include_empty: :return: &quot;&quot;&quot; if include_empty is None: include_empty = self.export_empty_fields if self.fields_to_export is None: if include_empty and not isinstance(item, dict): field_iter = six.iterkeys(item.fields) else: field_iter = six.iterkeys(item) else: if include_empty: field_iter = self.fields_to_export else: field_iter = (x for x in self.fields_to_export if x in item) for field_name in field_iter: if field_name in item: field = {} if isinstance(item, dict) else item.fields[field_name] value = self.serialize_field(field, field_name, item[field_name]) else: value = default_value yield field_name, value # json class JsonItemExporter(BaseItemExporter): def __init__(self, file, **kwargs): self._configure(kwargs, dont_fail=True) self.file = file kwargs.setdefault(&quot;ensure_ascii&quot;, not self.encoding) self.encoder = ScrapyJSONEncoder(**kwargs) self.first_item = True def start_exporting(self): &quot;&quot;&quot; 保证最终导出结果是一个 json 列表. :return: &quot;&quot;&quot; self.file.write(b&quot;[\n&quot;) def finish_exporting(self): &quot;&quot;&quot; 保证最终导出结果是一个 json 列表. :return: &quot;&quot;&quot; self.file.write(b&quot;\n]&quot;) def export_item(self, item): &quot;&quot;&quot; 调用 self.encoder.encode 将每一项数据转换成 json 串. :param item: :return: &quot;&quot;&quot; if self.first_item: self.first_item = False else: self.file.write(b&quot;,\n&quot;) itemdict = dict(self._get_serialized_field(item)) data = self.encoder.encode(itemdict) self.file.write(to_bytes(data, self.encoding)) # 自定义 excel 导出格式. class ExcelItemExporter(BaseItemExporter): def __init__(self, file, **kwargs): self._configure(kwargs) self.file = file self.wbook = xlwt.Workbook() self.wsheet = self.wbook.add_sheet(&quot;scrapy&quot;) self.row = 0 def finish_exporting(self): self.wbook.save(self.file) def export_item(self, item): fields = self._get_serialized_field(item) # 获取所有字段的迭代器. for col, v in enumerate(x for _, x in fields): self.wsheet.write(self.row, col, v) self.row += 1 $ vim settings.py # my_exporters.py 与 settings.py 位于同级目录下. FEED_EXPORTERS = {&quot;excel&quot;: &quot;example.my_exporters.ExcelItemExporter&quot;} 八. 下载文件(FilesPipeline)和图片(ImagesPipeline)FilesPipeline 和 ImagesPipeline 可以看做两个特殊的下载器, 用户使用时, 只需要铜鼓 item 的一个特殊字段将要下载文件或图片的 URL 传递给他们, 他们会自动将文件或图片下载到本地, 并将下载结果信息存入 item 的另一个特殊字段, 以便用户在导出文件中查阅. 1. FilesPipeline 使用方法 在 settings.py 中启用 FilesPipeline, 通常将其置于其他 Item Pipelines 之前 ITEM_PIPELINES = {&quot;scrapy.pipelines.files.FilesPipeline&quot;: 1} 在 settings.py 中使用 FILES_STORE 指定文件下载目录. FILES_STORE = &quot;/path/to/my/download&quot; 下载文件 在 Spider 解析一个包含文件下载链接的也面试, 将所有需要下载文件的 url 地址收集到一个列表, 赋给 item 的 file_urls 字段(item[&quot;file_urls&quot;]). FilesPipeline 在处理每一项 item 时, 会读取 item[&#39;file_urls&#39;], 对其中每一个 url 进行下载. class DownloadBookSpider(scrapy.Spider): ... def parse(response): item = {} item[&quot;file_urls&quot;] = [] for url in response.xpath(&quot;//a/@href&quot;).extract(): download_url = response.urljoin(url) item[&quot;file_urls&quot;].append(download_url) yield item 当 FilesPipeline 下载完 item[&quot;file_urls&quot;] 中的所有文件后, 会将各文件的下载结果信息收集到另一个列表, 赋给 item[&quot;files&quot;] 字段, 下载信息结果包含以下内容: Path : 文件下载到本地的路径, 相对于 FILES_STORE 的相对路径. Checksum : 文件的校验和 url : 文件的 url 地址. 示例代码# items.py import scrapy class MatplotlibFileItem(scrapy.Item): file_urls = scrapy.Field() files = scrapy.Field() # spiders/matplotlib.py import scrapy from scrapy.linkextractors import LinkExtractor from ..items import MatplotlibFileItem class MatplotlibSpider(scrapy.Spider): name = &apos;matplotlib&apos; allowed_domains = [&apos;matplotlib.org&apos;] start_urls = [&apos;https://matplotlib.org/examples/index.html&apos;] def parse(self, response): # 爬取所有 二级 页面地址 le = LinkExtractor(restrict_css=&quot;div.toctree-wrapper.compound&quot;, deny=&quot;/index.html$&quot;) links = le.extract_links(response) for link in links: yield scrapy.Request(link.url, callback=self.parse_page) def parse_page(self, response): href = response.css(&quot;a.reference.external::attr(href)&quot;).extract_first() url = response.urljoin(href) example = MatplotlibFileItem() example[&quot;file_urls&quot;] = [url] yield example # pipelines.py : 重写 FilesPipeline 的 file_path 代码, 以自定义保存路径. from scrapy.pipelines.files import FilesPipeline from urlparse import urlparse from os.path import basename, dirname, join class MyFilesPipeline(FilesPipeline): def file_path(self, request, response=None, info=None): path = urlparse(request.url).path return join(basename(dirname(path)), basename(path)) # settings.py ITEM_PIPELINES = { # &quot;scrapy.pipelines.files.FilesPipeline&quot;: 1, &apos;matplotlib_file.pipelines.MyFilesPipeline&apos;: 1, } FILES_STORE = &quot;example_src&quot; # 文件下载路径 # 运行: $ scrapy crawl matplotlib -o examp.json 2. ImagesPipeline图片本身也是文件, ImagesPipeline 是 FilesPipeline 的子类, 使用上和 FilesPipeline 大同小异, 只是在所使用的 item 字段和配置选项上略有差别. Desc FilesPipeline ImagesPipeline 导入路径 scrapy.pipelines.files.FilesPipeline scrapy.pipelines.images.ImagesPipeline Item 字段 file_urls, files image_urls, images 下载目录 FILES_STORE IMAGES_STORE 2.1 生成缩略图在 settings.py 中设置 IMAGES_THUMBS, 他是一个字典, 每一项的值是缩略图的尺寸. IMAGES_THUMBS = { &quot;small&quot;: (50, 50), &quot;big&quot;: (270, 270) } 开启该功能后, 下载一张图片时, 会在本地出现 3 张图片, 其存储路径如下: [IMAGES_STORE]/full/name.jpg [IMAGES_STORE]/thumbs/small/name.jpg [IMAGES_STORE]/thumbs/big/name.jpg 2.2 过滤尺寸过小的图片在 settings.py 中设置 IMAGES_MIN_WIDTH 和 IMAGES_MIN_HEIGHT IMAGES_MIN_WIDTH = 110 IMAGES_MIN_HEIGHT = 110 示例代码# settings.py ITEM_PIPELINES = { &quot;scrapy.pipelines.images.ImagesPipeline&quot;: 1, # &apos;so_img.pipelines.SoImgPipeline&apos;: 300, } IMAGES_STORE = &apos;download_images&apos; # spider import json import scrapy class ImagesSpider(scrapy.Spider): IMG_TYPE = &quot;wallpaper&quot; IMG_START = 0 BASE_URL = &quot;http://image.so.com/zj?ch=%s&amp;sn=%s&amp;listtype=new&amp;temp=1&quot; name = &apos;wallpaper&apos; # allowed_domains = [&apos;image.so.com&apos;] start_urls = [BASE_URL % (IMG_TYPE, IMG_START)] MAX_DOWNLOAD_NUM = 100 start_index = 0 def parse(self, response): infos = json.loads(response.body.decode(&quot;utf-8&quot;)) yield {&quot;image_urls&quot;: [info[&quot;qhimg_url&quot;] for info in infos[&quot;list&quot;]]} # 如果 count 字段大于 0, 并且下载数量不足 MAX_DOWNLOAD_NUM, 继续获取下一页信息. self.start_index += infos[&quot;count&quot;] if infos[&quot;count&quot;] &gt; 0 and self.start_index &lt; self.MAX_DOWNLOAD_NUM: yield scrapy.Request(self.BASE_URL % (self.IMG_TYPE, self.start_index)) # 爬取图片 $ scrapy crawl wallpaper 九. 模拟登陆Scrapy 提供一个 FormRequest 类(Request 的子类), 专门用于构造含有表单数据的请求, FormRequest 的构造器方法有一个 formdata 参数, 接受字典形式的表单数据. 直接构造 FormRequest $ scrapy shell http://example.webscraping.com/places/default/user/login &gt;&gt;&gt; sel = response.xpath(&apos;//div[@style]/input&apos;) &gt;&gt;&gt; sel [&lt;Selector xpath=&apos;//div[@style]/input&apos; data=u&apos;&lt;input name=&quot;_next&quot; type=&quot;hidden&quot; value=&apos;&gt;, &lt;Selector xpath=&apos;//div[@style]/input&apos; data=u&apos;&lt;input name=&quot;_formkey&quot; type=&quot;hidden&quot; val&apos;&gt;, &lt;Selector xpath=&apos;//div[@style]/input&apos; data=u&apos;&lt;input name=&quot;_formname&quot; type=&quot;hidden&quot; va&apos;&gt;] &gt;&gt;&gt; fd = dict(zip(sel.xpath(&apos;./@name&apos;).extract(), sel.xpath(&apos;./@value&apos;).extract())) &gt;&gt;&gt; fd {u&apos;_formkey&apos;: u&apos;9c751a58-3dc2-489f-bf7b-93c31fa00c7f&apos;, u&apos;_formname&apos;: u&apos;login&apos;, u&apos;_next&apos;: u&apos;/places/default/index&apos;} &gt;&gt;&gt; fd[&apos;email&apos;] = &quot;liushuo@webscraping.com&quot; &gt;&gt;&gt; fd[&apos;password&apos;] = &quot;12345678&quot; &gt;&gt;&gt; fd {u&apos;_formkey&apos;: u&apos;9c751a58-3dc2-489f-bf7b-93c31fa00c7f&apos;, u&apos;_formname&apos;: u&apos;login&apos;, u&apos;_next&apos;: u&apos;/places/default/index&apos;, &apos;email&apos;: &apos;liushuo@webscraping.com&apos;, &apos;password&apos;: &apos;12345678&apos;} &gt;&gt;&gt; from scrapy.http import FormRequest &gt;&gt;&gt; request = FormRequest(&quot;http://example.webscraping.com/places/default/user/login&quot;, formdata=fd) &gt;&gt;&gt; fetch(request) &gt;&gt;&gt; response.url &apos;http://example.webscraping.com/places/default/index&apos; &gt;&gt;&gt; &quot;Welcome&quot; in response.text True 调用 FormRequest 的 from_response 方法 调用时, 只需传入一个 Response 对象作为第一个参数, 该方法会解析 Response 对象所包含的页面中的 元素, 帮助用户创建 FormRequest 对象, 并将隐藏 中的信息自动填入表单数据. $ scrapy shell http://example.webscraping.com/places/default/user/login &gt;&gt;&gt; fd = {&quot;email&quot;: &quot;liushuo@webscraping.com&quot;, &quot;password&quot;: &quot;12345678&quot;} &gt;&gt;&gt; from scrapy.http import FormRequest &gt;&gt;&gt; req = FormRequest.from_response(response, fd) &gt;&gt;&gt; fetch(req) &gt;&gt;&gt; response.url &apos;http://example.webscraping.com/places/default/index&apos; 1. 实现登录 Spiderclass LoginSpider(scrapy.Spider): name = &quot;login&quot; allowed_domains = [&quot;example.webscraping.com&quot;] start_urls = [&quot;http://example.webscraping.com/places/default/user/profile&quot;] login_url = &quot;http://example.webscraping.com/places/default/user/login&quot; def parse(self, response): keys = response.css(&quot;table label::text&quot;).re(&quot;(.+):&quot;) values = response.css(&quot;table td.w2p_fw::text&quot;).extract() yield dict(zip(keys, values)) def start_requests(self): yield scrapy.Request(self.login_url, callback=self.login) def login(self, response): fd = {&quot;email&quot;: &quot;liushuo@webscraping.com&quot;, &quot;password&quot;: &quot;12345678&quot;} yield scrapy.http.FormRequest.from_response(response, formdata=fd, callback=self.parse_login) def parse_login(self, response): # 如果 if 判断成功, 调用基类的 start_request() 方法, 继续爬取 start_urls 中的页面. if &quot;Welcome&quot; in response.text: yield from super().start_requests() # Python 3 语法 2. 识别验证码 OCR 识别: tesseract-ocr pytesseract 可以识别的验证码比较简单, 对于某些复杂的验证码, pytesseract 识别率很低, 或无法识别. 基本安装与使用 # 安装 $ yum install tesseract -y $ pip install pillow $ pip install pytesseract # 使用 &gt;&gt;&gt; from PIL import Image &gt;&gt;&gt; import pytesseract &gt;&gt;&gt; img = Image.open(&quot;code.png&quot;) &gt;&gt;&gt; img = img.convert(&quot;L&quot;) # 为提高图像识别率, 把图片转换成黑白图. &gt;&gt;&gt; pytesseract.image_to_string(img) 代码示例: import json from PIL import Image from io import BytesIO import pytesseract class CaptchaLoginSpider(scrapy.Spider): name = &quot;login_captcha&quot; start_urls = [&quot;http://xxx.com&quot;] login_url = &quot;http://xxx.com/login&quot; user = &quot;tom@example.com&quot; password = &quot;123456&quot; def parse(self, response): pass def start_requests(self): yield scrapy.Request(self.login_url, callback=self.login, dont_filter=True) def login(self, response): &quot;&quot;&quot; 该方法即是登录页面的解析方法, 又是下载验证码图片的响应处理函数. :param response: :return: &quot;&quot;&quot; # 如果 response.meta[&quot;login_response&quot;] 存在, 当前 response 为验证码图片的响应, # 否则, 当前 response 为登录页面的响应. login_response = response.meta.get(&quot;login_response&quot;) if not login_response: # 此时 response 为 登录页面的响应, 从中提取验证码图片的 url, 下载验证码图片 captchaUrl = response.css(&quot;label.field.prepend-icon img::attr(src)&quot;).extract_first() captchaUrl = response.urljoin(captchaUrl) yield scrapy.Request(captchaUrl, callback=self.login, meta={&quot;login_response&quot;: response}, dont_filter=True) else: # 此时, response 为验证码图片的响应, response.body 为图片二进制数据, # login_response 为登录页面的响应, 用其构造表单请求并发送. formdata = { &quot;email&quot;: self.user, &quot;password&quot;: self.password, &quot;code&quot;: self.get_captcha_by_ocr(response.body) } yield scrapy.http.FormRequest.from_response(login_response, callback=self.parse_login, formdata=formdata, dont_click=True) def parse_login(self, response): info = json.loads(response.text) if info[&quot;error&quot;] == &quot;0&quot;: scrapy.log.logger.info(&quot;登录成功!&quot;) return super().start_requests() scrapy.log.logger.info(&quot;登录失败!&quot;) return self.start_requests() def get_captha_by_ocr(self, data): img = Image.open(BytesIO(data)) img = img.convert(&quot;L&quot;) captcha = pytesseract.image_to_string(img) img.close() return captcha 网络平台识别 阿里云市场提供很多验证码识别平台, 他们提供了 HTTP 服务接口, 用户通过 HTTP 请求将验证码图片发送给平台, 平台识别后将结果通过 HTTP 响应返回. 人工识别 在 Scrapy 下载完验证码图片后, 调用 Image.show 方法将图片显示出来, 然后调用 Python 内置的 Input 函数, 等待用户肉眼识别后输入识别结果. def get_captha_by_user(self, data): img = Image.open(BytesIO(data)) img.show() captha = input(&quot;请输入验证码: &quot;) img.close() return captha 3. Cookie 登录 &amp;&amp; CookiesMiddleware在使用浏览器登录网站后, 包含用户身份信息的 Cookie 会被浏览器保存到本地, 如果 Scrapy 爬虫能直接使用浏览器的 Cookie 发送 HTTP 请求, 就可以绕过提交表单登录的步骤. 3.1 browsercookie第三方 Python 库 browsercookie 便可以获取 Chrome 和 Firefox 浏览器中的 Cookie. $ pip install browsercookie &gt;&gt;&gt; import browsercookie &gt;&gt;&gt; chrome_cookiejar = browsercookie.chrome() &gt;&gt;&gt; firefox_cookiejar = browsercookie.firefox() &gt;&gt;&gt; type(chrome_cookiejar) http.cookiejar.CookieJar &gt;&gt;&gt; for cookie in chrome_cookiejar: # 对 http.cookiejar.CookieJar 对象进行迭代, 可以访问其中的每个 Cookie 对象. print cookie 3.2 CookiesMiddlewareimport six import logging from collections import defaultdict from scrapy.exceptions import NotConfigured from scrapy.http import Response from scrapy.http.cookies import CookieJar from scrapy.utils.python import to_native_str logger = logging.getLogger(__name__) class CookieMiddleware(object): &quot;&quot;&quot; This middleware enables working with sites that need cookies. &quot;&quot;&quot; def __init__(self, debug=False): &quot;&quot;&quot; jars 中的每一项值, 都是一个 scrapy.http.cookies.CookisJar 对象, CookieMiddleware 可以让 Scrapy 爬虫同时使用多个不同的 CookieJar, 即多个不同的账号. Request(url, meta={&quot;cookiejar&quot;: &quot;account_1&quot;}} :param debug: &quot;&quot;&quot; self.jars = defaultdict(CookieJar) self.debug = debug @classmethod def from_crawler(cls, crawler): &quot;&quot;&quot; 从配置文件读取 COOKIES_ENABLED, 决定是否启用该中间件. :param crawler: :return: &quot;&quot;&quot; if not crawler.settings.getbool(&quot;COOKIES_ENABLED&quot;): raise NotConfigured return cls(crawler.settings.getbool(&quot;COOKIES_DEBUG&quot;)) def process_request(self, request, spider): &quot;&quot;&quot; 处理每一个待发送到额 Request 对象, 尝试从 request.meta[&quot;cookiejar&quot;] 获取用户指定使用的 Cookiejar, 如果用户未指定, 就是用默认的 CookieJar(self.jars[None]). 调用 self._get_request_cookies 方法获取发送请求 request 应携带的 Cookie 信息, 填写到 HTTP 请求头. :param request: :param spider: :return: &quot;&quot;&quot; if request.meta.get(&quot;dont_merge_cookies&quot;, False): request cookiejarkey = request.meta.get(&quot;cookiejar&quot;) jar = self.jar[cookiejarkey] cookies = self._get_request_cookies(jar, request) for cookie in cookies: jar.set_cookie_if_ok(cookie, request) # set Cookie header request.headers.pop(&quot;Cookie&quot;, None) jar.add_cookie_header(request) self._debug_cookie(request, spider) def process_response(self, request, response, spider): &quot;&quot;&quot; 处理每一个 response 对象, 依然通过 request.meta[&quot;cookiejar&quot;] 获取用户指定使用的 cookiejar, 调用 extract_cookies 方法将 HTTP 响应头部中的 Cookie 信息保存到 CookieJar 对象中. :param request: :param response: :param spider: :return: &quot;&quot;&quot; if request.meta.get(&quot;dont_merge_cookies&quot;, False): return response # extract cookies from Set-Cookie and drop invalid/expired cookies. cookiejarkey = request.meta.get(&quot;cookiejar&quot;) jar = self.jars[cookiejarkey] jar.extract_cookies(response, request) self._debug_set_cookie(response, request) return response def _debug_cookie(self, request, spider): if self.debug: cl = [to_native_str(c, errors=&quot;replace&quot;) for c in request.headers.getlist(&quot;Cookie&quot;)] if cl: cookies = &quot;\n&quot;.join(&quot;Cookie: {}\n&quot;.format(c) for c in cl) msg = &quot;Sending cookies to:{}\n&quot;.format(request, cookies) logger.debug(msg, extra={&quot;spider&quot;: spider}) def _debug_set_cookie(self, response, spider): if self.debug: cl = [to_native_str(c, errors=&quot;replace&quot;) for c in response.headers.getlist(&quot;Set-Cookie&quot;)] if cl: cookies = &quot;\n&quot;.join(&quot;Set-Cookie: {}\n&quot;.format(c) for c in cl) msg = &quot;Received cookies from:{}\n&quot;.format(response, cookies) logger.debug(msg, extra={&quot;spider&quot;: spider}) def _format_cookie(self, cookie): # build cookie string cookie_str = &quot;%s=%s&quot; % (cookie[&quot;name&quot;], cookie[&quot;value&quot;]) if cookie.get(&quot;path&quot;, None): cookie_str += &quot;;Path=%s&quot; % cookie[&quot;path&quot;] if cookie.get(&quot;domain&quot;, None): cookie_str += &quot;;Domain=%s&quot; % cookie[&quot;domain&quot;] return cookie_str def _get_request_cookies(self, jar, request): if isinstance(request.cookies, dict): cookie_list = [{&quot;name&quot;: k, &quot;value&quot;: v} for k, v in six.iteritems(request.cookies)] else: cookie_list = request.cookies cookies = [self._format_cookie(x) for x in cookie_list] headers = {&quot;Set-Cookie&quot;: cookies} response = Response(request.url, headers=headers) return jar.make_cookies(response, request) 3.3 实现 BrowserCookieMiddleware利用 browsercookie 对 CookieMiddleware 进行改良. import browsercookie from scrapy.downloadermiddlewares.cookies import CookiesMiddleware class BrowserCookiesMiddleware(CookiesMiddleware): def __init__(self, debug=False): super().__init__(debug) self.load_browser_cookies() def load_browser_cookies(self): # for chrome jar = self.jars[&quot;chrome&quot;] chrome_cookies = browsercookie.chrome() for cookie in chrome_cookies: jar.set_cookie(cookie) # for firefox jar = self.jars[&quot;firefox&quot;] firefox_cookies = browsercookie.firefox() for cookie in firefox_cookies: jar.set_cookie(cookie) 使用示例: # settings.py USER_AGENT = &quot;Mozilla/5.0 (X11; CrOS i686 3912.101.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/27.0.1453.116 Safari/537.36&quot; DOWNLAODER_MIDDLEWARES = { &quot;scrapy.downloademiddleware.cookies.CookiesMiddleware&quot;: None, &quot;browser_cookie.middlewares.BrowserCookiesMiddleware&quot;: 701 } $ scrapy shell &gt;&gt;&gt; from scrapy import Request &gt;&gt;&gt; url = &quot;https://www.zhihu.com/settings/profile&quot; &gt;&gt;&gt; fetch(Request(url, meta={&quot;cookiejar&quot;: &apos;chrome&apos;})) &gt;&gt;&gt; view(response) 十. 爬取动态页面: Splash 渲染引擎Splash 是 Scrapy 官方推荐的 javascript 渲染引擎, 他是用 Webkit 开发的轻量级无界面浏览器, 提供基于 http 接口的 javascript 渲染服务, 支持如下功能: 为用户返回经过渲染的 HTML 页面或页面截图 并发渲染多个页面 关闭图片加载, 加速渲染 在页面中执行用户自定义的 javascript 代码. 指定用户自定义的渲染脚本(lua), 功能类似于 PhantomJS. 1. 安装# 安装 docker $ yum remove docker docker-client docker-client-latest docker-common docker-latest docker-latest-logrotate docker-logrotate docker-selinux docker-engine-selinux docker-engine -y $ yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo $ yum install -y yum-utils device-mapper-persistent-data lvm2 $ yum install docker-ce $ systemctl start docker # 获取镜像 $ docker pull scrapinghub/splash # 运行 splash 服务 $ docker run -p 8051:8051 -p 8050:8050 scrapinghub/splash 2. render.html : 提供 javascript 渲染服务 服务端点 render.html 请求地址 http://localhost:8051/render.html 请求方式 GET/POST 返回类型 html 参数列表: 参数 是否必选 类型 描述 url 必选 string 需要渲染的页面 url timeout 可选 float 渲染页面超时时间 proxy 可选 string 代理服务器地址 wait 可选 float 等待页面渲染的时间 images 可选 integer 是否下载图片, 默认为 1 js_source 可选 string 用自定义的 javascript 代码, 在页面渲染前执行 示例: 使用 request 库调用 render.html 渲染页面. &gt;&gt;&gt; import requests &gt;&gt;&gt; from scrapy.selector import Selector &gt;&gt;&gt; splash_url = &quot;http://localhost:8050/render.html&quot; &gt;&gt;&gt; args = {&quot;url&quot;: &quot;http://quotes.toscrape.com/js&quot;, &quot;timeout&quot;:5,&quot;image&quot;:0} &gt;&gt;&gt; response = requests.get(splash_url, params=args) &gt;&gt;&gt; sel = Selector(response) &gt;&gt;&gt; sel.css(&quot;div.quote span.text::text&quot;).extract() 3. execute : 指定用户自定义的 lua 脚本, 利用该断点可在页面中执行 javascript 代码.在爬去某些页面时, 希望在页面中执行一些用户自定义的 javascript 代码, 例如, 用 javascript 模拟点击页面中的按钮, 或调用页面中的 javascript 函数与服务器交互, 利用 Splash 的 execute 端点可以实现这样的功能. 服务端点 execute 请求地址 http://localhost:8051/execute 请求方式 POST 返回类型 自定义 参数 : 参数 是否必选 类型 描述 lua_source 必选 string 用户自定义的 Lua 脚本 timeout 可选 float 渲染页面超时时间 proxy 可选 string 代理服务器地址 可以将 execute 端点的服务看做一个可用 lua 语言编程的浏览器, 功能类似于 PhantomJS. 使用时需传递一个用户自定义的 Lua 脚本给 Spalsh, 该 Lua 脚本中包含用户想要模拟的浏览器行为. 如 打开某 url 地址的页面 等待页面加载完成 执行 javascript 代码 获取 HTTP 响应头部 获取 Cookie 用户定义的 lua 脚本必须包含一个 main 函数作为程序入口, main 函数被调用时传入一个 splash 对象(lua中的对象), 用户可以调用该对象上的方法操作 Splash. main 函数的返回值可以使 字符串, 也可以是 lua 中的表(类似 python 字典), 表会被编码成 json 串. Splash 对象常用属性和方法 splash.args属性 用户传入参数的表, 通过该属性可以访问用户传入的参数, 如 splash.args.url splash.js_enabled 用于开启/禁止 javascript 渲染, 默认为 True splash.images_enabled 用于开启/禁止 图片加载, 默认为 True splash:go() splash:go(url, baseurl=nil, headers=nil, http_method=&quot;GET&quot;, body=nil, formdata=nil) 类似于在浏览器中打开某 url 地址的页面, 页面所需资源会被加载, 并进行 javascript 渲染, 可以通过参数指定 HTTP 请求头部, 请求方法, 表单数据等. splash:wait() splash:wait(time, cancel_on_redirect=false, cancel_on_error=true) 等待页面渲染, time 参数为等待的秒数. splash:evaljs() splash:evaljs(snippet) 在当前页面下, 执行一段 javascript 代码, 并返回最后一句表达式的值. splash:runjs() splash:runjs(snippet) 在当前页面下, 执行一段 javascript 代码, 与 evaljs 相比, 该函数只执行代码, 不返回值. splash:url() 获取当前页面的 url splash:html() 获取当前页面的 html 文本. splash:get_cookits() 获取全部 cookie 信息. 示例代码: requests 库调用 execute 端点服务 &gt;&gt;&gt; import json &gt;&gt;&gt; lua_script = &quot;&quot;&quot; ...: function main(splash) ...: splash:go(&apos;http://example.com&apos;) ...: splash:wait(0.5) ...: local title = splash:evaljs(&quot;document.title&quot;) ...: return {title=title} ...: end &quot;&quot;&quot; &gt;&gt;&gt; splash_url = &quot;http://localhost:8050/execute&quot; &gt;&gt;&gt; headers = {&quot;content-type&quot;: &quot;application/json&quot;} &gt;&gt;&gt; data = json.dumps({&quot;lua_source&quot;: lua_script}) &gt;&gt;&gt; response = requests.post(splash_url, headers=headers, data=data) &gt;&gt;&gt; response.content &gt;&gt;&gt; &apos;{&quot;title&quot;: &quot;Example Domain&quot;}&apos; &gt;&gt;&gt; response.json() &gt;&gt;&gt; {u&apos;title&apos;: u&apos;Example Domain&apos;} 4. scrapy-splash 安装 $ pip install scrapy-splash 配置 $ cat settings.py # Splash 服务器地址 SPLASH_URL = &quot;http://localhost:8050&quot; # 开启 Splash 的两个下载中间件并调整 HttpCompressionMiddleware 的次序 DOWNLOADER_MIDDLEWARES = { &quot;scrapy_splash.SplashCookiesMiddleware&quot;: 723, &quot;scrapy_splash.SplashMiddleware&quot;: 725, &quot;scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware&quot;: 810, } # 设置去重过滤器 DUPEFILTER_CLASS = &quot;scrapy_splash.SplashAwareDupeFilter&quot; # 用来支持 cache_args (可选) SPIDER_MIDDLEWARES = { &quot;scrapy_splash.SplashDeduplicateArgsMiddleware&quot;: 100, } 使用 Scrapy_splash 调用 Splash 服务非常简单. scrapy_Splash 中定义了一个 SplashRequest 类, 用户只需使用 scrapy_splash.SplashRequest (替代 scrapy.Request) 提交请求即可. SplashRequest 构造器方法参数 url : 待爬去页面的 url headers : 请求 headers, 同 scrapy.Request. cookies : 请求 cookie, 同 scrapy.Request. args : 传递给 Splash 的参数(除 url), 如 wait, timeout, images, js_source 等 cache_args : 如果 args 中的某些参数每次调用都重复传递, 并且数据量巨大, 此时可以把该参数名填入 cache_args 列表中, 让 Splash 服务器缓存该参数. 如 SplashRequest(url, args={&quot;js_source&quot;: js, &quot;wait&quot;: 0.5}, cache_args=[&quot;js_source&quot;]) endpoint : Splash 服务端点, 默认为 render.html, 即 javascript 渲染服务. 该参数可以设置为 render.json, render.har, render.png, render.jpeg, execute 等. 详细参考文档. splash_url : Splash 服务器地址, 默认为 None, 即使用配置文件中的 SPLASH_URL 地址. 5. 代码示例 quote 名人名言爬取 import scrapy from scrapy_splash import SplashRequest class QuotesSpider(scrapy.Spider): name = &apos;quotes&apos; allowed_domains = [&apos;quotes.toscrape.com&apos;] start_urls = [&apos;http://quotes.toscrape.com/js&apos;] splash_base_args = {&quot;images&quot;: 0, &quot;timeout&quot;: 3} def start_requests(self): for url in self.start_urls: yield SplashRequest(url, args=self.splash_base_args) def parse(self, response): for sel in response.css(&quot;div.quote&quot;): quote = sel.css(&quot;span.text::text&quot;).extract_first() author = sel.css(&quot;small.author::text&quot;).extract_first() yield {&quot;quote&quot;: quote, &quot;author&quot;: author} href = response.css(&quot;li.next &gt; a::attr(href)&quot;).extract_first() if href: url = response.urljoin(href) yield SplashRequest(url, args=self.splash_base_args) jd 图书 爬取 import scrapy from scrapy import Request from scrapy_splash import SplashRequest lua_script = &quot;&quot;&quot; function main(splash) splash:go(splash.args.url) splash:wait(2) splash:runjs(&quot;document.getElementsByClassName(&apos;page&apos;)[0].scrollIntoView(true)&quot;) splash:wait(2) return splash:html() end &quot;&quot;&quot; class JdBookSpider(scrapy.Spider): name = &apos;jd_book&apos; allowed_domains = [&apos;search.jd.com&apos;] base_url = &quot;https://search.jd.com/Search?keyword=python&amp;enc=utf-8&amp;book=y&amp;wq=python&quot; def start_requests(self): # 请求第一个页面, 无需渲染 js yield Request(self.base_url, callback=self.parse_url, dont_filter=True) def parse_url(self, response): # 获取商品总数, 计算出总页数. total = int(response.css(&quot;span#J_resCount::text&quot;).re_first(&quot;(\d+)\D?&quot;)) pageNum = total // 60 + (1 if total % 60 else 0) # 构造每一页的 url, 向 Splash 端点发送请求 for i in xrange(pageNum): url = &quot;%s&amp;page=%s&quot; % (self.base_url, 2*i + 1) headers = {&quot;refer&quot;: self.base_url} yield SplashRequest(url, endpoint=&quot;execute&quot;, headers=headers, args={&quot;lua_source&quot;: lua_script}, cache_args=[&quot;lua_source&quot;]) def parse(self, response): # 获取单个页面中每本书的名字和价格 for sel in response.css(&quot;ul.gl-warp.clearfix &gt; li.gl-item&quot;): yield { &quot;name&quot;: sel.css(&quot;div.p-name&quot;).xpath(&quot;string(.//em)&quot;).extract_first(), &quot;price&quot;: sel.css(&quot;div.p-price i::text&quot;).extract_first() } $ vim settings.py USER_AGENT = u&apos;Mozilla/5.0 (Windows NT 6.2; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1667.0 Safari/537.36&apos; 十一. HTTP 代理Scrapy 内部提供了一个下载中间件HttpProxyMiddleware, 专门用于给 Scrapy 设置代理, 他默认是启动的, 他会在系统环境变量中搜索当前系统代理(名称格式为xxx_proxy的环境变量), 作为 Scrapy 爬虫使用的带来. $ export http_proxy=&quot;http://192.168.1.1:8000&quot; $ export https_proxy=&quot;http://192.168.1.1:8001&quot; # 包含用户名和密码 $ export https_proxy=&quot;http://username:password@192.168.1.1:8001&quot; $ curl http(s)://httpbin.org/ip # 返回一个包含请求源 ip 地址信息的额 json 字符串. Scrapy 中为一个请求设置代理的本质就是将代理服务器的 url 填写到 request.meta[&quot;proxy&quot;]. class HttpProxyMiddleware(object): ... def _set_proxy(self, request, scheme): creds, proxy = self.proxies[scheme] request.meta[&quot;proxy&quot;] = proxy if creds: # 如果需要认证, 传递包含用户账号和密码的身份验证信息 request.headers[&quot;Proxy-Authorization&quot;] = b&quot;Basic&quot; + creds # 手动实现 $ scrapy shell &gt;&gt;&gt; from scrapy import Request &gt;&gt;&gt; import base64 &gt;&gt;&gt; req = Request(&quot;http://httpbin.org/ip&quot;, meta={&quot;proxy&quot;: &quot;http://192.168.1.1:8000&quot;}) &gt;&gt;&gt; user = &quot;tom&quot; &gt;&gt;&gt; password = &quot;tom123&quot; &gt;&gt;&gt; user_passwd = (&quot;%s:%s&quot; % (user, password)).encode(&quot;utf8&quot;) &gt;&gt;&gt; req.headers[&quot;Proxy-Authorization&quot;] = b&quot;Basic&quot; + base64.b64encode(user_passwd) &gt;&gt;&gt; fetch(req) 1. 抓取免费代理:代理网站: http://proxy-list.org https://free-proxy-list.net http://www.xicidaili.com http://www.proxy360.cn http://www.kuaidaili.com 获取西祠代理代码 # settings.py USER_AGENT = &quot;Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN) AppleWebKit/523.15 (KHTML, like Gecko, Safari/419.3) Arora/0.3 (Change: 287 c9dfb30)&quot; # spider.py import json import scrapy from scrapy import Request class XiciSpider(scrapy.Spider): name = &apos;xici&apos; allowed_domains = [&apos;www.xicidaili.com&apos;] # start_urls = [&apos;http://www.xicidaili.com/nn/&apos;] base_url = &quot;http://www.xicidaili.com/nn/%s&quot; check_url = &quot;%s://httpbin.org/ip&quot; def start_requests(self): for i in xrange(1, 5): yield Request(self.base_url % i) def parse(self, response): for sel in response.xpath(&quot;//table[@id=&apos;ip_list&apos;]/tr[position()&gt;1]&quot;): ip = sel.css(&apos;td:nth-child(2)::text&apos;).extract_first() port = sel.css(&apos;td:nth-child(3)::text&apos;).extract_first() scheme = sel.css(&apos;td:nth-child(6)::text&apos;).extract_first().lower() url = self.check_url % scheme proxy = &quot;%s://%s:%s&quot; % (scheme, ip, port) meta = { &quot;proxy&quot;: proxy, &quot;dont_retry&quot;: True, &quot;download_timeout&quot;: 10, &quot;_proxy_scheme&quot;: scheme, &quot;_proxy_ip&quot;: ip } yield Request(url, callback=self.check_available, meta=meta, dont_filter=True) def check_available(self, response): proxy_ip = response.meta[&quot;_proxy_ip&quot;] if proxy_ip == json.loads(response.text)[&quot;origin&quot;]: yield { &quot;proxy_scheme&quot;: response.meta[&quot;_proxy_scheme&quot;], &quot;proxy&quot;: response.meta[&quot;proxy&quot;] } 2. 基于 HttpProxyMiddleware 实现随机代理# middlewares.py class RandomHttpProxyMiddleware(HttpProxyMiddleware): def __init__(self, auth_encoding=&quot;latin-1&quot;, proxy_list_file=None): if not proxy_list_file: raise NotConfigured self.auth_encoding = auth_encoding # 用两个列表维护 HTTP 和 HTTPS 代理, {&quot;http&quot;: [...], &quot;https&quot;: [...]} self.proxies = defaultdict(list) with open(proxy_list_file) as f: proxy_list = json.load(f) for proxy in proxy_list: scheme = proxy[&quot;proxy_scheme&quot;] url = proxy[&quot;proxy&quot;] self.proxies[scheme].append(self._get_proxy(url, scheme)) @classmethod def from_crawler(cls, crawler): auth_encoding = crawler.settings.get(&quot;HTTPPROXY_AUTH_ENCODING&quot;, &quot;latin-1&quot;) proxy_list_file = crawler.settings.get(&quot;HTTPPROXY_PROXY_LIST_FILE&quot;) return cls(auth_encoding, proxy_list_file) def _set_proxy(self, request, scheme): creds, proxy = random.choice(self.proxies[scheme]) request.meta[&quot;proxy&quot;] = proxy if creds: request.headers[&quot;Proxy-Authorization&quot;] = b&quot;Basic&quot; + creds # spider.py : 测试随机 proxy 是否 work import json import scrapy from scrapy import Request class TestRandomProxySpider(scrapy.Spider): name = &quot;random_proxy&quot; def start_requests(self): for _ in range(100): yield Request(&quot;http://httpbin.org/ip&quot;, dont_filter=True) yield Request(&quot;https://httpbin.org/ip&quot;, dont_filter=True) def parse(self, response): print json.loads(response.text) # settings.py DOWNLOADER_MIDDLEWARES = { &apos;proxy_example.middlewares.RandomHttpProxyMiddleware&apos;: 543, } HTTPPROXY_PROXY_LIST_FILE = &quot;proxy.json&quot; 3. 实战: 豆瓣电影# Spider.py import json import re import scrapy from scrapy import Request class DmovieSpider(scrapy.Spider): BASE_URL = &quot;https://movie.douban.com/j/search_subjects?type=movie&amp;tag=%s&amp;sort=recommend&amp;page_limit=%s&amp;page_start=%s&quot; MOVIE_TAG = &quot;豆瓣高分&quot; PAGE_LIMIT = 20 page_start = 0 name = &apos;dmovie&apos; allowed_domains = [&apos;movie.douban.com&apos;] start_urls = [BASE_URL % (MOVIE_TAG, PAGE_LIMIT, page_start)] def parse(self, response): infos = json.loads(response.body.decode(&quot;utf-8&quot;)) for movie_info in infos[&quot;subjects&quot;]: movie_item = {} movie_item[&quot;片名&quot;] = movie_info[&quot;title&quot;] movie_item[&quot;评分&quot;] = movie_info[&quot;rate&quot;] yield Request(movie_info[&quot;url&quot;], callback=self.parse_movie, meta={&quot;_movie_item&quot;: movie_item}) if len(infos[&quot;subjects&quot;]) == self.PAGE_LIMIT: self.page_start += self.PAGE_LIMIT url = self.BASE_URL % (self.MOVIE_TAG, self.PAGE_LIMIT, self.page_start) yield Request(url) def parse_movie(self, response): movie_item = response.meta[&quot;_movie_item&quot;] info = response.css(&quot;div.subject div#info&quot;).xpath(&quot;string(.)&quot;).extract_first() fields = [s.strip().replace(&quot;:&quot;, &quot;&quot;) for s in response.css(&quot;div#info span.pl::text&quot;).extract()] values = [re.sub(&quot;\s+&quot;, &quot;&quot;, s.strip()) for s in re.split(&apos;\s*(?:%s):\s*&apos; % &quot;|&quot;.join(fields), info)][1:] movie_item.update(dict(zip(fields, values))) yield movie_item # settings.py DOWNLOADER_MIDDLEWARES = { &apos;douban_movie.middlewares.RandomHttpProxyMiddleware&apos;: 543, } HTTPPROXY_PROXY_LIST_FILE = &quot;proxy.json&quot; USER_AGENT = &quot;Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN) AppleWebKit/523.15 (KHTML, like Gecko, Safari/419.3) Arora/0.3 (Change: 287 c9dfb30)&quot; DOWNLOAD_DELAY = 2 ROBOTSTXT_OBEY = False # middleware.py class RandomHttpProxyMiddleware(HttpProxyMiddleware): def __init__(self, auth_encoding=&quot;latin-1&quot;, proxy_list_file=None): if not proxy_list_file: raise NotConfigured self.auth_encoding = auth_encoding # 用两个列表维护 HTTP 和 HTTPS 代理, {&quot;http&quot;: [...], &quot;https&quot;: [...]} self.proxies = defaultdict(list) with open(proxy_list_file) as f: proxy_list = json.load(f) for proxy in proxy_list: scheme = proxy[&quot;proxy_scheme&quot;] url = proxy[&quot;proxy&quot;] self.proxies[scheme].append(self._get_proxy(url, scheme)) @classmethod def from_crawler(cls, crawler): auth_encoding = crawler.settings.get(&quot;HTTPPROXY_AUTH_ENCODING&quot;, &quot;latin-1&quot;) proxy_list_file = crawler.settings.get(&quot;HTTPPROXY_PROXY_LIST_FILE&quot;) return cls(auth_encoding, proxy_list_file) def _set_proxy(self, request, scheme): creds, proxy = random.choice(self.proxies[scheme]) request.meta[&quot;proxy&quot;] = proxy if creds: request.headers[&quot;Proxy-Authorization&quot;] = b&quot;Basic&quot; + creds # proxy.json [ {&quot;proxy_scheme&quot;: &quot;http&quot;, &quot;proxy&quot;: &quot;http://111.155.116.237:8123&quot;}, {&quot;proxy_scheme&quot;: &quot;https&quot;, &quot;proxy&quot;: &quot;https://222.188.190.99:6666&quot;}, {&quot;proxy_scheme&quot;: &quot;https&quot;, &quot;proxy&quot;: &quot;https://60.23.36.250:80&quot;}, {&quot;proxy_scheme&quot;: &quot;https&quot;, &quot;proxy&quot;: &quot;https://120.79.216.57:6666&quot;}, {&quot;proxy_scheme&quot;: &quot;https&quot;, &quot;proxy&quot;: &quot;https://120.92.88.202:10000&quot;}, {&quot;proxy_scheme&quot;: &quot;https&quot;, &quot;proxy&quot;: &quot;https://120.79.151.197:6666&quot;}, {&quot;proxy_scheme&quot;: &quot;http&quot;, &quot;proxy&quot;: &quot;http://118.114.77.47:8080&quot;}, {&quot;proxy_scheme&quot;: &quot;http&quot;, &quot;proxy&quot;: &quot;http://112.74.62.69:8081&quot;}, {&quot;proxy_scheme&quot;: &quot;https&quot;, &quot;proxy&quot;: &quot;https://218.93.166.4:6666&quot;}, {&quot;proxy_scheme&quot;: &quot;http&quot;, &quot;proxy&quot;: &quot;http://58.216.202.149:8118&quot;}, {&quot;proxy_scheme&quot;: &quot;http&quot;, &quot;proxy&quot;: &quot;http://14.118.253.233:6666&quot;} ] 十二. scrapy-redis 分布式爬虫Scrapy-redis 利用 Redis 数据库重新实现了 Scrapy 中的某些组件. 基于 Redis 的请求队列(优先队列, FIFO, LIFO) 基于 Redis 的请求去重过滤器(过滤掉重复的请求) 基于以上两个组件的调度器 Scrapy-redis 为多个爬虫分配爬取任务的方式是: 让所有爬虫共享一个存在于 Redis 数据库中的请求队列(替代各爬虫独立的请求队列), 每个爬虫从请求队列中获取请求, 下载并解析出新请求再添加到 请求队列中, 因此, 每个爬虫即是下载任务的生产者, 又是消费者. 搭建分布式环境 # 在所有机器上安装包 $ pip install scrapy $ pip install scrapy-redis # 启动redis server, 确保分布式环境中每台机器均可访问 redis-server $ redis-cli -h REDIS_SERVER ping 配置项目 # settings.py ## 指定爬虫使用的 redis 数据库 REDIS_URL = &quot;redis://192.168.1.10:6379&quot; ## 使用 scrapy-redis 的调度器替代 scrapy 原版调度器 SCHEDULER = &quot;scrapy_redis.scheduler.Scheduler&quot; ## 使用 scrapy-redis 的 RFPDupeFilter 作为去重过滤器 DUPEFILTER_CLASS = &quot;scrapy_redis.dupefilter.RFPDupeFilter&quot; ## 启动 scrapy_redis 的 RedisPipeline 将爬取到的数据汇总到 数据库. ITEM_PIPELINES = { &quot;scrapy_redis.pipelines.RedisPipeline&quot;: 300, } ## 爬虫停止后, 保留/清理 redis 中的请求队列及去重即可. True: 保留, False: 清理(默认). SCHEDULER_PERSIST = True Scrapy-redis 提供了一个新的 Spider 基类 RedisSpider, RedisSpider 重写了 start_requests 方法, 他重试从 redis 数据库的某个特定列表中获取起始爬取点, 并构造 Request 对象(dont_filter=False), 该列表的键可通过配置文件设置(REDIS_START_URLS_KEY), 默认为 &lt;spider_name&gt;:start_urls. 在分布式爬取时, 用户运行所有爬虫后, 需要手动使用 Redis 命令向该列表添加起始爬取点, 从而避免重复. # spider.py from scrapy_redis.spiders import RedisSpider class BooksSpider(RedisSpider): # 爬虫 继承 RedisSpider 类 pass # 注释 start_urls # start_urls = [&quot;http://book.toscrape.com&quot;] # 命令行 写入队列开始值. $ redis-cli -h 192.168.1.10 &gt; lpush books:start_urls &quot;http://books.toscrape.com/&quot; 十三. 奇技淫巧1. scrapy 项目的一般步骤 创建 项目 $ scrapy startproject PROJECT_NAME 创建 spider $ cd PROJECT_NAME $ scrapy genspider SPIDER_NAME DOMAIN 封装 Item 类 完成 Spider 类 配置 settings.py ## 指定输出序列 FEED_EXPORT_FIELDS = [] ## 绕过 roobot.txt ## USER_AGENT 配置 编写 Pipeline, 实现 item 字段转换 : settings.py ITEM_PIPELINES = { PIPELINE_NAME: rate, } 运行 crawl $ scrapy list $ scrapy crawl MySpider 2. User-Agent 使用 fake-useragent GitHub - hellysmile/fake-useragent: up to date simple useragent faker with real world database $ pip install fake-useragent 各大搜索引擎的 UA 可以伪装成各大搜索引擎网站的UA， 比如 Google UA 添加referfer字段为 搜索引擎网站 也是有用的，因为网站是希望被索引的，所以会放宽搜索引擎的爬取策略。 useragentstring.com 3. 代理网上的开源代理: https://github.com/xiaosimao/IP_POOL 代理网站: http://www.kuaidaili.com/free/ http://www.66ip.cn/ http://www.goubanjia.com/free/gngn/index.shtml http://www.xicidaili.com/ data5u proxydb 测试网站: 百度 https://httpbin.org/get 十四. 参考链接 精通 Scrapy 网络爬虫 Scrapy 文档 1. First Step2. 基本概念2.1 命令行工具2.2 spiders 爬虫2.3 Selectors 选择器2.4 Items2.5 Item Loaders2.6 Item Pipeline2.7 Feed exports : 输出和存储数据2.8 请求和响应2.9 连接提取2.10 设置, 配置2.11 Exceptions3. 内置服务3.1 Logging3.2 Stats Collection3.3 Sending e-mail3.4 Telnet Console3.5 Web Service4. Q&amp;A5.扩展 scrapy]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>Scrapy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ansible-基本原理与安装配置]]></title>
    <url>%2F2018%2F03%2F15%2FAnsible-%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[Ansible 基本原理, 安装配置与命令行工具的使用. Ansible 学习总结 1. Ansible 简介Ansible 是使用 Python 开发的, 基于 SSH 协议的, Agentless 的配置管理工具. 其源代码存放于 githb 上, 分隔成 三部分 分别存放在不同的代码仓库上. 主仓库 : https://github.com/ansible/ansible 核心模块 : https://github.com/ansible/ansible-modules-core 其他模块 : https://github.com/ansible/ansible-modules-extras 2. Ansible 任务的执行细节原理2.1 角色与依赖: 被管理主机 : 需要 ssh 和 python2.5 或者更高版本. 管理主机 : 需要 Python2.6 或者更高的版本. 有些模块需要额外的依赖, 如 ec2模块 依赖 boto模块, docker 模块依赖 docker-py 等. 2.2 工作机制Ansible 默认采用推送模式, 但是也支持 拉取模式, 使用 ansible-pull 命令. 2.3 工作原理示例代码: - name: install nginx apt: name=nginx ansible 操作如下: 在管理主机生成安装 nginx 软件包的 python 程序 将该程序复制到 目标服务器. 在目标服务器上完成操作, 执行 程序 等待改程序在所有主机上完成. ansible 执行模块指令的注意事项: 对于每一个任务, ansible 都是并行执行的. 在开始下一个任务之前, ansible 会等待所有主机都完成上一个任务. ansible 任务的执行顺序, 为管理员定义的执行顺序. 幂等性 3. 安装配置3.1 安装12$ yum install python-pip$ pip install ansible 3.2 配置文件 配置段 ansible.cfg 有 defaults, ssh_connection, paramiko, accelerate 四个配置段, 其具体配置项见ansible 番外篇之 ansible.cfg 配置参数 配置文件及优先级 ansible 配置文件 : ansible.cfg, ansible 使用如下位置和顺序来查找 ansible.cfg 文件 ANSIBLE_CONFIG 环境变量指向的文件 ./ansible.cfg ~/.ansible.cfg /etc/ansible/ansible.cfg 基础示例: [defaults] hostfile = hosts remote_user = ec2-user private_key_file = /path/to/my_private_key host_key_checking = False # 关闭 host key 检查. 4. Ansible 抽象实体4.1 inventoryAnsible 执行的目标主机的配置文件. Ansible 支持静态 Inventory 文件 和 动态 Inventory , 默认的 静态 Inventory 文件为 /etc/ansible/hosts, 同时支持 Cobbler Inventory , AWS ec2.py 等动态 Inventory. 4.2 变量与factAnsible 支持如下变量类型及定义, 可以提高 task 的可复用性和适用性: 自定义变量 注册变量 内置变量 fact 变量 4.3 模块模块定义 Ansible 可以在目标主机执行的具体操作, 是由 Ansible 包装好后在这几上执行一系列操作的脚本, 是 Ansible 执行操作的最小粒度.Ansible 支持如下模块类型: 内置模块 自定义模块: 支持多种编程语言, 如 shell, python, ruby 等. 4.4 task/play/role/playbook task : 由模块定义的具体操作. play : 多个包含 host, task 等多个字段部分的任务定义集合. 是一个 YAML 字典结构. playbook : 多个 play 组成的列表 role : 是将 playbook 分割为多个文件的主要机制, 用于简化 playbook 的编写, 并提高 playbook 的复用性. 5. Ansible 命令5.1 ansible123456789101112131415161718192021$ ansible -i INVENTORY HOST_GROUP [ -s ] -m MODEL -a ARGS [-vvvv] -i INVENTORY : 指定 INVENTORY -s : sudo 为 root 执行 -m MODEL : 模块 -a ARGS : 模块参数 -vvvv : 输出详细信息.# 检测是否可以连接到服务器.$ ansible testserver -i hosts -m ping [-vvvv]# 查看服务器运行时间$ ansible testserver -i hosts -m command -a uptime# 参数中包含空格, 应该使用 引号 引起来.$ ansible testserver -i hosts -m command -a "tail /var/log/messages"# 安装 nginx 包$ ansible testserver -s -m apt -a name=nginx# 重启 nginx 服务$ ansible testserver -s -m service -a name=nginx state=restarted 5.2 ansible-docAnsible 模块的帮助文档. 12345678# 列出所有可用模块$ ansible-doc --list # 查看指定 模块的帮助$ ansible-doc MOD_NAME# 查看模块的示例$ ansible-doc MOD_NAME -s 5.3 ansible-galaxyansible-galaxy : 创建 role 初始文件和目录 5.3.1 创建初始 role 文件和目录$ ansible-galaxy init -p playbook/roles web -p /path/to/roles : 指定 roles 的目录, 未指定则为当前目录. 5.3.2 从 role 仓库中检索, 安装,删除 role.ansible-galaxy [delete|import|info|init|install|list|login|remove|search|setup] [--help] [options] # 检索 $ ansible-galaxy search ntp # 安装 $ ansible-galaxy install -p ./roles bennojoy.ntp # 列出 $ ansible-galaxy list # 删除 $ ansible-galaxy remove bennojoy.ntp 5.4 ansible-vaultansible-vault 用于创建和编辑加密文件, ansible-playbook 可以自动识别并使用密码解密这些文件. 5.4.1 ansible-vault 命令$ ansible-vault [create|decrypt|edit|encrypt|encrypt_string|rekey|view] [--help] [options] vaultfile.yml SubCmd: - encrypt : 加密 - decrypt : 解密 - create : 创建 - edit : 编辑 - view : 查看 - rekey : 修改密码 Options: --ask-vault-pass : ask for vault password --new-vault-password-file=NEW_VAULT_PASSWORD_FILE : new vault password file for rekey --output=OUTPUT_FILE : output file name for encrypt or decrypt; use - for stdout --vault-password-file=VAULT_PASSWORD_FILE : vault password file -v, --verbose : verbose mode (-vvv for more, -vvvv to enable connection debugging) --version : show program&apos;s version number and exit 5.4.2 与playbook 结合的使用 在 playbook 中引用 vault 文件: 可以在 vars_file 区段像一般文件一样易用 vault 加密的文件. 即, 如果加密了一个 file 文件, 在 playbook 中也无需修改. ansible-playbook 使用用 --ask-value-pass 或 --vault-password-file 参数 $ ansible-playbook myplay.yml --ask-value-pass # password.file 可以为文本文件, 如果该为文件可执行脚本, 则 ansible 使用它的标准输出内容作为密码 $ ansible-playbook myplay.yml --vault-password-file /path/to/password.file 5.5 ansible-playbook5.5.1 命令行参数Usage: ansible-playbook playbook.yml Options: --ask-vault-pass ask for vault password -C, --check don&apos;t make any changes; instead, try to predict some of the changes that may occur -D, --diff when changing (small) files and templates, show the differences in those files; works great with --check -e EXTRA_VARS, --extra-vars=EXTRA_VARS set additional variables as key=value or YAML/JSON --flush-cache clear the fact cache --force-handlers run handlers even if a task fails -f FORKS, --forks=FORKS specify number of parallel processes to use (default=5) -i INVENTORY, --inventory-file=INVENTORY specify inventory host path (default=./hosts) or comma separated host list. -l SUBSET, --limit=SUBSET further limit selected hosts to an additional pattern --list-hosts outputs a list of matching hosts; does not execute anything else --list-tags list all available tags --list-tasks list all tasks that would be executed -M MODULE_PATH, --module-path=MODULE_PATH specify path(s) to module library (default=None) --new-vault-password-file=NEW_VAULT_PASSWORD_FILE new vault password file for rekey --output=OUTPUT_FILE output file name for encrypt or decrypt; use - for stdout --skip-tags=SKIP_TAGS only run plays and tasks whose tags do not match these values --start-at-task=START_AT_TASK start the playbook at the task matching this name --step one-step-at-a-time: confirm each task before running --syntax-check perform a syntax check on the playbook, but do not execute it -t TAGS, --tags=TAGS only run plays and tasks tagged with these values --vault-password-file=VAULT_PASSWORD_FILE vault password file -v, --verbose verbose mode (-vvv for more, -vvvv to enable connection debugging) Connection Options: control as whom and how to connect to hosts -k, --ask-pass ask for connection password --private-key=PRIVATE_KEY_FILE, --key-file=PRIVATE_KEY_FILE use this file to authenticate the connection -u REMOTE_USER, --user=REMOTE_USER connect as this user (default=root) -c CONNECTION, --connection=CONNECTION connection type to use (default=smart) -T TIMEOUT, --timeout=TIMEOUT override the connection timeout in seconds (default=10) --ssh-common-args=SSH_COMMON_ARGS specify common arguments to pass to sftp/scp/ssh (e.g. ProxyCommand) --sftp-extra-args=SFTP_EXTRA_ARGS specify extra arguments to pass to sftp only (e.g. -f, -l) --scp-extra-args=SCP_EXTRA_ARGS specify extra arguments to pass to scp only (e.g. -l) --ssh-extra-args=SSH_EXTRA_ARGS specify extra arguments to pass to ssh only (e.g. -R) Privilege Escalation Options: control how and which user you become as on target hosts -s, --sudo run operations with sudo (nopasswd) (deprecated, use become) -U SUDO_USER, --sudo-user=SUDO_USER desired sudo user (default=root) (deprecated, use become) -S, --su run operations with su (deprecated, use become) -R SU_USER, --su-user=SU_USER run operations with su as this user (default=root) (deprecated, use become) -b, --become run operations with become (does not imply password prompting) --become-method=BECOME_METHOD privilege escalation method to use (default=sudo), valid choices: [ sudo | su | pbrun | pfexec | doas | dzdo | ksu | runas ] --become-user=BECOME_USER run operations as this user (default=root) --ask-sudo-pass ask for sudo password (deprecated, use become) --ask-su-pass ask for su password (deprecated, use become) -K, --ask-become-pass ask for privilege escalation password 部分示例 -e var=valur : 传递变量给 playbook # 列出主机, 但不会执行 playbook $ ansible-playbook -i hosts --list-hosts web-tls.yml # 语法检查 $ ansible-playbook --syntax-check web-tls.yml # 列出 task, 但不会执行 playbook $ ansible-playbook -i hosts --list-tasks web-tls.yml # 检查模式, 会检测 playbook 中的每个任务是否会修改主机的状态, 但并不会对主机执行任何实际操作. # 需要注意 playbook 中的 task 中的依赖关系, 可能会报错. $ ansible-playbook [ -C | --check ] web-tls.yml # diff 将会为任何变更远程主机状态的文件输出差异信息. 与 --check 结合尤其好用. $ ansible-playbook [ -D | --diff ] playbook.yml 5.5.2 ansible-playbook 控制 task 的执行 step --step 参数会在执行每个 task 之前都做提示. $ ansible-playbook --step playbook.yml Perform task: install package (y/n/c) : y : 执行 n : 不执行, 跳过 c : 继续执行剩下 playbook , 并不再提示. start-at-task --start-at-task 用于让 ansible 从指定 task 开始运行 playbook, 而不是从头开始. 常用于 playbook 中存在 bug , 修复之后, 从bug处再次重新运行. tags ansible 允许对一个 task 或者 play 添加一个或多个 tags, 如: - hosts: myservers tags: - foo tasks: - name: install packages apt: name={{ item }} with_items: - vim - emacs - nano - name: run arbitrary command command: /opt/myprog tags: - bar - quux -t TAG_NAME 或 --tags TAG_NAME 告诉 ansible 仅允许具有指定 tags 的 task 或 play. --skip-tags TAG_NAME 告诉 ansible 跳过具有指定 tags 的 task 或者 play. # 仅允许指定 tags 的 task/play $ ansible-playbook -t foo,bar playbook.yml $ ansible-playbook --tags=foo,bar playbook.yml # 跳过指定 tags 的 task/play $ ansible-playbook --skip-tags=baz,quux playbook.yml 5.6 ansible-consoleREPL console for executing Ansible tasks $ ansible-console [&lt;host-pattern&gt;] [options] 5.7 ansible-configView, edit and manage ansible configuratin $ ansible-config [view|dump|list] [--help] [options] [ansible.cfg] 5.8 ansible-inventoryused to display or dump the configured inventory as Ansible sees it. $ ansible-inventory [options] [host|group] 5.9 ansible-pullpulls playbooks from a VCS repo and executes them for the local host. $ ansible-pull -U &lt;repository&gt; [options] [&lt;playbook.yml&gt;] 6. ansible 优化加速6.1 SSH Multiplexing (ControlPersist)原理 : 第一次尝试 SSH 连接到远程主机时, OpenSSH 创建一个主链接 OpenSSH 创建一个 UNIX 域套接字(控制套接字), 通过主链接与远程主机相连接 在 ControlPersist 超时时间之内, 再次连接到该远程主机, OpenSSH 将使用控制套接字与远程主机通信, 而不创建新的 TCP 连接, 省去了 TCP 三次握手的时间. Ansible 支持的 SSH Multiplexing 选项列表: 选项 值 说明 ControlMaster auto 开启 ControlPersist ControlPath $HOME/.ansible/cp/ansible-ssh-%h-%p-%r UNIX 套接字文件存放路径, 操作系统对 套接字 的最大长度有限制, 所以太长的套接字, 则 ControlPersist 将不工作, 并且 Ansible 不会报错提醒. ControlPersist 60s SSH 套接字连接空闲时间, 之后关闭 如果启用了 SSH Multiplexing 设置, 并且变更了 SSH 连接的配置, 如修改了 ssh_args 配置项, 那么, 新配置对于之前连接打开的未超时的控制套接字不会生效. 6.2 fact 缓存 关闭 fact 缓存 # ansible.cfg [defaults] gathering=explicit 开启 fact 缓存 请确保, playbook 中没有指定 gather_facts: True 或 gather_facts: False 配置项. # ansible.cfg [defaults] gathering=smart # 缓存过期时间, 单位 秒 fact_cache_timeout=86400 # 缓存实现机制. fact_caching=... JSON ansible 将fact 缓存写入到 JSON 文件中, Ansible 使用文件修改时间来决定fact 缓存是否过期. # ansible.cfg [defaults] gathering=smart # 缓存过期时间, 单位 秒 fact_cache_timeout=86400 fact_caching = jsonfile # 指定 fact 缓存文件保存目录 fact_caching_connection = /tmp/ansible_fact/cache redis 需要安装 redis 包, $ pip install redis; 需要本机提供 redis 服务. # ansible.cfg [defaults] gathering=smart # 缓存过期时间, 单位 秒 fact_cache_timeout=86400 fact_caching = redis memcache 需要安装 python-memcached 包, $ pip install python-memcached; 需要本机提供 memcached 服务. # ansible.cfg [defaults] gathering=smart # 缓存过期时间, 单位 秒 fact_cache_timeout=86400 fact_caching = memcached 希望在 playbook 云子能够之前清除 fact 缓存, 使用 --flush-cache 参数 6.3 pipeline原理: 默认执行 task 步骤: 首先, 基于调用的 module 生成一个 python 脚本; 将 Python 脚本复制到远程主机; 最后, 执行 Python 脚本. 将产生两个 SSH 会话. pipeline 模式 : Ansible 执行 Python 脚本时, 并不复制他, 而是通过管道传递给 SSH 会话, 从而减少了 SSH 会话的数目, 节省时间. 配置: 控制主机开启 pipeline # ansible.cfg [defaults] pipeline=True 远程主机 /etc/sudoers 中的 requiretty 没有启用. Defaults: !requiretty 6.4 并发 设置 ANSIBLE_FORKS 环境变量 修改 ansible.cfg 配置文件, forks = 20.]]></content>
      <categories>
        <category>DevOps</category>
      </categories>
      <tags>
        <tag>ansible</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ansible task/role/playbook]]></title>
    <url>%2F2018%2F03%2F15%2FAnsible-task-role-playbook%2F</url>
    <content type="text"><![CDATA[使用 Ansible task, role, playbook 定义任务, 实现自动化服务器管理. Ansible 学习总结 task name : 可选配置, 用于提示task 的功能. 另, ansible-playbook –start-at-task &lt;task_name&gt; 可以调用 name, 从 task 的中间开始执行. 模块(功能) : 必选配置, 有模块的名称组成的 key 和 模块参数组成的 value. 从 Ansible 前段所使用的 YAML 解析器角度看, 参数将被按照字符串处理, 而不是字典 apt: name=nginx update_cache=yes 复杂参数: ansible 提供一个将模块调用分隔成多行的选择, 可以传递 key 为变量名的字典, 而不是传递字符串参数. 这种方式, 在调用拥有复杂参数的模块时, 十分有用. 如 ec2 模块. - name: install pip pkgs pip: name: &quot;{{ item.name }}&quot; version: &quot;{{ item.version }}&quot; virtualenv: &quot;{{ venv_path }}&quot; with_items: - {name: mazzanine, version: 3.1.10} - {name: gunicorn, version: 19.1.1} environment : 设置环境变量 传入包含变量名与值的字典, 来设置环境变量. - name: set the site id script: scripts/setsite.py environment: PATH: &quot;{{ venv_path }}/bin&quot; PROJECT_DIR: &quot;{{ proj_path }}&quot; ADMIN_PASS: &quot;{{ admin_pass }}&quot; sudo, sudo_user notify 触发 handler 任务. when 当 when 表达式返回 True 时, 执行该 task , 否则不执行. local_action : 运行本地任务 在控制主机本机上(而目标主机)执行命令. 如果目标主机为多台, 那么, local_action 执行的task 将执行多次, 可以指定 run_once , 来限制 local_action 的执行次数. # 调用 wait_for 模块 : 注: inventory_hostname 的值仍然是远程主机, 因为这些变量的范围仍然是远程主机, 即使 task 在本机执行. - name: wait for ssh server to be running local_action: wait_for port=22 host=&quot;{{ inventory_hostname }}&quot; search_regex=OpenSSH # 调用 command 模块 - name: run local cmd hosts: all gather_facts: False tasks: - name: run local shell cmd local_action: command touch /tmp/new.txtxt delegate_to: 在涉及主机之外的主机上运行 task 使用场景: 在报警主机中, 启用基于主机的报警, 如 Nagios 向负载均衡器中, 添加一台主机, 如 HAProxy # 配置 Nagios 示例, inventory_hostname 仍然指 web 主机, 而非 nagios_server.example.com . - name: enable alerts for web servers hosts: web tasks: - name: enable alerts nagios: action=enanle_alerts service=web host={{ inventory_hostname }} delegate_to: nagios_server.example.com --- # This playbook does a rolling update for all webservers serially (one at a time). # Change the value of serial: to adjust the number of server to be updated. # # The three roles that apply to the webserver hosts will be applied: common, # base-apache, and web. So any changes to configuration, package updates, etc, # will be applied as part of the rolling update process. # # gather facts from monitoring nodes for iptables rules - hosts: monitoring tasks: [] - hosts: webservers serial: 1 # These are the tasks to run before applying updates: pre_tasks: - name: disable nagios alerts for this host webserver service nagios: &apos;action=disable_alerts host={{ inventory_hostname }} services=webserver&apos; delegate_to: &quot;{{ item }}&quot; with_items: groups.monitoring - name: disable the server in haproxy haproxy: &apos;state=disabled backend=myapplb host={{ inventory_hostname }} socket=/var/lib/haproxy/stats&apos; delegate_to: &quot;{{ item }}&quot; with_items: groups.lbservers roles: - common - base-apache - web # These tasks run after the roles: post_tasks: - name: wait for webserver to come up wait_for: &apos;host={{ inventory_hostname }} port=80 state=started timeout=80&apos; - name: enable the server in haproxy haproxy: &apos;state=enabled backend=myapplb host={{ inventory_hostname }} socket=/var/lib/haproxy/stats&apos; delegate_to: &quot;{{ item }}&quot; with_items: groups.lbservers - name: re-enable nagios alerts nagios: &apos;action=enable_alerts host={{ inventory_hostname }} services=webserver&apos; delegate_to: &quot;{{ item }}&quot; with_items: groups.monitoring run_once : 值为 True/False 该 task 是否只运行一次, 与 local_action 配合十分好用. changed_when &amp; failed_when 使用 changed_when 和 failed_when 语句改变 Ansible 对 task 是 chenged 状态还是 failed 状态的认定.需要了解命令的输出结果 - name: initialize the database django_manage: command: createdb --noinput --nodata app_path: &quot;{{ proj_path }}&quot; virtualenv: &quot;{{ venv_path }}&quot; register: result changed_when: not result.failed and &quot;Creating tables&quot; in result.out failed_when: result.failed and &quot;Database already created&quot; not in result.msg 循环 playbook 执行后, 跟踪主机状态.playplay 可以想象为连接到主机(host)上执行任务(task)的事务. 选项: host : 必选配置, 需要配置的一组主机 task : 必选配置, 需要在主机上执行的任务 name : 可选配置, 一段注释, 用来描述 play 的功能, ansible 在 play 开始执行的时候, 会把 name 打印出来. sudo : 可选配置, 如果为真, ansible 会在运行每个 task 的时候, 都是用 sudo 命令切换为 (默认) root. vars : 可选配置, 变量与其值组成的列表. 任何合法的 YAML 对象都可以作为变量的值. 变量不仅可以在 tasks 中使用, 还可以在 模板文件 中使用. vars_files : 可选, 把变量放到一个或者多个文件中. gather_facts : 是否收集 fact. handlers : 可选, ansible 提供的 条件机制, 和 task 类似, 但只有在被 task 通知的时候才会运行. 如果 ansible 识别到 task 改变了系统的状态, task 就会触发通知机制. task 将 handler 的名字作为参数传递, 依此来通知 handler. handler 只会在所有任务执行完成之后执行, 而且即使被通知了多次, 也只会执行一次. handler 按照play 中定义的顺序执行, 而不是被通知的顺序. handler 常见的用途就是重启服务和重启服务器. serial, max_fail_percentage 默认情况下, Ansible 会并行的在所有相关联主机上执行每一个 task. 可以使用 serial 限制并行执行 play 的主机数量. 一般来说, 当 task 失败时, Ansible 会停止执行失败的那台主机上的任务, 但是继续对其他主机执行. 在负载均衡场景中, 可能希望 Ansible 在所有主机都发生失败前让整个 play 停止执行, 否则将会导致, 所有主机都从 负载均衡器上移除, 并且全部执行失败, 最终负载均衡器上没有任何主机的局面. 此时, 可以使用 serial 和 max_fail_percentage 语句来指定, 最大失败主机比例达超过 max_fail_percentage 时, 让整个 play 失败. 如果希望 Ansible 在任何主机出现 task 执行失败的时候, 都放弃执行, 则需要设置max_fail_percentage=0. - name: upgrade packages on servers behind load balancer hosts: myhosts serial: 1 max_fail_percentage: 25 tasks: - name: get the ec2 instance id and elastic load balancer id ec2_facts: - name: task the out of the elastic load balancer local_action: ec2_elb args: instance_id: &quot;{{ ansible_ec2_instance_id }}&quot; state: absent - name: upgrade packages apt: update_cache=yes upgrade=yes - name: put the host back in the elastic load balancer local_action: ec2_elb args: instance_id: &quot;{{ ansible_ec2_instance_id }}&quot; state: present ec2_elbs: &quot;{{ item }}&quot; with_items: ec2_elbs roles pre-task post-task rolerole 是将 playbook 分隔为多个文件的主要机制, 他大大简化了复杂 playbook 的编写, 同时使得 role 更加易于复用. role 的基本构成.每个 role 都会用一个名字, 如 ‘database’, 与该 role 相关的文件都放在 roles/database 目录下. 其结构如下: 每个单独文件都是可选的 task: task 定义 roles/database/tasks/main.yml files 需要上传到目标主机的文件: roles/database/files/ templates Jinja2 模板文件 roles/database/templates handlers handler roles/database/handler/main.yml vars 不应被覆盖的变量 roles/database/vars/main.yml defaults 可以被覆盖的默认变量 roles/database/default/main.yml meta role 的依赖信息 roles/database/meta/main.yml default 变量与 vars 变量: default : 希望在 role 中变更变量的值. vars : 不希望变量的值变更. role 中变量命名的一个良好实践: 变量建议以 role 的名称开头, 因为在 Ansible 中不同的 role 之间没有命名空间概念, 这意味着在其他 role 中定义的变量, 或者再 playbook 中其他地方定义的变量, 可以在任何地方被访问到. 如果在两个不同的 role 中使用了同名的变量, 可能导致意外的行为. role 的存放位置 playbook 并列的 roles 目录下; /etc/ansible/roles/ 下 ansible.cfg 中 default 段 roles_path 指向的位置 环境变量 ANSIBLE_ROLES_PATH 指向的位置 在 playbook 中使用 role# mezzaning-single-host.yml - name: deploy mezzanine on vagrant hosts: web vars_file: - secrets.yml roles: - role: database database_name: &quot;{{ mezzanine_proj_name }}&quot; # 定义覆盖变量 database_pass: &quot;{{ mezzanine_proj_name }}&quot; # 定义覆盖变量 - role: mezzanine live_hostname: 192.168.33.10.xip.io domains: - 192.168.33.10.xip.io - www.192.168.33.10.xip.io # mezzaning-across-host.yml - name: deploy postgres vagrant hosts: db vars_files: - secrets.yml roles: - role: database database_name: &quot;{{ mezzanine_proj_name }}&quot; # 定义覆盖变量 database_pass: &quot;{{ mezzanine_proj_name }}&quot; # 定义覆盖变量 - name: deploy mezzanine on vagrant hosts: web vars_files: - secrets.yml roles: - role: mezzanine database_host: &quot;{{ hostvars.db.ansible_eth1.ipv4.address }}&quot; live_hostname: 192.168.33.10.xip.io domains: - 192.168.33.10.xip.io - www.192.168.33.10.xip.io pre-task &amp; post-taskpre-task : 定义在 role 执行之前, 执行的 taskpost-task : 定义在 role 执行之后, 执行的 task. - name: deploy mezzanine on vagrant hosts: web vars_files: - secrets.yml pre_tasks: - name: update the apt cache apt: update_cache=yes roles: - role: mezzanine database_host: &quot;{{ hostvars.db.ansible_eth1.ipv4.address }}&quot; live_hostname: 192.168.33.10.xip.io domains: - 192.168.33.10.xip.io - www.192.168.33.10.xip.io post_tasks: - name: notify Slack that the servers have been updated local_action:&gt; slack domain=acme.slack.com token={{ slack_token }} msg=&quot;web server {{ inventory_hostname }} configured&quot; inclued用于调用位于同一目录下的其他 定义文件, 可用于 Tasks,Playbook, Vars, Handler, Files 等. # task example --- - name: install apt packages apt: pkg={{ item }} update_cache=yes cache_valid_time=3600 sudo: True with_items: - git - libjpeg-dev - libpq-dev - memcached - nginx - include: django.yml - include: nginx.yml # example 2 --- - name: check host environment include: check_environment.yml - name: include OS family/distribution specific variables include_vars: &quot;{{ item }}&quot; with_first_found: - &quot;../defaults/{{ ansible_distribution | lower }}-{{ ansible_distribution_version | lower }}.yml&quot; - &quot;../defaults/{{ ansible_distribution | lower }}.yml&quot; - &quot;../defaults/{{ ansible_os_family | lower }}.yml&quot; - name: debug variables include: debug.yml tags: - debug ansible-galaxy : 创建 role 初始文件和目录 创建初始 role 文件和目录 $ ansible-galaxy init -p playbook/roles web -p /path/to/roles : 指定 roles 的目录, 未指定则为当前目录. 从 role 仓库中检索, 安装,删除 role. ansible-galaxy [delete|import|info|init|install|list|login|remove|search|setup] [--help] [options] 检索 $ ansible-galaxy search ntp 安装 $ ansible-galaxy install -p ./roles bennojoy.ntp 列出 $ ansible-galaxy list 删除 $ ansible-galaxy remove bennojoy.ntp 在线网站 https://galaxy.ansible.com dependent role:dependent role 用于指定 role 依赖的其他一个或多个 role, Ansible 会确保被指定依赖的role 一定会优先被执行. Ansible 允许向 dependent role 传递参数 dependent role 一般在 myrole/meta/main.yml 中指定. # roles/web/meta/main.yml dependencies: - { role: ntp, ntp_server=ntp.ubuntu.com } - { role: common } - { role: memcached } playbook : 用于实现 ansible 配置管理的脚本.playbook 其实就是一个字典组成的列表. 一个 playbook 就是一组 play 组成的列表. 一个 play 由 host 的无序集合与 task 的有序列表组成. 每一个 task 由一个模块构成. ansible 中的 True/False 和 yes/no模块参数(如 update_cache=yes)对于值的处理, 使用字符串传递: 真值 yes,on,1,true 假值 no,off,0,false 其他使用 YAML 解析器来处理: 真值 true,True,TRUE,yes,Yes,YES,on,On,ON,y,Y 假值 false,False,FALSE,no,No,NO,off,Off,OFF,n,N 推荐做法: 模块参数: yes/no 其他地方: True,False playbook 文件的执行方法: 使用 ansible-playbook 命令 $ ansible-playbook myplaybook.yml shebang $ chmod +x myplaybook.yml $ head -n 1 myplaybook.yml #!/usr/bin/env ansible-playbook $ ./myplaybook.yml 当 Ansible 开始运行 playbook 的时候, 他做的第一件事就是从他连接到的服务器上收集各种信息. 这些信息包括操作系统,主机名,网络接口等. 建立 nginx web 服务器$ cat web-notls.yml - name: Configure webserver with nginx and tls hosts: webservers sudo: true vars: key_file: /etc/nginx/ssl/nginx.key cert_file: /etc/nginx/ssl/nginx.crt conf_file: /etc/nginx/sites-available/default server_name: localhost tasks: - name: install nginx apt: name=nginx update_cache=yes cache_valid_time=3600 - name: create directories for ssl certificates file: path=/etc/nginx/ssl state=directory - name: copy TLS key copy: src=files/nginx.key desc={{ key_file }} owner=root mode=06-- notify: restart nginx - name: copy TLS certificate copy: src=files/nginx.crt dest={{ cert_file }} notify: restart nginx - name: copy nginx config file copy: src=files/nginx.conf.j2 dest={{ conf_file }} notify: restart nginx - name: enable configuration file: dest=/etc/nginx/sites-enabled/default src={{ conf_file }} state=link notify: restart nginx - name: copy index.html template: src=templates/index.html.j2 dest=/usr/share/nginx/html/index.html mode=0644 handlers: - name: restart nginx service: name=nginx state=restarted 内部变量 ansible_managed : 和模板文件生成时间相关的信息. inventory 文件使用 .ini 格式, 默认为 hosts 文件. [webservers] testserver ansible_ssh_host=127.0.0.1 ansible_ssh_port=22 YAML 文件格式 文件开始. --- 如果没有---标记, 也不影响 ansible 的运行. 注释: # 字符串 : 即使字符串中有空格, 也无需使用引号. 布尔型 : 有多种, 推荐使用 True/False 列表: 使用-作为分隔符 标准列表 - My Fair Lady - Oklahoma - The Pirates of Penzance 内联式列表 [My Fair Lady, Oklahoma, The Pirates of Penzance] 字典: 标准字典 name: tom age: 12 job: manager 内联式字典 {name: tom, age: 12, job: manager} 折行: 使用大于号(&gt;)表示折行]]></content>
      <categories>
        <category>DevOps</category>
      </categories>
      <tags>
        <tag>ansible</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ansible-配置文件]]></title>
    <url>%2F2018%2F03%2F15%2FAnsible-%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[Ansible 配置文件 ansible.cfg 参数总结.Ansible 学习总结 #ansible.cfg 有 defaults, ssh_connection,paramiko,accelerate四个配置段. 1. defaults 段 配置名称 环境变量 默认值 hostfile ANSIBLE_HOSTS /etc/ansible/hosts library ANSIBLE_LIBRARY (none) roles_path ANSIBLE_ROLES_PATH /etc/ansible/roles remote_tmp ANSIBLE_REMOTE_TEMP $HOME/.ansible/tmp module_name (none) command pattern (none) * forks ANSIBLE_FORKS 5 module_args ANSIBLE_MODULE_ARGS (empty string) module_lang ANSIBLE_MODULE_LANG en_US.UTF-8 timeout ANSIBLE_TIMEOUT 10 poll_interval ANSIBLE_POLL_INTERVAL 15 remote_user ANSIBLE_REMOTE_USER current user ask_pass ANSIBLE_ASK_PASS false private_key_file ANSIBLE_PRIVATE_KEY_FILE (none) sudo_user ANSIBLE_SUDO_USER root ask_sudo_pass ANSIBLE_ASK_SUDO_PASS false remote_port ANSIBLE_REMOTE_PORT (none) ask_vault_pass ANSIBLE_ASK_VAULT_PASS false vault_password_file ANSIBLE_VAULT_PASSWORD_FILE (none) ansible_managed (none) Ansible managed: { file} modi ed on %Y-%m-%d %H:%M:%S by {uid} on {host} syslog_facility ANSIBLE_SYSLOG_FACILITY LOG_USER keep_remote_ les ANSIBLE_KEEP_REMOTE_FILES true sudo ANSIBLE_SUDO false sudo_exe ANSIBLE_SUDO_EXE sudo sudo_flags ANSIBLE_SUDO_FLAGS -H hash_behaviour ANSIBLE_HASH_BEHAVIOUR replace jinja2_extensions ANSIBLE_JINJA2_EXTENSIONS (none) su_exe ANSIBLE_SU_EXE su su ANSIBLE_SU false su_flags ANSIBLE_SU_FLAGS (empty string) su_user ANSIBLE_SU_USER root ask_su_pass ANSIBLE_ASK_SU_PASS false gathering ANSIBLE_GATHERING implicit action_plugins ANSIBLE_ACTION_PLUGINS /usr/share/ansible_plugins/action_plugins cache_plugins ANSIBLE_CACHE_PLUGINS /usr/share/ansible_plugins/cache_plugins callback_plugins ANSIBLE_CALLBACK_PLUGINS /usr/share/ansible_plugins/callback_plugins connection_plugins ANSIBLE_CONNECTION_PLUGINS /usr/share/ansible_plugins/connection_plugins lookup_plugins ANSIBLE_LOOKUP_PLUGINS /usr/share/ansible_plugins/lookup_plugins vars_plugins ANSIBLE_VARS_PLUGINS /usr/share/ansible_plugins/vars_plugins filter_plugins ANSIBLE_FILTER_PLUGINS /usr/share/ansible_plugins/ lter_plugins log_path ANSIBLE_LOG_PATH (empty string) fact_caching ANSIBLE_CACHE_PLUGIN memory fact_caching_connection ANSIBLE_CACHE_PLUGIN_CONNECTION (none) fact_caching_prefix ANSIBLE_CACHE_PLUGIN_PREFIX ansible_facts fact_caching_timeout ANSIBLE_CACHE_PLUGIN_TIMEOUT 86400 (seconds) force_color ANSIBLE_FORCE_COLOR (none) nocolor ANSIBLE_NOCOLOR (none) nocows ANSIBLE_NOCOWS (none) display_skipped_hosts DISPLAY_SKIPPED_HOSTS true error_on_unde ned_vars ANSIBLE_ERROR_ON_UNDEFINED_VARS true host_key_checking ANSIBLE_HOST_KEY_CHECKING true system_warnings ANSIBLE_SYSTEM_WARNINGS true deprecation_warnings ANSIBLE_DEPRECATION_WARNINGS true callable_whitelist ANSIBLE_CALLABLE_WHITELIST (empty list) command_warnings ANSIBLE_COMMAND_WARNINGS false bin_ansible_callbacks ANSIBLE_LOAD_CALLBACK_PLUGINS false 2. ssh_connection 段 配置名称 环境变量 默认值 ssh_args ANSIBLE_SSH_ARGS -o ControlMaster=auto -o ControlPersist=60s -o ControlPath=&quot;$ANSIBLE_SSH_CONTROL_PATH” control_path ANSIBLE_SSH_CONTROL_PATH %(directory)s/ansible-ssh-%%h-%%p-%%r pipelining ANSIBLE_SSH_PIPELINING false scp_if_ssh ANSIBLE_SCP_IF_SSH false 3. paramiko 段 配置名称 环境变量 默认值 record_host_keys ANSIBLE_PARAMIKO_RECORD_HOST_KEYS true pty ANSIBLE_PARAMIKO_PTY true 4. accelerate 段 (不推荐使用)]]></content>
      <categories>
        <category>DevOps</category>
      </categories>
      <tags>
        <tag>ansible</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ansible-API]]></title>
    <url>%2F2018%2F03%2F15%2FAnsible-API%2F</url>
    <content type="text"><![CDATA[ansible api 开发篇Ansible 学习总结 ansible api Callbacks Inventory Playbook Script]]></content>
      <categories>
        <category>DevOps</category>
      </categories>
      <tags>
        <tag>ansible</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ansible-模块]]></title>
    <url>%2F2018%2F03%2F15%2FAnsible-%E6%A8%A1%E5%9D%97%2F</url>
    <content type="text"><![CDATA[Ansible 常用模块使用方法 自定义 Ansible 模块 Ansible 学习总结 1. 内置模块是由 ansible 包装后, 在主机上执行一系列操作的脚本. 1.1 查看模块帮助$ ansible-doc MOD_NAME 1.2 查找第三方模块$ ansible-galaxy search MOD_NAME 1.3 常用模块apt update_cache=yes 在安装软件之前, 首先更新 repo 缓存. cache_valid_time=3600 上次 repo 缓存的有效时间. upgrade=yes pipAnsible 的 pip 模块支持向 virtualenv 中安装软件包, 并且还支持在没有可用的 virtualenv 时, 自动创建一个. - name: install required python packages pip: name={{ item }} virtualenv={{ venv_path }} with_items: - gunicorn - django - django-compressor 支持 requirements 文件 - name: install required python pkg pip: requirements={{ proj_path }}/{{ reqs_file }} virtualenv={{ venv_path }} Options : chdircd into this directory before running the command [Default: None] editablePass the editable flag for versioning URLs. [Default: True] executableThe explicit executable or a pathname to the executable to be used to run pip for a specific version of Python installed in the system. For example `pip-3.3&apos;, if there are both Python 2.7 and 3.3 installations in the system and you want to run pip for the Python 3.3 installation. It cannot be specified together with the &apos;virtualenv&apos; parameter (added in 2.1). By default, it will take the appropriate version for the python interpreter use by ansible, e.g. pip3 on python 3, and pip2 or pip on python 2. [Default: None] extra_argsExtra arguments passed to pip. [Default: None] nameThe name of a Python library to install or the url of the remote package. As of 2.2 you can supply a list of names. [Default: None] requirementsThe path to a pip requirements file, which should be local to the remote system. File can be specified as a relative path if using the chdir option. [Default: None] stateThe state of module The &apos;forcereinstall&apos; option is only available in Ansible 2.1 and above. (Choices: present, absent, latest, forcereinstall)[Default: present] umaskThe system umask to apply before installing the pip package. This is useful, for example, when installing on systems that have a very restrictive umask by default (e.g., 0077) and you want to pip install packages which are to be used by all users. Note that this requires you to specify desired umask mode in octal, with a leading 0 (e.g., 0077). [Default: None] versionThe version number to install of the Python library specified in the `name&apos; parameter [Default: None] virtualenvAn optional path to a `virtualenv&apos; directory to install into. It cannot be specified together with the &apos;executable&apos; parameter (added in 2.1). If the virtualenv does not exist, it will be created before installing packages. The optional virtualenv_site_packages, virtualenv_command, and virtualenv_python options affect the creation of the virtualenv. [Default: None] virtualenv_commandThe command or a pathname to the command to create the virtual environment with. For example `pyvenv&apos;, `virtualenv&apos;, `virtualenv2&apos;, `~/bin/virtualenv&apos;, `/usr/local/bin/virtualenv&apos;. [Default: virtualenv] virtualenv_pythonThe Python executable used for creating the virtual environment. For example `python3.5&apos;, `python2.7&apos;. When not specified, the Python version used to run the ansible module is used. [Default: None] virtualenv_site_packagesWhether the virtual environment will inherit packages from the global site-packages directory. Note that if this setting is changed on an already existing virtual environment it will not have any effect, the environment must be deleted and newly created. (Choices: yes, no)[Default: no] copyfileservicetemplatesetup实现 fact 收集的模块. 一般无需再 playbook 中调用该模块, Ansible 会在采集 fact 时, 自动调用. $ ansible server_name -m setup -a &#39;filter=ansible_eth*&#39; 其返回值为一个字典, 字典的 key 是 ansible_fact, 他的 value 是一个有实际 fact 的名字与值组成的字典. setup 模块支持 filter 参数, 可以实现 shell 通配符的匹配过滤. - name: gather facts setup: set_fact使用 set_fact 模块在 task 中设置 fact(与定义一个新变量是一样的). 可以在 register 关键字后, 立即使用 set_fact , 这样使得变量引用更简单. - name: get snapshot id shell: &gt; aws ec2 describe-snapshot --filters Name=tag:Name, Valuse=my-snapshot | jq --raw-outpuy &quot;.Snapshots[].SnapshtId&quot; register: snap_result - set_fact: snap={{ snap_result.stdout }} - name: delete old snapshot command: aws ec2 delete-snapshot --snapshot-id &quot;{{ snap }}&quot; command在 command 中保持幂等性的方法: 指定 creates 参数. # 当 Vagrantfile 存在, 则表示已经处于正确状态, 而且不需要再次执行命令, 从而实现幂等性. - name: create a vagrantfile command: vagrant init {{ box }} creates=Vagrantfile 官方文档: - creates a filename or (since 2.0) glob pattern, when it already exists, this step will *not* be run. [Default: None] - removes a filename or (since 2.0) glob pattern, when it does not exist, this step will *not* be run. [Default: None] script实现幂等性方法: creates 和 removes 参数. 官方文档: - creates a filename, when it already exists, this step will *not* be run. [Default: None] - removes a filename, when it does not exist, this step will *not* be run. [Default: None] debug&gt; DEBUG (/opt/virtualEnv/ansibleEnv/lib/python2.7/site-packages/ansible/modules/utilities/logic/debug.py) This module prints statements during execution and can be useful for debugging variables or expressions without necessarily halting the playbook. Useful for debugging together with the &apos;when:&apos; directive. * note: This module has a corresponding action plugin. Options (= is mandatory): - msg The customized message that is printed. If omitted, prints a generic message. [Default: Hello world!] - var A variable name to debug. Mutually exclusive with the &apos;msg&apos; option. [Default: (null)] - verbosity A number that controls when the debug is run, if you set to 3 it will only run debug when -vvv or above [Default: 0] EXAMPLES: # Example that prints the loopback address and gateway for each host - debug: msg: &quot;System {{ inventory_hostname }} has uuid {{ ansible_product_uuid }}&quot; - debug: msg: &quot;System {{ inventory_hostname }} has gateway {{ ansible_default_ipv4.gateway }}&quot; when: ansible_default_ipv4.gateway is defined - shell: /usr/bin/uptime register: result - debug: var: result verbosity: 2 - name: Display all variables/facts known for a host debug: var: hostvars[inventory_hostname] verbosity: 4 postgresql_userpostgresql_dbdjango_managecron :# 安装 cron job, 注意 name 参数, 该参数必须要有, 该参数将用于删除计划任务时所使用的名称. - name: install poll twitter cron job cron: name=&quot;Poll twitter&quot; minute=&quot;*/5&quot; user={{ user }} job=&quot;{{ manage }} poll_twitter&quot; # 删除计划任务, 基于 name 参数, 在删除时, 会连带注释一起删掉. - name: remote cron job cron: name=&quot;Poll twitter&quot; state=absent git :- name: check out the repository on the host git: repo={{ repo_url }} dest={{ proj_path }} accept_host_key=yes wait_for:You can wait for a set amount of time `timeout’, this is the default if nothing is specified. Waiting for a port to become available is useful for when services are not immediately available after their init scripts return which is true of certain Java application servers. It is also useful when starting guests with the [virt] module and needing to pause until they are ready. This module can also be used to wait for a regex match a string to be present in a file. In 1.6 and later, this module can also be used to wait for a file to be available or absent on the filesystem. In 1.8 and later, this module can also be used to wait for active connections to be closed before continuing,useful if a node is being rotated out of a load balancer pool. Options: active_connection_statesThe list of tcp connection states which are counted as active connections [Default: [u&apos;ESTABLISHED&apos;, u&apos;SYN_SENT&apos;, u&apos;SYN_RECV&apos;, u&apos;FIN_WAIT1&apos;, u&apos;FIN_WAIT2&apos;, u&apos;TIME_WAIT&apos;]] connect_timeoutmaximum number of seconds to wait for a connection to happen before closing and retrying [Default: 5] delaynumber of seconds to wait before starting to poll [Default: 0] exclude_hostslist of hosts or IPs to ignore when looking for active TCP connections for `drained&apos; state [Default: None] hostA resolvable hostname or IP address to wait for [Default: 127.0.0.1] pathpath to a file on the filesytem that must exist before continuing [Default: None] portport number to poll [Default: None] search_regexCan be used to match a string in either a file or a socket connection. Defaults to a multiline regex. [Default: None] sleepNumber of seconds to sleep between checks, before 2.3 this was hardcoded to 1 second. [Default: 1] stateeither `present&apos;, `started&apos;, or `stopped&apos;, `absent&apos;, or `drained&apos; When checking a port `started&apos; will ensure the port is open, `stopped&apos; will check that it is closed, `drained&apos; will check for active connections When checking for a file or a search string `present&apos; or `started&apos; will ensure that the file or string is present before continuing, `absent&apos; will check that file is absent or removed (Choices: present, started, stopped, absent, drained)[Default: started] timeoutmaximum number of seconds to wait for [Default: 300] wait_for_connection : 默认超时时间 600s Waits until remote system is reachable/usable 等待目标主机可以成为 reachable/usable 状态, 即 ssh 22 端口可以连通. - name: Wait 300 seconds, but only start checking after 60 seconds wait_for_connection: delay: 60 # 等待 60s 之后执行 本task timeout: 300 # 超时时间, 默认300s sleep: 2 # 在检查期间, 每次检查之间的间隔时间, 默认为 1s - name: Wait 600 seconds for target connection to become reachable/usable wait_for_connection: wait_for : Waits for a condition before continuing 等待某个主机或端口可用, 适用范围比 wait_for_connection 更加广泛. 可以在本机或目标主机检查其他或本地主机的端口,. - name: Wait 300 seconds for port 22 to become open wait_for: port: 22 sleep: 3 # A resolvable hostname or IP address to wait for. host: &apos;{{ (ansible_ssh_host|default(ansible_host))|default(inventory_hostname) }}&apos; # Can be used to match a string in either a file or a socket connection. search_regex: OpenSSH timeout: 300 # This overrides the normal error message from a failure to meet the required conditions. msg: Timeout to connect through OpenSSH # Either present, started, or stopped, absent, or drained. # When checking a port started will ensure the port is open, stopped will check that it is closed, drained will check for active connections. # When checking for a file or a search string present or started will ensure that the file or string is present before continuing, absent will check that file is absent or removed. state: drained # List of hosts or IPs to ignore when looking for active TCP connections for drained state. exclude_hosts: 10.2.1.2,10.2.1.3 delegate_to: localhost - name: Wait until the process is finished and pid was destroyed wait_for: # Path to a file on the filesystem that must exist before continuing. path: /proc/3466/status state: absent lineinfilestat收集关于文件路径状态的各种信息, 返回一个字典, 该字典包含一个 stat 字段. 部分字段返回值表: 字段 描述 dev inode 所在设备 ID 编号 gid 路径的所属组 ID 编号 inode inode 号 mode 字符串格式的八进制文件模式,如 1777 atime 路径的最后访问时间, 使用 UNIX 时间戳 ctime 路径的创建时间, 使用 UNIX 时间戳, 文件元数据变更时间 mtime 路径的最后修改时间, 使用 UNIX 时间戳 , 文件内容修改时间. nlink 文件硬链接的数量 pw_name 文件所属者的登录名 size 如果是文件, 返回字节单位的文件大小 uid 路径所属者的 uid isblk 如果路径为指定块设备文件, 返回 true ischr 如果路径为指定字符设备文件,返回 true isdir 如果路径为目录, 返回 true isfifo 如果路径为 FIFO(管道), 返回 true isgid 如果文件设置了 setgid , 返回 true isuid 如果文件设置了 setuid , 返回 true islnk 如果文件时符号链接, 返回 true isreg 如果路径是常规文件, 返回 true issock 如果路径是UNIX 域socket, 返回 true rgrp 如果设置所属组可读权限, 返回 true roth 如果设置其他人可读权限, 返回 true rusr 如果设置了属主可读权限, 返回 true wgrp 如果设置所属组可写权限, 返回 true woth 如果设置所属组可写权限, 返回 true wusr 如果设置所属组可写权限, 返回 true xgrp 如果设置所属组可执行权限, 返回 true xoth 如果设置所属组可执行权限, 返回 true xusr 如果设置所属组可执行权限, 返回 true exists 如果存在, 返回 true md5 文件的 md5 值 checksum 文件的hash 值, 可以设置 sha 算法. assertassert 模块在指定的条件不符合是,返回错误, 并失败退出. 主要用于调试.that : 后跟计算表达式msg : 失败后的提示信息. - name: stat /opt/foo stat: path=/opt/foo register: st - name: assert that /opt/foo is a directory assert: that: st.stat.isdir ------- - assert: that: - &quot;my_param &lt;= 100&quot; - &quot;my_param &gt;= 0&quot; msg: &quot;&apos;my_param&apos; must be between 0 and 100&quot; 2. 自定义模块自定义模块存放路径: playbooks/library 2.1 使用 script 自定义 模块2.2 使用 Python 自定义模块.]]></content>
      <categories>
        <category>DevOps</category>
      </categories>
      <tags>
        <tag>ansible</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ansible-变量]]></title>
    <url>%2F2018%2F03%2F15%2FAnsible-%E5%8F%98%E9%87%8F%2F</url>
    <content type="text"><![CDATA[摘要 Ansible 学习总结 变量与 fact在 Ansible 中, 变量的作用域是按照主机划分的, 只有针对特定主机讨论变量的值才有意义. 1. 变量1.1. 定义变量vars : 定义变量的列表或字典vars_file : 指定 定义变量的文件列表 vars 区段的定义, 实际上是在 当前 play 中针对一组主机定义了变量, 但 Ansible 实际做法其实时, 对这个群组的每一个主机创建一个变量的副本. ansible 允许定义于主机或群组有关的变量, 这些变量可以定义在 inventory 文件中, 也可以定义在与 inventory 文件放在一起的独立文件中. Ansible 变量定义位置 变量标识 描述 vars playbook 区段, 为字典列表 vars_file playbook 区段, 为指向文件的列表 host_vars 目录, 主机变量 group_vars 目录, 群组变量 主机变量 inventory中, 单独针对主机的变量 群组变量 inventory中, 单独针对单个群组的变量 1.2. 显示变量: debug 模块- debug: var=myvarname 1.3. register 注册变量: 基于 task 的执行结果, 设置变量的值.示例: - name: Run MyProg command: /opt/myprog register: result ignore_errors: True - debug: var=result ignore_errors 语句, 可以实现, 在 task 失败的时候, 是否忽略错误, 继续执行下面的 task, 默认为 False. 访问变量中字典的key, 有两种方式: { { login.stdout } } { { ansible_eth1[&quot;ipv4&quot;][&quot;address&quot;] } } 当 task 在目标主机, 没有执行命令时, 即当目标主机已经符合目标结果时, 输出中没有 stdout,stderr,stdout_lines 三个键值. 如果在 playbook 中使用了注册变量, 那么无论模块是否改变了主机的状态, 请确保你了解变量的内容, 否则, 当你的 playbook 尝试访问注册变量中不存的 key时, 可能会导致失败. 1.4. set_fact 定义新变量使用 set_fact 模块在 task 中设置 fact(与定义一个新变量是一样的). 可以在 register 关键字后, 立即使用 set_fact , 这样使得变量引用更简单. - name: get snapshot id shell: &gt; aws ec2 describe-snapshot --filters Name=tag:Name, Valuse=my-snapshot | jq --raw-outpuy &quot;.Snapshots[].SnapshtId&quot; register: snap_result - set_fact: snap={ { snap_result.stdout } } - name: delete old snapshot command: aws ec2 delete-snapshot --snapshot-id &quot;{ { snap } }&quot; 1.5. 内置变量 参数 说明 hostvars 字典, key 为 Ansible 主机的名字, value 为所有变量名与相应变量值映射组成的字典 inventory_hostname 当前主机被 Ansible 识别的名字, 如果定义了别名, 则为别名. group_names 列表, 由当前主机所属的所有群组组成 groups 字典, key 为 ansible 群组名, value 为群组成员的主机名所组成的列表. 包括 all 分组和 ungrouped 分组 play_hosts 列表, 成员是当前 play 涉及的主机的 inventory 主机名. ansible_version 字典, 由 Ansible 版本信息组成. hostvars : 在 Ansible 中, 变量的作用域是按照主机划分的, 只有针对特定主机讨论变量的值才有意义. 有时候 , 针对一组主机定义的变量, 该变量实际始于特定的主机相关联的. 例如 vars 区段的定义, 实际上是在 当前 play 中针对一组主机定义了变量, 但 Ansible 实际做法其实时, 对这个群组的每一个主机创建一个变量的副本. hostvars变量包含了在所有主机上定义的所有变量, 并以 ansible 识别的主机名作为 key. 如果 Ansible 还未对主机采集 fact, 那么除非启动 fact 缓存, 否则无法使用 hostvars 访问fact. 有时, 在某一个主机上运行的 task 可能会需要在另一台主机上定义的变量. 例如, web 服务器, 可能需要 数据库服务器的 ansible_eth1.ipv4.address 这个 fact. 如果 数据库服务器为 db.example.com, 那么, 其变量引用为: { { hostvars[&apos;db.example.com&apos;].ansible_eth1.ipv4.address } } - debug: var=hostvars[inventory_hostname] : 输出与当前主机相关联的所有变量. groups : 代表当前 inventory 所定义的所有组的集合, 为一个字典. 示例: web 负载均衡配置文件 backend web-backend {% for host in groups.web %} server { { host.inventory_hostname } } { { host.ansible_default_ipv4.address } }:80 {% endfor %} 1.6. 在命令行设置变量向 ansible-playbook 传入 -e var=value 参数设置变量或传递参数, 有最高优先级. 可以覆盖已定义的变量值. $ ansible-playbook example.yml -e token=123456 希望在变量中出现空格, 需要使用引号: $ ansible-playbook playbooks/greeting.yml -e &apos;greeting=&quot;Oops you have another hello world&quot;&apos; `@filename.yml` 传递参数: $ cat greetvars.yml greeting: &quot;ops you have another hello world&quot; $ ansible-playbook playbooks/greeting.yml -e @greetvars.yml 2. fact当 Ansible 采集 fact 的时候, 他会连接到目标主机收集各种详细信息: CPU 架构,操作系统,IP地址,内存信息,磁盘信息等. 这些信息保存在被称为 fact 的变量中. fact 与其他变量的行为一模一样. 2.1. setup 模块实现 fact 收集的模块. 一般无需再 playbook 中调用该模块, Ansible 会在采集 fact 时, 自动调用. `$ ansible server_name -m setup -a &apos;filter=ansible_eth*&apos;` 其返回值为一个字典, 字典的 key 是 ansible_fact, 他的 value 是一个有实际 fact 的名字与值组成的字典. setup 模块支持 filter 参数, 可以实现 shell 通配符的匹配过滤. 2.2. 模块返回 fact如果一个模块返回一个字典且包含名为 ansible_facts 的key, 那么 ansible 将会根据对应的 value 创建响应的变量, 并分配给相对应的主机. 对于返回 fact 的模块, 并不需要使用注册变量, 因为 ansible 会自动创建. 可以自动返回 fact 的模块: $ ansible-doc --list |grep facts - ec2_facts - docker_image_facts 2.3. 本地 fact可将一个或者多个文件放置在目标主机的 /etc/ansible/facts.d/ 目录下, 如果该目录下的文件以 init格式, JSON格式 或者输出JSON格式的可执行文件(无需参数), 以这种形式加载的 fact 是 ansible_local 的特殊变量. 示例: # 目标主机 $ /etc/ansible/facts.d/books.fact [book] title=Ansible: Up and Running author=Lorin Hochstein publisher=P&apos;Reilly Media # ansible 主机 $ cat playbooks/local.yml - name: get local variables hosts: host_c gather_facts: True tasks: - name: print local variables; debug: var=ansible_local - name: print book title debug: msg=&quot;The Book Title is { { ansible_local.books.book.title } }&quot; 注意 ansible_local 变量值的结构, 因为 fact 文件的名称为 books, 所以 ansible_local 变量是一个字典, 且包含一个名为 &quot;books&quot; 的 key. 3. 变量优先级:以下优先级依次降低 命令行参数 其他 通过 inventory 文件或 YAML 文件定义的主机变量或群组变量 Fact 在 role 的 defaults/mail.yml 文件中的变量. 4. 过滤器: 变量加工处理Ansible 除了使用 Jinja2 作为模板之外, 还将其用于变量求值. 即, 可以在 playbook 中在 { {} } 内使用过滤器.除了可以用 Jinja2 的内置过滤器外, Ansible 还有一些自己扩展的过滤器. 有些参数, 需要参数, 有些则不需要. Jinja2 内置过滤器Ansible 过滤器 4.1. default : 设置默认值.# 设置 HOST 变量的默认值, 如果 database 没有被定义, 则使用 localhost . &quot;HOST&quot;: &quot;{ { database | default(&apos;localhost&apos;) } }&quot; 4.2. 用于注册变量的过滤器对注册变量状态检查状态的过滤器 名称 描述 failed 如果注册变量的值是任务 failed , 则返回 True changed 如果注册变量的值是任务 changed , 则返回 True success 如果注册变量的值是任务 success , 则返回 True skipped 如果注册变量的值是任务 skipped , 则返回 True 示例: - name: Run myprog command: /opt/myprog register: result ignore_errors: True - debug: var=result - debug: msg=&quot;Stop Running the playbook if myprog failed&quot; failed_when: result|failed 4.3. 用于文件路径的过滤器用于处理包含控制主机文件系统的路径的变量. 过滤器 描述 basename 文件路径中的目录 dirname 文件路径中的目录 expanduser 将文件路径中的 ~ 替换为用户家目录 realpath 处理符号链接后的文件实际路径 示例: vars: homepages: /usr/share/nginx/html/index.html tasks: - name: copy home page copy: src=files/{ { homepages| basename } } desc={ { homepages } } 4.4. 自定义过滤器Ansible 会在存放 playbook 的目录下的 filter_plugins 目录中寻找自定义过滤器. 也可以放在 /usr/share/ansible_plugins/filter_plugins/ 目录下, 或者 环境变量ANSIBLE_FILTER_PLUGINS 环境变量设置的目录. # filter_plugins/surround_by_quotes.py def surround_by_quote(a_list): return [&apos;&quot;%S&quot;&apos; % an_element for an_element in a_list] class FilterModule(object): def filters(self): return {&apos;surround_by_quote&apos;: surround_by_quote} surround_by_quote 函数定义了 Jinja2 过滤器.FilterModule 类定义了一个 filter 方法, 该方法返回由过滤器名称和函数本身组成的字典. FilterModule 是 Ansible 相关代码, 他使得 Jinja2 过滤器可以再 Ansible 中使用. 5. lookup: 从多种来源读取配置数据.lookup 官方文档说明Ansible 所有的 lookup 插件都是在控制主机, 而不是远程主机上执行的 支持的数据来源表: 名称 描述 file 文件的内容 password 随机生成密码 pipe 本地命令执行的输出 env 环境变量 template Jinja2 模板渲染的结果 csvfile .csv 文件中的条目 dnstxt DNS 的 TXT 记录 redis_ke 对 Redis 的key 进行查询 etcd 对 etcd 中的key 进行查询 file 示例: 在 playbook 中调用 lookup - name: Add my public key as an EC2 key ec2_key: name=mykey key_material=&quot;{ { lookup(&apos;file&apos;, &apos;/home/me/.ssh/id_rsa.pub&apos;) } }&quot; 示例: 使用 Jinja2 模板 # authorized_keys.j2 { { lookup(&apos;file&apos;, &apos;/home/me/.ssh/id_rsa.pub&apos;) } } # playbook - name: copy authorized_host file template: src=authorized_keys.j2 desc=/home/deploy/.ssh/authorized_keys pipe 在控制主机上调用一个外部程序, 并将这个程序的输出打印到标准输出上. 示例: 得到最新的 git commit 使用的 SHA-1 算法的值. - name: get SHA of most recent commit debug: msg=&quot;{ { lookup(&apos;pipe&apos;, &apos;git rev-parse HEAD&apos;) } }&quot; env 获取在控制主机上的某个环境变量的值. 示例: - name: get the current shell debug: msg=&quot;{ { lookup(&apos;env&apos;, &apos;SHELL&apos;) } }&quot; password 随机生成一个密码, 并将这个密码写入到参数指定的(控制主机)文件中. 示例: 生成 deploy 的 Postgre 用户和密码, 并将密码写入到 deploy-password.txt 中: - name: create deploy postgre user postgresql_user: name: deploy password: &quot;{ { lookup(&apos;password&apos;, &apos;deploy-password.txt&apos;) } }&quot; template 指定一个 Jinji2 模板文件, 并返回这个模板渲染的结果. # message.j2 This host runs { { ansible_distribution } } # task - name: output message from template debug: msg=&quot;{ { lookup(&apos;template&apos;, &apos;message.j2&apos;) } }&quot; csvfile 从 csv 文件中读取一个条目. # users.csv username, email lorin, lorin@example.com john, john@example.com sue, sue@example.com # 调用 : 查看名为 users.csv 的文件, 使用逗号作为分隔符来定位区域, 寻找第一列的值是 sue 的那一行, 返回第二列(索引从 0 开始)的值. lookup(&apos;csvfile&apos;, &apos;sue file=users.csv delimiter=, col=1&apos;) --&gt; sue@example.com # 用户名被存储在 username 变量中, 可以用 &quot;+&quot; 连接其他参数, 构建完整的参数字符串. lookup(&apos;csvfile&apos;, username + &apos;file=users.csv delimiter=, col=1&apos;) dnstxt 需要安装 dnspython 包, $ pip install dnspython TXT 记录是 DNS 中一个可以附加在主机名上的任意字符串, 一旦为主机名关联了一条 TXT 记录, 则任何人都可以使用 DNS 客户端获取这段文本. # 使用 dig 查看 TXT 记录 $ dig +short ansiblebook.com TXT &quot;isbn=97801491915325&quot; # task - name: look up TXT record debug: msg=&quot;{ { lookup(&apos;dnstxt&apos;, &apos;ansiblebook.com&apos;) } }&quot; redis-kv 需要安装 redis 包: $ pip install pip 可以使用 redis-kv 获取一个 key 的value, key 必须为字符串. # 设置一个值: $ redis-cli SET weather sunny # task - name: look up value in redis debug: msg=&quot;{ { lookup(&apos;redis_kv&apos;, &apos;redis://localhost:6379,weather&apos;) } }&quot; etcd etcd lookup 默认在 http://127.0.0.1:4001 上查找 etcd 服务器, 可以在执行 ansible-playbook 之前, 通过设置 ANSIBLE_ETCD_URL 改变这个值. # 设置测试值 $ curl -L http://127.0.0.1:4001/v2/keys/weather -XPUT -d value=cloudy # task - name: loop up value in etcd debug: msg=&quot;{ { lookup(&apos;etcd&apos;, &apos;weather&apos;) } }&quot;]]></content>
      <categories>
        <category>DevOps</category>
      </categories>
      <tags>
        <tag>ansible</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ansible-流程控制]]></title>
    <url>%2F2018%2F03%2F15%2FAnsible-%E6%B5%81%E7%A8%8B%E6%8E%A7%E5%88%B6%2F</url>
    <content type="text"><![CDATA[Ansible 的流程控制机制, 与使用方法. Ansible 学习总结 Ansible 本身除了配置管理工具外, 也可以说是一门配置管理语言. 循环 条件 函数 与 role handler 触发器. 循环迭代 Ansible 总是使用 item 作为 循环迭代变量的名字. 循环结构汇总: 官方文档 | 名称 | 输入 | 循环策略 | | — | — | — | | with_items | 列表 | 对列表元素进行循环 | | with_lines | 要执行的命令 | 对命令输出结果进行逐行循环 | | with_fileglob | glob | 对文件名进行循环 | | with_first_found | 路径的列表 | 输入中第一个存在的文件 | | with_dict | 字典 | 对字典元素进行循环 | | with_flattened | 列表的列表 | 对所有列表的元素顺序循环 | | with_indexed_items | 列表 | 单次迭代 | | with_nested | 列表 | 循环嵌套 | | with_random_choice | 列表 | 单次迭代 | | with_sequence | 参数数组 | 对数组进行循环 | | with_subelements | 字典的列表 | 嵌套循环 | | with_together | 列表的列表 | 对多个列表进行循环 | | with_inventory_hostname | 主机匹配模式 | 对匹配的主机进行循环 | with_items # apt 模块会一次调用整个列表作为参数, 而不是单个调用. # 有些模块支持(调用整个列表), 有些不支持, 如 pip 即是单个调用. - name: install apt packages apt: pkg={{ item }} update_cache=yes cache_valid_time=3600 sudo: True with_items: - git - python-dev - python-pip - supervisor # 另种方式: 传递字典的列表. - name: install python packages pip: name={{ item.name }} version={{ item.version }} virtualenv={{ venv_path }} with_items: - {name: mazzanine, version: 3.1.10} - {name: gunicorn, version: 19.1.1} with_lines 可以在控制主机执行任意命令, 并对命令的输出进行逐行迭代. - name: Send out a slack messag slack: domain: example.slack.com token: &quot;{{ slack_token }}&quot; msg: &quot;{{ item }} was in the list&quot; with_lines: - cat /path/to/mylist.txt with_fileglob 对于迭代控制主机上的一系列文件很有用. - name: add public keys to account authorized_key: user=deploy key=&quot;{{ lookup('file', item) }}&quot; with_fileglob: - /var/keys/*.pub - keys/*.pub with_dict 对字典进行迭代. 当使用该循环结构时, item 循环变量是一个含有如下两个 key 的字典: key: 字典中的一个 key value: 字典中与上面的key 相对应的 value 示例: ansible_eth0.ipv4 { &quot;address&quot;: &quot;10.0.2.15&quot;, &quot;netmask&quot;: &quot;255.255.255.0&quot;, &quot;network&quot;: &quot;10.0.2.0&quot; } task 示例 name: iterate over ansible_eth0debug: msg==with_dict: ansible_eth0.ipv4 条件执行 when: 当 when 表达式返回 True 时, 执行该 Task , 否则跳过该 Task. changed_when &amp; failed_when 函数 与 role 类似 其他编程语言中的 函数或类, 可以实现复用.]]></content>
      <categories>
        <category>DevOps</category>
      </categories>
      <tags>
        <tag>ansible</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ansible-Inventory.md]]></title>
    <url>%2F2018%2F03%2F15%2FAnsible-Inventory%2F</url>
    <content type="text"><![CDATA[Ansible 静态 Inventory 与 动态 Inventory 配置与使用.Ansible 学习总结 inventory : Ansible 可管理主机的集合.1. 静态 Inventory1.1 inventory 行为参数示例 : [targets] localhost ansible_connection=local other1.example.com ansible_connection=ssh ansible_ssh_user=mpdehaan ansible_ssh_pass=123456 other2.example.com ansible_connection=ssh ansible_ssh_user=mdehaan ansible_ssh_pass=123456 名称 默认值 说明 ansible_ssh_host 主机的名字 ssh 目的主机的主机名或IP ansible_ssh_port 22 ssh 默认端口号 ansible_ssh_user root ssh 登录使用的用户名 ansible_ssh_pass none ssh 认证使用的密码(这种方式并不安全,我们强烈建议使用 –ask-pass 或 SSH 密钥) ansible_sudo_pass none sudo 密码(这种方式并不安全,我们强烈建议使用 –ask-sudo-pass) ansible_sudo_exe (new in version 1.8) none sudo 命令路径(适用于1.8及以上版本) ansible_connection smart Ansible 使用何种连接模式连接到目标主机 . 与主机的连接类型.比如:local, ssh 或者 paramiko. Ansible 1.2 以前默认使用 paramiko. 1.2 以后默认使用 ‘smart’,’smart’ 方式会根据是否支持 ControlPersist, 来判断’ssh’ 方式是否可行. ansible_ssh_private_key_file none SSH 认证使用的私钥 ansible_shell_type sh 命令所使用的 shell, 除了 sh 外, 还支持 csh,fish,powershell ansible_python_interpreter /usr/bin/python 目标主机上的 python 解释器 ansible_*_interperter none 与 ansible_python_interpreter 的工作方式相同,可设定如 ruby 或 perl 的路径…. 1.2 ansible.cfg 设置 Inventory 行为参数默认值可以在 [defaults] 中改变一些行为参数的默认值: inventory 行为参数 ansible.cfg 选项 ansible_ssh_port remote_port ansible_ssh_user remote_user ansible_ssh_private_key_file private_key_file ansible_shell_type, shell 的名称 executable, shell 的绝对路径 1.3 群组 all 群组 ansible 自动定义了一个群组为 `all` 或 `*` , 包括 inventory 中的所有主机. 群组嵌套 [django:children] web mysql 模式匹配的主机 : 正则表达式永远以 ~ 开头 | 匹配行为 | 用法示例 | | — | — | | 所有主机 | all | | 所有主机 | * | | 群组的并集 | dev:staging | | 群组的交集 | dev:&amp;staging | | 排除 | dev:!staging | | 通配符 | *.example.com | | 数字范围 | web[1:20].example.com,web[01:20].example.com | | 字母范围 | web-[a-z].example.com | | 正则表达式 | ~web\d\.example\.(com | | 多种模式匹配组合使用 | hosts: dev:staging:&amp;database:!queue | 限制某些主机执行: -l 或 --limit 只针对限定的主机运行. $ ansible-playbook -l hosts playbook.yml $ ansible-playbook --limit hosts playbook.yml # 使用模式匹配语法 $ ansible-playbook -l &apos;staging:&amp;database&apos; playbook.yml 1.4 主机与群组变量 主机变量, 在 inventory 文件中: a.example.com color=red b.example.com color=green 群组变量, 在 inventory 文件中: [all:vars] ntp_server=ntp.ubuntu.com [prod:vars] db_primary_host=prod.db.com db_primary_port=5432 db_replica_host=rep.db.com db_name=mydb db_user=root db_pass=123456 [staging:vars] ... 主机变量和群组变量: 在各自的文件中 可以为每个主机和群组创建独立的变量文件. ansible 使用 YAML 格式来解析这些变量文件. host_vars 目录 : 主机变量文件 group_vars 目录 : 群组变量文件 ansible 假设这些目录在包含 playbook 的目录下 或者与 inventory 文件相邻的目录下. 键值格式 : # playbooks/group_vars/production db_primary_host: prod.db.com db_primary_port: 5432 db_replica_host: rep.db.com db_name: mydb db_user: root db_pass: 123456 # 访问方法: {{ db_primary_host }} 字典格式 : # playbooks/group_vars/production db: user: root password: 123456 name: mydb primary: host: primary.db.com port: 5432 replica: host: replica.db.com port: 5432 rabbitmq: host: rabbit.example.com port: 6379 # 访问方法 {{ db.primary.host }} 将 group_vars/production/ 定义为目录, 将多个包含变量定义的 YAML 文件存放其中; # group_vars/production/db db: user: root password: 123456 name: mydb primary: host: primary.db.com port: 5432 replica: host: replica.db.com port: 5432 # group_vars/production/rebbitmq rabbitmq: host: rabbit.example.com port: 6379 2. 动态 inventory如果 inventory 文件标记为可执行, 那么 Ansible 会假设这是一个动态 inventory 脚本, 并且会执行他, 而不是读取他的内容. 2.1 动态 inventory 脚本的接口--list : 列出所有群组. 输出为一个 JSON 对象, 该对象名为群组名, 值为主机的名字组成的数组. --host=&lt;host_name&gt; : 输出是一个名为变量名, 值为变量值的 JSON 对象. 包含主机的所有特定变量和行为参数. 2.2 在运行时添加主机或群组: add_host, group_by add_host : 调用方式如下: 当做一个模块使用即可 # 使用方法: add_host name=hostname groups=web,staging myvar=myval` # 示例 - name: add the vagrant hosts to the inventory add_host: name=vagrant ansible_ssh_host=127.0.0.1 ansible_ssh_port=2222 ansible_ssh_user=vagrant add_host 模块添加主机仅在本次 playbook 执行过程中有效, 他并不会修改 inventory 文件. group_by : 是一个 模块. 允许在 playbook 执行的时候, 使用 group_by 模块创建群组, 他可以基于已经为每台主机自动设定好的变量值(fact)来创建群组, Ansible 将这些变量称为 fact. - name: grout hosts by distribution hosts: myhosts gather_facts: True tasks: - name: create groups based on Linux distribution group_by: key={{ ansible_distribution }} - name: do something to CentOS hosts hosts: CentOS tasks: - name: install htop yum: name=htop - name: do something to Ubuntu hosts hosts: Ubuntu tasks: - name: install htop apt: name=htop 2.3 ec2.py &amp; ec2.ini 安装配置 AWS EC2 External Inventory Script ec2.py : 动态 Inventory ec2.ini : Inventory 配置文件. 只支持 Python 2.x 缓存 \$HOME/.ansible/tmp/ansible-ec2.cache \$HOME/.ansible/tmp/ansible-ec2.index 缓存过期时间: ec2.ini [ec2] cache_max_age = 0 # 默认 300s, 当值为 0 时, 为不使用缓存. $ ./ec2.py –refresh-cache # 强制刷新缓存 群组 自动生成的群组: | 类型 | 示例 | ansible 群组名 | | — | — | — | | Instance | i-123456 | i-123456 | | Instance type | c1.medium | type_c1_medium | | Security group | ssh | secutity_group_ssh | | Keypair | foo | key_foo | | Region | us-east-1 | us-east-1 | | Tag | env=staging | tag_env_staging | | Availability zone | us-easr-1b | us-easr-1b | | VPC | vpc-14dd1b70 | vpc_id_vpc-14dd1b70 | | all ec2 instance | N/A | ec2 | 在群组名中只有字母,连字符,下划线是合法的. 动态 Inventory 脚本会自动将其他的字符(如空格)转换成下划线.如 Name=My cool name 变为 tag_Name_my_cool_server. 群组操作 ec2.py 生成的群组, 支持 Ansible 群组的交集,并集等操作. 自动生成的群组与 静态 inventory 结合使用: 假设 ec2.py 生成的群组中有一个从 tag 取名的群组名为 tag_type_web, 则可以在 静态inventory文件中重新定义, 或者组合群组. 必须在 静态 inventory 中定义一个空的名为 tag_type_web 的群组, 如果没有定义, 则ansible 会报错. 示例如下: [web:children] tag_type_web [tag_type_web] 使用方法 # 简单使用方法 $ ansible -i ec2.py -u ubuntu us-east-1 -m ping # 复杂使用方法. $ cp ec2.py /etc/ansible/hosts &amp;&amp; chmod +x /etc/ansible/hosts/ec2.py $ cp ec2.ini /etc/ansible/ec2.ini $ export AWS_ACCESS_KEY_ID=&apos;AK123&apos; $ export AWS_SECRET_ACCESS_KEY=&apos;abc123&apos; # just for test, you should see your entire EC2 inventory across all regions in JSON. $ ./ec2.py --list [ --profile PROFILE ] --profile : manage multple AWS accounts, a profile example : [profile dev] aws_access_key_id = &lt;dev access key&gt; aws_secret_access_key = &lt;dev secret key&gt; [profile prod] aws_access_key_id = &lt;prod access key&gt; aws_secret_access_key = &lt;prod secret key&gt; --profile prod, --profile dev ec2.ini : is configured for all Amazon cloud services, but you can comment out any features that aren’t applicable. including cache control and destination variables. 3. 静态 Inventory 与 动态 Inventory 结合使用配置步骤如下: 将 动态inventory 和 静态inventory 放在同一目录下; 在 ansible.cfg 中将 hostfile 的值, 指向该目录即可.]]></content>
      <categories>
        <category>DevOps</category>
      </categories>
      <tags>
        <tag>ansible</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[自动化测试-Splinter]]></title>
    <url>%2F2018%2F03%2F15%2F%E8%87%AA%E5%8A%A8%E5%8C%96%E6%B5%8B%E8%AF%95-splinter%2F</url>
    <content type="text"><![CDATA[使用 Splinter 做自动化测试.Splinter Doc Splinter是一个使用Python开发的开源Web应用测试工具，它可以帮你实现自动浏览站点和与其进行交互。 Splinter 是一个基于 Selenium, PhantomJS, zope.testbrowser 等已存在浏览器的抽象层度较高的自动化测试工具. 1. 特点 API 简单 多浏览器支持, 支持的 浏览器列表如下: chrome webdriver, firefox webdriver, phantomjs webdriver, zopetestbrowser, remote webdriver 支持 CSS 选择器和 Xpath 选择器 支持 iframe 和 alert 支持执行 JavaScript 支持 ajax 调用和 async JavaScript 2. 入门2.1 安装 安装 python 支持 Python 2.7+ 版本 安装 splinter $ pip install splinter 安装浏览器驱动: 以 chrome 为例: https://chromedriver.storage.googleapis.com/index.html?path=2.35/ 2.2 示例:from splinter import Browser # 初始化 browser browser = Browser() # 打开首页 browser.visit(&apos;http://google.com&apos;) browser.fill(&apos;q&apos;, &apos;splinter - python acceptance testing for web applications&apos;) browser.find_by_name(&apos;btnG&apos;).click() if browser.is_text_present(&apos;splinter.readthedocs.io&apos;): print &quot;Yes, the official website was found!&quot; else: print &quot;No, it wasn&apos;t found... We need to improve our SEO techniques&quot; browser.quit() Browser 对象支持 上下文管理: with Browser() as browser: # code here 3. 基础浏览器行为和交互3.1 网页对象 Browser 对象初始化 browser = Browser(&apos;chrome&apos;) browser = Browser(&apos;firefox&apos;) browser = Browser(&apos;zope.testbrowser&apos;) browser = Browser(driver_name=&quot;chrome&quot;, executable_path=&quot;/path/to/chrome&quot;, user_agent=&quot;Mozilla/5.0 (iPhone; U; CPU like Mac OS X; en)&quot;, incognito=True) 浏览网页 browser.visit(&apos;http://cobrateam.info&apos;) browser.visit(&apos;http://username:password@cobrateam.info/protected&apos;) # basic HTTP 认证 重载网页 browser.reload() You can back and forward on your browsing history using back and forward methods: browser.visit(&apos;http://cobrateam.info&apos;) browser.visit(&apos;https://splinter.readthedocs.io&apos;) browser.back() browser.forward() 获取当前网页的相关内容 browser.title # 网页标题 browser.html # 网页的 html 代码 browser.url # 网页的 url 管理多个窗口, 使用 windows 对象, 例如弹出窗口. browser.windows # all open windows browser.windows[0] # the first window browser.windows[window_name] # the window_name window browser.windows.current # the current window browser.windows.current = browser.windows[3] # set current window to window 3 window = browser.windows[0] window.is_current # boolean - whether window is current active window window.is_current = True # set this window to be current window window.next # the next window window.prev # the previous window window.close() # close this window window.close_others() # close all windows except this one 3.2 查找网页元素 网页元素获取 splinter 支持 6 种元素查找方式, 每种方式均返回列表作为查找结果. 支持 first, last 快捷方式, 查找第一个和最后一个元素. 因为每个页面中, id 的值一般是不重复的, 因此 find_by_id 总是返回只有一个元素的列表. browser.find_by_css(&apos;h1&apos;) browser.find_by_xpath(&apos;//h1&apos;) browser.find_by_tag(&apos;h1&apos;) browser.find_by_name(&apos;name&apos;) browser.find_by_text(&apos;Hello World!&apos;) browser.find_by_id(&apos;firstheader&apos;) browser.find_by_value(&apos;query&apos;) browser.find_by_xpath(&apos;//h1&apos;).first browser.find_by_name(&apos;name&apos;).last browser.find_by_tag(&apos;h1&apos;)[1] 获取元素的值 browser.find_by_css(&apos;h1&apos;).first.value 查找 URL 返回列表作为结果. links_found = browser.find_link_by_text(&apos;Link for Example.com&apos;) links_found = browser.find_link_by_partial_text(&apos;for Example&apos;) links_found = browser.find_link_by_href(&apos;http://example.com&apos;) links_found = browser.find_link_by_partial_href(&apos;example&apos;) Clicking links : These methods return the first element always. # 绝对 url browser.click_link_by_href(&apos;http://www.the_site.com/my_link&apos;) # 相对 url browser.click_link_by_partial_href(&apos;my_link&apos;) browser.click_link_by_text(&apos;my link&apos;) browser.click_link_by_partial_text(&apos;part of link text&apos;) browser.click_link_by_id(&apos;link_id&apos;) 链式查找 Finding method are chainable, so you can find the descendants of a previously found element. divs = browser.find_by_tag(&quot;div&quot;) within_elements = divs.first.find_by_name(&quot;name&quot;) ElementDoesNotExist 异常 If an element is not found, the find_* methods return an empty list. But if you try to access an element in this list, the method will raise the splinter.exceptions.ElementDoesNotExist exception. Clicking buttons You can click in buttons. Splinter follows any redirects, and submits forms associated with buttons. browser.find_by_name(&apos;send&apos;).first.click() browser.find_link_by_text(&apos;my link&apos;).first.click() 表单 browser.fill(&apos;query&apos;, &apos;my name&apos;) browser.attach_file(&apos;file&apos;, &apos;/path/to/file/somefile.jpg&apos;) browser.choose(&apos;some-radio&apos;, &apos;radio-value&apos;) browser.check(&apos;some-check&apos;) browser.uncheck(&apos;some-check&apos;) browser.select(&apos;uf&apos;, &apos;rj&apos;) To trigger JavaScript events, like KeyDown or KeyUp, you can use the type method. browser.type(&quot;type&quot;, &quot;typing text&quot;) If you pass the argument slowly=True to the type method you can interact with the page on every key pressed. Useful for test field’s autocompletion (The browser will wait until next iteration to type the subsequent key). for key in browser.type(&quot;type&quot;, &quot;typing slowly&quot;, slowly=True): pass You can also use type and fill methods in an element. browser.find_by_name(&quot;name&quot;).type(&quot;Steve Jobs&quot;, slowly=True) browser.find_by_css(&quot;.city&quot;).fill(&quot;San Francisco&quot;) 判断元素是否可见. # 返回布尔值 browser.find_by_css(&apos;h1&apos;).first.visible 判断元素是否有 className # 返回布尔值 browser.find_by_css(&apos;.content&apos;).first.has_class(&apos;content&apos;) Interacting with elements through a ElementList object You can invoke any Element method on ElementList and it will be proxied to the first element of the list. So the two lines below are quivalent. assert browser.find_by_css(&apos;a.banner&apos;).first.visible assert browser.find_by_css(&apos;a.banner&apos;).visible 3.3 鼠标大多数鼠标事件目前只支持 Chrome 和 Firefox 27.0.1 支持 mouse_over, mouse_out,单击, 双击, 右击鼠标. mouse_over : puts the mouse above the element. browser.find_by_tag(&apos;h1&apos;).mouse_over() mouns_out : puts the mouse out of the element. browser.find_by_tag(&apos;h1&apos;).mouse_out() click : 单击 browser.find_by_tag(&apos;h1&apos;).click() double_click : 双击 browser.find_by_tag(&apos;h1&apos;).double_click() right_click : 右击 browser.find_by_tag(&apos;h1&apos;).right_click() drag_and_drop : You can drag an element and drop it to another element. The example below drags the &lt;h1&gt; ... &lt;/h1&gt; element and drop it to a container element (identified by a CSS class). draggable = browser.find_by_tag(&apos;h1&apos;) target = browser.find_by_css(&apos;.container&apos;) draggable.drag_and_drop(target) 3.4 Ajax &amp; Async JavaScriptWhen working with Ajax and Asynchronous JavaScript, it’s common to have elements which are not present in the HTML code(they are created with JavaScript, dynamically). In this case, you can use the methods is_element_present and is_text_present to check the existence of an element or text – Splinter will load the HTML and JavaScript in the browser and the check will be performed before processing JavaScript. There is also the optional argument wait_time (given in seconds), it’s a timeout: if the verification method gets True it will return the result (even if the wait_time is not over); if it doesn’t get True, the method will wait until the wait_time is over (sp it’ll return the result). # 检查文本是否 存在 browser = Browser() browser.visit(&apos;https://splinter.readthedocs.io/&apos;) browser.is_text_present(&apos;splinter&apos;) # True browser.is_text_present(&apos;splinter&apos;, wait_time=10) # True, using wait_time browser.is_text_present(&apos;text not present&apos;) # False # 检查文本是否 不存在 browser.is_text_not_present(&apos;text not present&apos;) # True browser.is_text_not_present(&apos;text not present&apos;, wait_time=10) # True, using wait_time browser.is_text_not_present(&apos;splinter&apos;) # False 元素(element)存在性检查, 返回布尔值. # 检查元素是否 存在 browser.is_element_present_by_css(&apos;h1&apos;) browser.is_element_present_by_xpath(&apos;//h1&apos;) browser.is_element_present_by_tag(&apos;h1&apos;) browser.is_element_present_by_name(&apos;name&apos;) browser.is_element_present_by_text(&apos;Hello World!&apos;) browser.is_element_present_by_id(&apos;firstheader&apos;) browser.is_element_present_by_value(&apos;query&apos;) browser.is_element_present_by_value(&apos;query&apos;, wait_time=10) # using wait_time # 检查元素是否 不存在 browser.is_element_not_present_by_css(&apos;h6&apos;) browser.is_element_not_present_by_xpath(&apos;//h6&apos;) browser.is_element_not_present_by_tag(&apos;h6&apos;) browser.is_element_not_present_by_name(&apos;unexisting-name&apos;) browser.is_element_not_present_by_text(&apos;Not here :(&apos;) browser.is_element_not_present_by_id(&apos;unexisting-header&apos;) browser.is_element_not_present_by_id(&apos;unexisting-header&apos;, wait_time=10) # using wait_time 3.5 cookie 管理It is possible to manipulate cookies using the cookies attribute from a Browser instance. The cookies attribute is a instance of a CookieManager class that manipulates cookies, like adding and deleting them. 添加 cookies browser.cookies.add({&quot;key&quot;: &quot;value&quot;}) 检索 cookies browser.cookies.all() 删除 cookies 删除 单个 cookies browser.cookies.delete(&quot;key1&quot;) # 删除 单个 cookies browser.cookies.delete(&quot;key1&quot;, &quot;key2&quot;) # 删除 两个 cookies 删除 所有 cookies browser.cookies.delete() 4. JavaScript 支持You can easily execute JavaScript in drivers which support it. browser.execute_script(&quot;$(&apos;body&apos;).empty()&quot;) You can return the result of the script. browser.evaluate_script(&quot;4+4&quot;) == 8 5. 其他5.1 HTTP 响应码处理及异常处理status_code and this HTTP exception handling is available only for selenium webdriver browser.visit(&quot;http://www.baidu.com&quot;) browser.status_code.is_success() # True browser.status_code == 200 # True browser.status_code.code # 200 当网页返回失败时, 触发 HttpResponseError 错误. try: browser.visit(&apos;http://cobrateam.info/i-want-cookies&apos;) except HttpResponseError, e: print &quot;Oops, I failed with the status code %s and reason %s&quot; % (e.status_code, e.reason) 5.2 iframersYou can use the get_iframe method and the with statement to interact with iframe. You can pass the iframe’s name, id, or index to get_iframe. with browser.get_iframe(&apos;iframemodal&apos;) as iframe: iframe.do_stuff() 5.3 alert and promptsOnly webdrivers (Firefox and Chrome) has support for alerts and prompts You can deal with alerts and prompts using the get_alert method. alert = browser.get_alert() alert.text alert.accept() alert.dismiss() In case of prompts, you can answer it using the fill_with method. prompts = browser.get_alert() prompts.text prompts.fill_with(&quot;text&quot;) prompts.accept() prompts.dismiss() You can use the with statement to interacte with both alerts and prompts too. with browser.get_alert() as alert: alert.do_stuff() IMPORTANT : if there’s not any prompt or alert, get_alert will return None. Remember to always use at least one of the alert/prompt ending methods(accept/dismiss). Otherwise your browser instance will be frozen until you accept or dismiss the alert/prompt correctly. 6. Drivers6.1 Chrome 安装 # 依赖 Selenium $ pip install selenium # 需要安装 chrome 浏览器 使用 headless option for Chrome browser = Browser(&quot;chrome&quot;, headless=True) incognito option: 隐身模式 browser = Browser(&quot;chrome&quot;, incognito=True) emulation option: 仿真模式 from selenium import webdriver from splinter import Browser mobile_enulation = {&quot;driverName&quot;: &quot;Google Nexus 5&quot;} chrome_options = webdriver.ChromeOptions() chrome_options.add_experimental_option(&quot;mobileEmulation&quot;, mobile_enulation) browser = Browser(&quot;chrome&quot;, options=chrome_options) screenshot: Take a screenshot of the current page and saves it locally. screenshot(name=None, suffix=&apos;.png&apos;)]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>自动化测试</tag>
        <tag>Splinter</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[fake-useragent-文档]]></title>
    <url>%2F2018%2F03%2F15%2Ffake-useragent-%E6%96%87%E6%A1%A3%2F</url>
    <content type="text"><![CDATA[使用 fake-useragent 为爬虫提供 UserAgent.grabs up to date useragent from useragentstring.comrandomize with real world statistic via w3schools.com https://fake-useragent.herokuapp.com/browsers/0.1.5 1. 安装$ pip install fake-useragent 2. 使用from fake_useragent import UserAgent ua = UserAgent() ua.random # and the best one, random via real world browser usage statistic ua.ie # Mozilla/5.0 (Windows; U; MSIE 9.0; Windows NT 9.0; en-US); ua.msie # Mozilla/5.0 (compatible; MSIE 10.0; Macintosh; Intel Mac OS X 10_7_3; Trident/6.0)&apos; ua[&apos;Internet Explorer&apos;] # Mozilla/5.0 (compatible; MSIE 8.0; Windows NT 6.1; Trident/4.0; GTB7.4; InfoPath.2; SV1; .NET CLR 3.3.69573; WOW64; en-US) ua.opera # Opera/9.80 (X11; Linux i686; U; ru) Presto/2.8.131 Version/11.11 ua.chrome # Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.2 (KHTML, like Gecko) Chrome/22.0.1216.0 Safari/537.2&apos; ua.google # Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_4) AppleWebKit/537.13 (KHTML, like Gecko) Chrome/24.0.1290.1 Safari/537.13 ua[&apos;google chrome&apos;] # Mozilla/5.0 (X11; CrOS i686 2268.111.0) AppleWebKit/536.11 (KHTML, like Gecko) Chrome/20.0.1132.57 Safari/536.11 ua.firefox # Mozilla/5.0 (Windows NT 6.2; Win64; x64; rv:16.0.1) Gecko/20121011 Firefox/16.0.1 ua.ff # Mozilla/5.0 (X11; Ubuntu; Linux i686; rv:15.0) Gecko/20100101 Firefox/15.0.1 ua.safari # Mozilla/5.0 (iPad; CPU OS 6_0 like Mac OS X) AppleWebKit/536.26 (KHTML, like Gecko) Version/6.0 Mobile/10A5355d Safari/8536.25 3. update saved database just:from fake_useragent import UserAgent ua = UserAgent() ua.update() Sometimes, useragentstring.com or w3schools.com changes their html, or down, in such case fake-useragent uses hosted cache server heroku.com fallback .If You don’t want to use hosted cache server (version 0.1.5 added) from fake_useragent import UserAgent ua = UserAgent(use_cache_server=False) 4. caech Exceptionfrom fake_useragent import FakeUserAgentError try: ua = UserAgent() except FakeUserAgentError: pass 5. if you use a unknown useragent , it will not raise a error, but return “Your favorite Browser”.import fake_useragent ua = fake_useragent.UserAgent(fallback=&apos;Your favorite Browser&apos;) ua.just_test_agent &apos;Your favorite Browser&apos;]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>UserAgent</tag>
        <tag>fake-useragent</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python-装饰器]]></title>
    <url>%2F2018%2F03%2F15%2FPython-%E8%A3%85%E9%A5%B0%E5%99%A8%2F</url>
    <content type="text"><![CDATA[摘要装饰器通常是一个命名的对象(不允许 lambda 表达式), 在被(装饰函数)调用时接受单一参数, 并返回另一个可调用对象. 这里的可调用对象, 不仅仅包含函数和方法, 还包括类. 任何可调用对象(任何实现了 call 方法的对象都是可调用的)都可用作装饰器, 他们返回的对象也不是简单的函数, 而是实现了自己的 call 方法的更复杂的类实例. @some_decorator def decorated_function(): pass # 以上写法总是可以替换为显式的装饰器调用和函数的重新赋值: decorated_function = some_decorator(decorated_function) 1. 装饰器定义/使用方法1.1 通用模式: 作为一个函数def mydecorator(function): def wrapped(*args, **kwargs): # 在函数调用之前, 做点什么 result = function(*args, **kwargs) # 在函数调用之后, 做点什么 # 返回结果 return result # 返回 wrapper 作为装饰函数 return wrapped 1.2 实现 call 方法: 作为一个类非参数化装饰器用作类的通用模式如下: class DecoratorAsClass: def __init__(self, function): self.function = function def __call__(self, *args, **kw): # 在调用原始函数之前, 做点什么 result = self.function(*args, **kwargs) # 在调用原始函数之后, 做点什么 # 返回结果 return result 1.3 参数化装饰器 : 实现第二层包装def repeat(number=3): &quot;&quot;&quot; 多次重复执行装饰函数, 返回最后一次原始函数调用的值作为结果. : param number: 重复次数, 默认值为 3 &quot;&quot;&quot; def actual_decorator(function): def wrapped(*args, **kwargs): result = None for _ in range(number): result = function(*args, **kwargs) return result return wrapped return actual_decorator @repeat(2) def foo(): print(&quot;foo&quot;) 带参数的装饰器总是可以做如下装换: foo = repeat(number=3)(foo) 即使参数化装饰器的参数有默认值, 但名字后面也必须加括号 @repeat() def bar(): print(&quot;bar&quot;) 1.4 保存内省的装饰器使用装饰器的常见缺点是: 使用装饰器时, 不保存函数元数据(主要是文档字符串和原始函数名). 装饰器组合创建了一个新函数, 并返回一个新对象, 完全没有考虑原函数的标志. 这将导致调试装饰器装饰过的函数更加困难, 也会破坏可能用到的大多数自动生产文档的工具, 应为无法访问原始的文档字符串和函数签名. 解决这个问题的方式, 就是使用 functools 模块内置的 wraps() 装饰器. from functools import wraps def preserving_decorator(function): @wraps(function) def wrapped(*args, **kwargs): &quot;&quot;&quot;包装函数内部文档&quot;&quot;&quot; return function(*args, **kwargs) return wrapped @preserving_decorator def function_with_important_docstring(): &quot;&quot;&quot;这是我们想要保存的文档字符串&quot;&quot;&quot; pass print(function_with_important_docstring.__name__) print(function_with_important_docstring.__doc__) 2. 装饰器常用示例2.1 参数检查检查函数接受或返回的参数, 在特定上下文中执行时可能有用. # 装饰器代码 rpc_info = {} # 在实际读取时, 这个类定义会填充 rpc_info 字典, 并用于检查参数类型的特定环境中. def xmlrpc(in_=(), out=(type(None), )): def _xmlrpc(function): # 注册签名 func_name = function.__name__ rpc_info[func_name] = (in_, out) def _check_types(elements, types): &quot;&quot;&quot;用来检查类型的子函数&quot;&quot;&quot; if len(elements) != len(types): raise TypeError(&quot;Argumen count is wrong&quot;) typed = enumerate(zip(elements, types)) for index, couple in typed: arg, of_the_right_type = couple if isinstance(arg, of_the_right_type): continue raise TypeError(&quot;Arg #%d should be %s&quot; % (index, of_the_right_type)) def __xmlrpc(*args): # 没有允许的关键词 # 检查输入的内容 if function.__class__ == &quot;method&quot;: checkable_args = args[1:] # 类方法, 去掉 self else: checkable_args = args[:] # 普通函数 _check_types(checkable_args, in_) # 运行函数 res = function(*args) # 检查输入内容 if not type(res) in (tuple, list): checkable_res = (res, ) else: checkable_res = res _check_types(checkable_res, out) # 函数机器类型检查成功 return res return __xmlrpc return _xmlrpc # 使用示例 class RPCView: @xmlrpc((int, int)) # two int --&gt; None def meth1(self, int1, int2): print(&quot;received %d and %d&quot; % (int1, int2)) @xmlrpc((str, ), (int, )) # string --&gt; int def meth2(self, phrase): print(&quot;received %s&quot; % phrase) return 12 # 调用输出 print(rpc_info) # 输出: # {&apos;meth1&apos;: ((&lt;class &apos;int&apos;&gt;, &lt;class &apos;int&apos;&gt;), (&lt;class &apos;NoneType&apos;&gt;,)), &apos;meth2&apos;: ((&lt;class &apos;str&apos;&gt;,), (&lt;class &apos;int&apos;&gt;,))} my = RPCView() my.meth1(1, 2) # 输出: 类型检查成功 # received 1 and 2 my.meth2(2) # 输出: 类型检查失败 # File &quot;D:\VBoxShare\Work\Documents\PyProject\PyCookbook\test.py&quot;, line 57, in &lt;module&gt; # my.meth2(2) # File &quot;D:\VBoxShare\Work\Documents\PyProject\PyCookbook\test.py&quot;, line 25, in __xmlrpc # _check_types(checkable_args, in_) # File &quot;D:\VBoxShare\Work\Documents\PyProject\PyCookbook\test.py&quot;, line 20, in _check_types # raise TypeError(&quot;Arg #%d should be %s&quot; % (index, of_the_right_type)) # TypeError: Arg #0 should be &lt;class &apos;str&apos;&gt; 2.2 缓存缓存装饰器与参数检查十分相似, 不过他重点是关注那些内容状态不会影响输入的函数, 每组参数都可以链接到唯一的结果. 因此, 缓存装饰器可以将输出与计算法所需的参数放在一起, 并在后续的调用中直接返回他(这种行为成为 memoizing). import time import hashlib import pickle cache = {} def is_obsolete(entry, duration): return time.time() - entry[&quot;time&quot;] &gt; duration def compute_key(function, args, kw): &quot;&quot;&quot; 利用已排序的参数来构建 SHA 哈希键, 并将结果保存在一个全局字典中. 利用 pickle 来建立 hash , 这是冻结所有作为参数传入的对象状态的快捷方式, 以确保所有参数都满足于要求. &quot;&quot;&quot; key = pickle.dumps((function.__name__, args, kw)) return hashlib.sha1(key).hexdigest() def memoize(duration=10): def _memoize(function): def __memoize(*args, **kw): key = compute_key(function, args, kw) # 是否已经拥有它了? if (key in cache and not is_obsolete(cache[key], duration)): print(&quot;We got a winner.&quot;) return cache[key][&quot;value&quot;] # 计算 result = function(*args, **kw) # 保存结果 cache[key] = { &quot;value&quot;: result, &quot;time&quot;: time.time() } return result return __memoize return _memoize @memoize() def func_1(a, b): return a + b print(func_1(2, 2)) # 4 print(func_1(2, 2)) # print , 4 @memoize(1) def func_2(a, b): return a + b print(func_2(2, 2)) # 4 time.sleep(1) print(func_2(2, 2)) # 4 缓存值还可以与函数本身绑定, 以管理其作用域和生命周期, 代替集中化的字典. 但在任何情况下, 更高效的装饰器会使用基于高级缓存算法的专用缓存库. 2.3 代理代理装饰器使用全局代理来标记和注册函数. 例如, 一个根据当前用户来保护代码访问的安全层可以使用集中式检查器和相关的可调用对象要求的权限来实现. class User: def __init__(self, roles): self.roles = roles class Unauthorized(Exception): pass def protect(role): def _protect(function): def __protect(*args, **kw): user = globals().get(&quot;user&quot;) if user is None or role not in user.roles: raise Unauthorized(&quot;I won&apos;t tell you.&quot;) return function(*args, **kw) return __protect return _protect tarek = User((&quot;admin&quot;, &quot;user&quot;)) bill = User((&quot;user&quot;,)) class MySecrets: @protect(&quot;admin&quot;) def waffle_recipe(self): print(&quot;use tons of butter&quot;) these_are = MySecrets() user = tarek these_are.waffle_recipe() # use tons of butter user = bill these_are.waffle_recipe() # __main__.Unauthorized: I won&apos;t tell you. 以上模型常用于 Python Web 框架中(权限验证), 用于定义可发布类的安全性. 例如, Django 提供装饰器来保护函数访问的安全. 2.4 上下文提供者上下文装饰器确保函数可以运行在正确的上下文中, 或者在函数前后运行一些代码, 换句话说, 他设定并复位一个特定的执行环境. 例如, 当一个数据项需要在多个线程之间共享时, 就要用一个锁来保护她避免多次访问, 这个锁可以在装饰器中编写. from threading import RLock lock = RLock() def synchronized(function): def _synchronized(*args, **kw): lock.acquire() try: return function(*args, **kw) finally: lock.release() return _synchronized @synchronized def thread_safe(): # 确保锁定资源 pass 上下装饰器通常会被上下文管理器(with) 替代.]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>python 装饰器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python-上下文管理]]></title>
    <url>%2F2018%2F03%2F15%2FPython-%E4%B8%8A%E4%B8%8B%E6%96%87%E7%AE%A1%E7%90%86%2F</url>
    <content type="text"><![CDATA[使用上下文协议创建和管理上下文 使用 contextlib.contextmanager 创建和管理上下文 上下文的基本使用和嵌套方法 1. 编写实现上下文管理器1.1 作为一个类: 上下文管理协议任何实现了 上下文管理协议的对象都可以用作上下文管理器. 该协议包含两个特殊方法: __enter__(self) : 调用该方法, 任何返回值都会绑定到指定的 as 语句. __exit__(self, exc_type, exc_value, traceback) : 接受代码块中出现错误时填入的 3 个参数. 如果没有错误, 三个都为 None. 出现错误时, __exit__ 不应该重新引发这个错误, 因为这是调用者(caller) 的责任. 但他可以通过返回 True 来避免引发异常. 多数情况下, 这一方法只是执行一些清理工作, 无论代码块中发生什么, 他都不会返回任何内容. 代码示例: class ContextIllustration: def __enter__(self): print(&quot;entering context&quot;) def __exit__(self, exc_type, exc_value, traceback): print(&quot;leveling context&quot;) if exc_type is None: print(&quot;With no ERROR&quot;) else: print(&quot;With an ERROR (%s)&quot; % exc_value) with ContextIllustration(): print(&quot;inside&quot;) # 输出: # entering context # inside # leveling context # With no ERROR with ContextIllustration(): raise RuntimeError(&quot;Raised within &apos;with&apos;&quot;) # 输出: # entering context # leveling context # With an ERROR (Raised within &apos;with&apos;) # Traceback (most recent call last): # File &quot;D:\VBoxShare\Work\Documents\PyProject\PyCookbook\test.py&quot;, line 23, in &lt;module&gt; # raise RuntimeError(&quot;Raised within &apos;with&apos;&quot;) # RuntimeError: Raised within &apos;with&apos; 通过返回 True 来避免触发异常: class ContextIllustration: def __enter__(self): print(&quot;entering context&quot;) def __exit__(self, exc_type, exc_value, traceback): print(&quot;leveling context&quot;) if exc_type is None: print(&quot;With no ERROR&quot;) else: print(&quot;With an ERROR (%s)&quot; % exc_value) return True with ContextIllustration(): raise RuntimeError(&quot;Raised within &apos;with&apos;&quot;) # 输出: # entering context # leveling context # With an ERROR (Raised within &apos;with&apos;) 1.2 作为一个函数: contextlib 模块标准库 contextlib 提供了与上下文管理器一起使用的辅助函数: contextmanager, 他可以在一个函数里同时提供 __enter__ 和 __exit__ 两部分, 中间用 yield 分开(函数变成了生成器). from contextlib import contextmanager thelist = [1, 2, 3] @contextmanager def ListTransaction(thelist): workingcopy = list(thelist) yield workingcopy # 尽在没有出现错误时才会修改原始列表. thelist[:] = workingcopy with ListTransaction(thelist) as l: print(l) print(type(l)) 传递给 yield 的值, 用作 __enter__() 方法的返回值, 调用 __exit__() 方法时, 执行将在 yield 语句后恢复. 如果上下文中出现异常, 他将以异常形式出现在生成器函数中.如有需要可以捕获异常, 以上例子中, 异常被传递出生成器, 并其他地方进行处理. 如果出现任何异常, 被装饰函数需要再次抛出异常, 以便传递异常 from contextlib import contextmanager @contextmanager def context_illustration(): print(&quot;Entering context&quot;) try: yield except Exception as e: print(&quot;Leaving context&quot;) print(&quot;with an ERROR (%s)&quot; % e) # 抛出异常 raise else: print(&quot;Leaving context&quot;) print(&quot;with no error&quot;) with context_illustration(): print(&quot;Entering&quot;) # 输出: # Entering context # Entering # Leaving context # with no error with context_illustration(): raise RuntimeError(&quot;MyError&quot;) # 输出: # Entering context # Traceback (most recent call last): # File &quot;D:\VBoxShare\Work\Documents\PyProject\PyCookbook\test.py&quot;, line 18, in &lt;module&gt; # Leaving context # with an ERROR (MyError) # raise RuntimeError(&quot;MyError&quot;) # RuntimeError: MyError contextlib 还提供其他三个辅助函数: closing(element) : 返回一个上下文管理器, 在退出时, 调用该元素的 close() 方法, 对处理流的类很有用. supress(*exceptions) : 他会压制发生在 with 语句正文中的特定异常. redirect_stdout(new_target) : 将代码内任何代码的 sys.stdout 输出重定向到类文件(file-like)对象的另一个文件. redirect_stderr(new_target) : 将代码内任何代码的 sys.stderr 输出重定向到类文件(file-like)对象的另一个文件. 2. 使用方式 基本使用 with context_manager: # code here ... 上下文变量: 使用 as 语句保存为局部变量 __enter__() 的任何返回值都会绑定到指定的 as 子句. with context_manager as context: # code here ... 多个上下文管理器(嵌套) with A() as a, B() as b: # code here ... 等价于嵌套使用: with A() as a: with B() as b: # code here ...]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>python 标准库</tag>
        <tag>python 上下文</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PyStdLib-optparse]]></title>
    <url>%2F2018%2F03%2F15%2FPyStdLib-optparse%2F</url>
    <content type="text"><![CDATA[使用 optparse 解析命令行参数. 代码示例: import optparse p = optparse.OptionParser() p.add_option(&quot;-t&quot;, action=&quot;store_true&quot;, dest=&quot;tracing&quot;) p.add_option(&quot;-o&quot;, &quot;--outfile&quot;, action=&quot;store&quot;, type=&quot;string&quot;, dest=&quot;outfile&quot;) p.add_option(&quot;-d&quot;, &quot;--debuglevel&quot;, action=&quot;store&quot;, type=&quot;int&quot;, dest=&quot;debug&quot;) p.add_option(&quot;--speed&quot;, action=&quot;store&quot;, type=&quot;choice&quot;, dest=&quot;speed&quot;, choices=[&quot;slow&quot;, &quot;fast&quot;, &quot;ludicrous&quot;]) p.add_option(&quot;--coord&quot;, action=&quot;store&quot;, type=&quot;int&quot;, dest=&quot;coord&quot;, nargs=2) p.add_option(&quot;--novice&quot;, action=&quot;store_const&quot;, const=&quot;novice&quot;, dest=&quot;mode&quot;) p.add_option(&quot;--guru&quot;, action=&quot;store_const&quot;, const=&quot;guru&quot;, dest=&quot;mode&quot;) p.set_defaults(tracing=False, debug=0, speed=&quot;fast&quot;, coord=(0, 0), mode=&quot;novice&quot;) opt, args = p.parse_args() print &quot;tracing: &quot;, opt.tracing print &quot;outfile: &quot;, opt.outfile print &quot;debug : &quot;, opt.debug print &quot;speed : &quot;, opt.speed print &quot;coord : &quot;, opt.coord print &quot;mode : &quot;, opt.mode print &quot;args : &quot;, args]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>python 标准库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django-学习总结]]></title>
    <url>%2F2018%2F03%2F14%2FDjango-%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>web development</tag>
        <tag>django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ansible-学习总结]]></title>
    <url>%2F2018%2F03%2F14%2FAnsible-%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[大纲Ansible 原理配置篇 Ansible 简介 Ansible 任务的执行细节原理2.1 角色与依赖:2.2 工作机制2.3 工作原理 安装配置3.1 安装3.2 配置文件 Ansible 抽象实体4.1 inventory4.2 变量与fact4.3 模块4.4 task/play/role/playbook Ansible 命令5.1 ansible5.2 ansible-doc5.3 ansible-galaxy5.4 ansible-vault5.5 ansible-playbook ansible 优化加速6.1 SSH Multiplexing (ControlPersist)6.2 fact 缓存6.3 pipeline6.4 并发 Ansible Inventory篇 静态 Inventory1.1 inventory 行为参数1.2 ansible.cfg 设置 Inventory 行为参数默认值1.3 群组1.4 主机与群组变量 动态 inventory2.1 动态 inventory 脚本的接口2.2 在运行时添加主机或群组: add_host, group_by2.3 ec2.py &amp; ec2.ini 静态 Inventory 与 动态 Inventory 结合使用 Ansible task/play/role篇 task play role Ansible api 开发篇Ansible 番外篇之 ansible.cfg 配置 defaults 段 ssh_connection 段 paramiko 段 accelerate 段 (不推荐使用) Ansible 番外篇之模块 内置模块1.1 查看模块帮助1.2 查找第三方模块1.3 常用模块 自定义模块2.1 使用 script 自定义 模块2.2 使用 Python 自定义模块. Ansible 番外篇之变量与fact 变量1.1. 定义变量1.2. 显示变量: debug 模块1.3. register 注册变量: 基于 task 的执行结果, 设置变量的值.1.4. set_fact 定义新变量1.5. 内置变量1.6. 在命令行设置变量 fact2.1. setup 模块2.2. 模块返回 fact2.3. 本地 fact 变量优先级: 过滤器: 变量加工处理4.1. default : 设置默认值.4.2. 用于注册变量的过滤器4.3. 用于文件路径的过滤器4.4. 自定义过滤器 lookup: 从多种来源读取配置数据. file pipe env password template csvfile dnstxt redis-kv etcd Ansible 番外篇之流程控制假如 ansible 是一门开发语言. 循环 条件 函数 与 role]]></content>
      <categories>
        <category>DevOps</category>
      </categories>
      <tags>
        <tag>ansible</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[AngularJS-学习总结]]></title>
    <url>%2F2018%2F03%2F14%2FAngularJS-%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"></content>
      <categories>
        <category>JavaScript</category>
      </categories>
      <tags>
        <tag>AngularJS</tag>
        <tag>JS 前端框架</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python-标准库]]></title>
    <url>%2F2018%2F03%2F14%2Fpython-%E6%A0%87%E5%87%86%E5%BA%93%2F</url>
    <content type="text"><![CDATA[python 标准库]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>python 标准库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flask-学习总结]]></title>
    <url>%2F2018%2F03%2F14%2FFlask-%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[Flask 学习总结]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>web development</tag>
        <tag>flask</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sublime-Text-使用总结]]></title>
    <url>%2F2018%2F03%2F14%2FSublime-Text-%E4%BD%BF%E7%94%A8%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[初始化配置设置 Linux 换行符Perference-&gt;Setting-*. 设置对象是 default_line_ending, 这个参数有三 个可用选项： - system : system是根据当前系统情况设置, - windows : windows使用的CRLF, - unix : unix使用的是 LF 设置 tab 为 4 个空格.Preference -&gt; Settings-User // The number of spaces a tab is considered equal to &quot;tab_size&quot;: 4, // Set to true to insert spaces when tab is pressed &quot;translate_tabs_to_spaces&quot;: true, 自动保存Preference -&gt; Settings-User &quot;save_on_focus_lost&quot;: true 手动安装插件插件Preference -&gt; Browse Package 把下载的插件解压到打开的文件夹中, 解压后即可. 去除解压后文件夹中的 `-` 等字符, 重启 sublime 即可. install package controllSUBLIME TEXT 3 import urllib.request,os,hashlib; h = &apos;df21e130d211cfc94d9b0905775a7c0f&apos; + &apos;1e3d39e33b79698005270310898eea76&apos;; pf = &apos;Package Control.sublime-package&apos;; ipp = sublime.installed_packages_path(); urllib.request.install_opener( urllib.request.build_opener( urllib.request.ProxyHandler()) ); by = urllib.request.urlopen( &apos;http://packagecontrol.io/&apos; + pf.replace(&apos; &apos;, &apos;%20&apos;)).read(); dh = hashlib.sha256(by).hexdigest(); print(&apos;Error validating download (got %s instead of %s), please try manual install&apos; % (dh, h)) if dh != h else open(os.path.join( ipp, pf), &apos;wb&apos; ).write(by) SUBLIME TEXT 2 import urllib2,os,hashlib; h = &apos;df21e130d211cfc94d9b0905775a7c0f&apos; + &apos;1e3d39e33b79698005270310898eea76&apos;; pf = &apos;Package Control.sublime-package&apos;; ipp = sublime.installed_packages_path(); os.makedirs( ipp ) if not os.path.exists(ipp) else None; urllib2.install_opener( urllib2.build_opener( urllib2.ProxyHandler()) ); by = urllib2.urlopen( &apos;http://packagecontrol.io/&apos; + pf.replace(&apos; &apos;, &apos;%20&apos;)).read(); dh = hashlib.sha256(by).hexdigest(); open( os.path.join( ipp, pf), &apos;wb&apos; ).write(by) if dh == h else None; print(&apos;Error validating download (got %s instead of %s), please try manual install&apos; % (dh, h) if dh != h else &apos;Please restart Sublime Text to finish installation&apos;) 大块方框,Sublime &gt; Preferences &gt; Package Settings &gt; Anaconda &gt; Settings User {&quot;anaconda_linting&quot;: false} 中文输入法光标跟随Packages Control -&gt; install -&gt; IMESupport 常用插件SublimeLinter : 用于高亮提示用户编写的代码中存在的不规范和错误的写法, 支持 JavaScript、CSS、HTML、Java、PHP、Python、Ruby 等十多种开发语言. SideBarEnhancements : SideBarEnhancements是一款很实用的右键菜单增强插件；在安装该插件前, 在Sublime Text左侧FOLDERS栏中点击右键, 只有寥寥几个简单的功能 Javascript-API-Completions : 支持Javascript、JQuery、Twitter Bootstrap框架、HTML5标签属性提示的插件, 是少数支持sublime text 3的后缀提示的插件, HTML5标签提示sublime text3自带, 不过JQuery提示还是很有用处的, 也可设置要提示的语言. Git : Glue : 会在界面下方显示一个小窗口, 你可以在那里写Shell脚本. 这样一来, 你的编辑器就不仅仅局限于使用Git了 GitGutter &amp; Modific : 这些插件可以高亮相对于上次提交有所变动的行, 换句话说是实时的diff工具 GitGutter : 这是一个小巧有用的插件, 它会告诉你自上次git commit以来已经改变的行. 一个指示器显示在行号的旁边. PlainTasks : 杰出的待办事项表！所有的任务都保持在文件中, 所以可以很方便的把任务和项目绑定在一起. 可以创建项目, 贴标签, 设置日期. 有竞争力的用户界面和快捷键. Lua : Python : AllAutocomplete : 搜索全部打开的标签页 Emmet : HTML 快速补全 markdown : anaconda : Python IDE anaconda 不能与 jedi 同时存在, 会出现 左括号无法写入的情况. GBK support : 支持 GBK 编码 SublimeTmpl : 支持文件模板, git 地址. 默认模板支持及快捷键 ctrl+alt+h html ctrl+alt+j javascript ctrl+alt+c css ctrl+alt+p php ctrl+alt+r ruby ctrl+alt+shift+p python 添加自定义模板文件及快捷键 : 参考 https://segmentfault.com/a/1190000008674119 1. 新建并编辑自定义模板文件 Packages\User\SublimeTmpl\templates\hexomd.tmpl --- title: ${saved_filename} date: ${date} categories: tags: --- 摘要 &lt;!-- more --&gt; 正文 2. sublime 模板文件定义 : Commands-User [ { &quot;caption&quot;: &quot;Tmpl: Create Hexo Markdown&quot;, &quot;command&quot;: &quot;sublime_tmpl&quot;, &quot;args&quot;: {&quot;type&quot;: &quot;hexomd&quot;} } ] 3. 快捷键定义 : KeyBing-User [ { &quot;keys&quot;: [&quot;ctrl+alt+m&quot;], &quot;command&quot;: &quot;sublime_tmpl&quot;, &quot;args&quot;: {&quot;type&quot;: &quot;hexomd&quot;}, &quot;context&quot;: [{&quot;key&quot;: &quot;sublime_tmpl.hexomd&quot;}] } ] 4. 用户设置定义 : Settings-User { # 支持 ${saved_filename} 变量 &quot;enable_file_variables_on_save&quot;: true, } 设置快捷键. 在SublimeText里, 打开Preferences -&gt; Key Bindings - User, 我设置的快捷键：[ { &quot;keys&quot;: [&quot;ctrl+f9&quot;], &quot;command&quot;: &quot;build&quot; }, { &quot;keys&quot;: [&quot;f10&quot;], &quot;command&quot;: &quot;build&quot;, &quot;args&quot;: {&quot;variant&quot;: &quot;Run&quot;} }, { &quot;keys&quot;: [&quot;ctrl+shift+x&quot;], &quot;command&quot;: &quot;toggle_comment&quot;, &quot;args&quot;: { &quot;block&quot;: true } }, ]]]></content>
      <categories>
        <category>Tools</category>
      </categories>
      <tags>
        <tag>sublime</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[markdown 语法总结]]></title>
    <url>%2F2018%2F03%2F14%2Fmarkdown-%E8%AF%AD%E6%B3%95%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[段落标题 在行首插入 1 - 6 个 # ,对应 标题1 - 标题6 ## 会在标题下面插入一个横线, 作为分割. 区块引用 普通区块引用 嵌套区块引用 列表 有序列表 数字 + . 无序列表 * + - , 作用相同, 无差别 代码 1.代码块 `缩进4个空格, 或一个制表符, 代码块会一直持续到没有缩进的哪一行, 或者文件结尾` 2.小段代码 `反引号包含小段代码` 分割线 三个以上的星号,减号, 来建立一个分割线, 行内不能有其他东西 链接 普通链接 行内式 [Name](http://www.baidu.com &quot;Title&quot;) 引用式 定义: [id]: http://example.com &quot;Optionnal Title&quot; 引用: [Name][id] 图片 行内式 ![Alt text](/path/to/img.jpg &quot;Optional title&quot;) 引用式 定义: [id]: url/to/image &quot;Optional title attribute&quot; 引用: ![Alt text][id] Markdown 无法指定图片的宽高, 如果需要请使用 自动链接 针对URL : &lt;http://www.baidu.com&gt; 针对邮箱 : &lt;admin@example.com&gt; 强调 斜体 *WORD* 加粗 **WORD** 转义 : 在一些符号前面加上 反斜杠 来插入特殊符号 9.表格基本 | 表头 | 表头 | 表头 | 表头 | | -- | --- | --- | --- | | 内容 | 内容 | 内容 | 内容 | 表格对齐 | :-- | ---&gt; 左对齐 | ---: | ---&gt; 右对齐 | :---:| ---&gt; 居中对齐 颜色 `&lt;font color=&quot;blue&quot;&gt;GREEN&lt;/font&gt;` GREEN 删除线 ~~这里是删除内容~~ 效果 : 这里是删除内容 区块元素 段落 1. 类 Setext 格式 : 底线 形式 最高阶标题 : ==== 第二阶标题 : ---- 示例 : This is H1 ========== This is H2 ---------- 2. 类 atx 格式 : # 形式 在行首插入 1 - 6 个 # , 对应标题1 - 标题6 示例 : # This is H1 ## This is H2 ### This is H3 ** 可以选择性的 [闭合] 类 atx 样式的标题 : 在行尾加上 # , 而且行尾的 # 数量也无需同开头一样. 区块引用 Blockquotes 普通区块引用 : &gt; &gt; This is a blockquote with two paragraphs. Lorem ipsum dolor sit amet, &gt; consectetuer adipiscing elit. Aliquam hendrerit mi posuere lectus. &gt; Vestibulum enim wisi, viverra nec, fringilla in, laoreet vitae, risus. 嵌套区块引用 : 根据层次加上不同数量的 &gt; &gt; This is a blockquote with two paragraphs. Lorem ipsum dolor sit amet, &gt;&gt; consectetuer adipiscing elit. Aliquam hendrerit mi posuere lectus. &gt;&gt; Vestibulum enim wisi, viverra nec, fringilla in, laoreet vitae, risus. 引用的区域也可以使用其他的 Markdown 语法 : 包括标题,列表,代码区块等. 列表 有序列表 : 数字 + . 无序列表 : * + - , 作用相同, 无差别. ** 层次化表示, 需要缩进. ** 转义 : \ 如 : 1991\.12\.12 代码区块 缩进4个空格 , 或者 1 个制表符. 代码区块会一直持续到没有缩进的那一行, 或是文件结尾. ** 代码区块中, 一般 Markdown 语法不会被转换. 代码 小段代码 `CODE` `` CODE `` ** 多个反引号时, 可以在代码中使用 反引号本身. ** 代码区段的起始和结束端都可以放入一个空白，起始端后面一个，结束端前面一个，这样你就可以在区段的一开始就插入反引号 ** 在代码区段内，&amp; 和尖括号都会被自动地转成 HTML 实体，这使得插入 HTML 原始码变得很容易 分割线 : 三个以上的星号,减号,底线来建立一个分割线, 行内不能有其他东西. 型号或减号之间可以插入空格. *** * * * --- - - - - - 链接 链接文字 : [文字] 行内式 [Name](http://www.baidu.com &quot;Title&quot;) 相对路径 [logo](/static/logo.jpg &quot;logo&quot;) 参考式 : 先定义, 后引用 定义 : 在文档的任意处, 把这个标记的链接内容定义出来： [id]: http://example.com &quot;Optionnal Title&quot; 引用 : 不区分大小写 [Name][id] 示例 : [foo]: http://example.com/ &quot;Optional Title Here&quot; [foo]: http://example.com/ &apos;Optional Title Here&apos; [foo]: http://example.com/ (Optional Title Here) [id]: &lt;http://example.com/&gt; &quot;Optional Title Here&quot; [link text][a] [link text][A] 强调 : *WORDS* : &lt;em&gt; _WORDS_ : &lt;em&gt; **WORD** : &lt;strong&gt; __WORD__ : &lt;strong&gt; \* : 转义 \_ : 转义 ** 如果 * 和 _ 两边都有空白的话，它们就只会被当成普通的符号。 图片 行内式 ![Alt text](/path/to/img.jpg) ![Alt text](/path/to/img.jpg &quot;Optional title&quot;) 参考式 ![Alt text][id] [id]: url/to/image &quot;Optional title attribute&quot; ** Markdown 无法指定图片的 宽高, 如果需要可以使用 &lt;img&gt; 标签. 自动链接 : 针对 URL 和 Email 地址 &lt;http://example.com/&gt; &lt;address@example.com&gt; 反斜杠 : 转义 Markdown 支持一下这些符号前面加上 反斜杠 来帮助插入普通的符号. \ 反斜线 ` 反引号 * 星号 _ 底线 {} 花括号 [] 方括号 () 括弧 # 井字号 + 加号 - 减号 . 英文句点 ! 惊叹号 免费编辑器 Windows 平台 MarkdownPad MarkPad Linux 平台 ReText Mac 平台 Mou 在线编辑器 Markable.in Dillinger.io 浏览器插件 MaDe (Chrome) 高级应用 Sublime Text 2 + MarkdownEditing / 教程]]></content>
      <categories>
        <category>tools</category>
      </categories>
      <tags>
        <tag>markdown</tag>
        <tag>标记语言</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo+nexT 博客建设指南]]></title>
    <url>%2F2018%2F03%2F14%2FHexo-nexT-%E5%8D%9A%E5%AE%A2%E5%BB%BA%E8%AE%BE%E6%8C%87%E5%8D%97%2F</url>
    <content type="text"><![CDATA[Hexo 搭建 Github 博客开始使用安装 git$ yum install git -y 安装 NodeJS$ git clone https://github.com/creationix/nvm.git $ source nvm/nvm.sh $ nvm install stable 设置 Github注册 github创建 github page创建仓库, 仓库的名字要和你的账号对应, 格式为: USERNAME.github.io 安装 hexo-cli$ chmod 755 /root &amp;&amp; mkdir -m 755 -p /root/.npm/_logs $ npm install -g hexo-cli $ chmod 700 /root $ npm install -g hexo-cli 建站安装配置$ hexo init &lt;floder&gt; $ cd &lt;floder&gt; $ npm install 文件件目录结构 . ├── _config.yml # 网站的配置信息, 可以在此配置大部分参数 ├── package.json # 应用程序的信息. EJS, Stylus 和 Markdown renderer 已默认安装，您可以自由移除。 ├── scaffolds # 模板文件夹, 当新建文章时, Hexo 会根据 scaffold 来建立文件. | # Hexo 的模板是指在新建的 markdown 文件中默认填充的内容, 每次新建一篇文章时都会包含这个修改. ├── source # 存放用户资源. 除 _posts 文件夹外, 开头命名为 _ 的文件/文件夹和隐藏的文件都会被忽略. | | # Markdown 和 HTML 文件会被解析并放到 public 文件夹, 而其他文件会被拷贝过去. | ├── _drafts | └── _posts └── themes # 主题文件夹, Hexo 会根据主题来生成静态内容. # 安装 next theme, 可选. $ mkdir themes/next $ curl -s https://api.github.com/repos/iissnan/hexo-theme-next/releases/latest | grep tarball_url | cut -d &apos;&quot;&apos; -f 4 | wget -i - -O- | tar -zx -C themes/next --strip-components=1 # 修改默认 主题设置, 可选 $ vim _config.yml theme: next # 安装 hexo server $ npm install hexo-server --save # 启动 hexo server $ hexo server --ip=0.0.0.0 写文章与提交部署安装 hexo-deployer-git 部署方式 $ npm install hexo-deployer-git --save # 配置部署方式 $ vim _config.yml deploy: type: git repo: https://github.com/pyfdtic/pyfdtic.github.io.git branch: master 写文章 # hexo new &quot;TITLE&quot; $ vim source/_posts/TITLE.md --- title: first post date: 2018-03-14 17:08:36 categories: - test tags: - tag-a - tag-b - tag-c --- # content 写摘要: --- 这里是摘要 &lt;!-- more --&gt; 这是正文 生成静态文件并部署 $ hexo g -d 密钥认证提交 $ vim _config.yml 其中 repo 配置为 ssh 协议地址 # 语言配置 language: zh-Hans $ 在 github 上配置 ssh 密钥. 配置主题文档 $ vim themes/next/_config.yml # 主页预览显示 auto_excerpt: enable: true length: 250 # 选择不同的主体 #scheme: Muse scheme: Mist # 主页设置 menu: home: / || home about: /about/ || user tags: /tags/ || tags categories: /categories/ || th archives: /archives/ || archive tags/categories 页面 # tags $ hexo new page &quot;tags&quot; $ vim source/tags/index.md --- title: Tags date: 2018-03-14 18:09:40 type: &quot;tags&quot; comments: false --- $ vim themes/next/_config.yml menu: # ... tags: /tags/ || tags # ... # categories $ hexo new page &quot;categories&quot; $ vim source/categories/index.md --- title: Tags date: 2018-03-14 18:09:40 type: &quot;categories&quot; comments: false --- $ vim themes/next/_config.yml menu: # ... categories: /categories/ || th # ... about页面 $ hexo new page &quot;about&quot; 谷歌/百度统计 $ vim _config.yml google_analytics: UA-[numbers] baidu_analytics: your-analytics-id 站内搜索: $ npm install hexo-generator-searchdb --save $ vim _config.yml search: path: search.xml field: post format: html limit: 10000 $ vim themes/next/_config.yml local_search: enable: true # if auto, trigger search by changing input # if manual, trigger search by pressing enter key or search button trigger: auto # show top n results per article, show all results by setting to -1 top_n_per_article: -1 站点地图: $ npm install hexo-generator-sitemap $ vim themes/next/_config.yml menu: # ... sitemap: /sitemap.xml || sitemap $ 在 google Search console 提交 siteamp 地图. 配置资源文件夹 $ mkdir source/images # 在文章中引用 ![](/images/image.jpg) 主题配置参考站点配置 # ============================================================================= # NexT Theme configuration # ============================================================================= avatar: https://avatars1.githubusercontent.com/u/32269?v=3&amp;s=460 # Duoshuo duoshuo_shortname: notes-iissnan # Disqus disqus_shortname: # Social links social: GitHub: https://github.com/iissnan Twitter: https://twitter.com/iissnan Weibo: http://weibo.com/iissnan DouBan: http://douban.com/people/iissnan ZhiHu: http://www.zhihu.com/people/iissnan # Creative Commons 4.0 International License. # http://creativecommons.org/ # Available: by | by-nc | by-nc-nd | by-nc-sa | by-nd | by-sa | zero creative_commons: by-nc-sa # Google Webmaster tools verification setting # See: https://www.google.com/webmasters/ google_site_verification: VvyjvVXcJQa0QklHipu6pwm2PJGnnchIqX7s5JbbT_0 # Google Analytics # Google分析ID google_analytics: # 百度分析ID baidu_analytics: 50c15455e37f70aea674ff4a663eef27 # Specify the date when the site was setup since: 2011 # ============================================================================= # End NexT Theme configuration # ============================================================================= 主题配置文件 menu: home: / categories: /categories archives: /archives tags: /tags #about: /about # Place your favicon.ico to /source directory. favicon: /favicon.ico # Set default keywords (Use a comma to separate) keywords: &quot;Hexo,next&quot; # Set rss to false to disable feed link. # Leave rss as empty to use site&apos;s feed link. # Set rss to specific value if you have burned your feed already. rss: # Icon fonts # Place your font into next/source/fonts, specify directory-name and font-name here # Avialable: default | linecons | fifty-shades | feather #icon_font: default #icon_font: fifty-shades #icon_font: feather icon_font: linecons # Code Highlight theme # Available value: normal | night | night eighties | night blue | night bright # https://github.com/chriskempson/tomorrow-theme highlight_theme: normal # MathJax Support mathjax: # Schemes scheme: Mist # Automatically scroll page to section which is under &lt;!-- more --&gt; mark. scroll_to_more: true # Automatically add list number to toc. toc_list_number: true ## DO NOT EDIT THE FOLLOWING SETTINGS ## UNLESS YOU KNOW WHAT YOU ARE DOING # Use velocity to animate everything. use_motion: true # Fancybox fancybox: true # Static files vendors: vendors css: css images: images # Theme version version: 0.4.2 删除文章$ hexo clean $ hexo g -d 配置 _config.yml网站 参数 描述 title 网站标题 subtitle 网站副标题 description 网站描述, 网站 SEO author 作者. 显示文章的作者 language 网站使用的语言 timezone 网站时区. Hexo 默认使用浏览器时区 网址 参数 描述 默认值 url 网址 - root 网站根目录 - permalink 文章的永久链接 :year/:mouth/:day/:title permalink_defaults 永久链接中各部分的默认值 网站存放在子目录, 如果网站存放在子目录中, 如 http://yoursite.com/blog , 则需要把 url 设为 ‘http://yoursite.com/blog&#39;, 并把 root 设为 /blog/; 目录 参数 描述 默认值 source_dir 资源文件夹, 用于存放内容 source public_dir 公共文件夹, 用于存放生成的站点文件 public tag_dir 标签文件夹 tags archive_dir 归档文件夹 archives category_dir 分类文件夹 categories code_dir include code 文件夹 downloads/code i18n_dir 国际化(i18n)文件夹 :lang skip_render 跳过指定文件的渲染, 可使用 glob 表达式来匹配路径 如果刚接触 hexo , 则没必要设置以上各值. 文章 参数 描述 默认值 new_post_name 新文章的文件名称 :title.md default_layout 预设布局 post auto_spacing 在中文和英文之间加入空格 false titlecase 把标题转换为 title case false external_link 在新标签中打开链接 true filename_case 把文件名称转换为 (1) 小写或 (2) 大写 0 render_drafts 显示草稿 false post_asset_folder 启动 Asset 文件夹 false relative_link 把链接改为与根目录的相对位址 false future 显示未来的文章 true highlight 代码块的设置 默认情况下，Hexo生成的超链接都是绝对地址. 建议使用绝对地址. 分类 &amp; 标签 参数 描述 默认值 default_category 默认分类 uncategorized category_map 分类别名 tag_map 标签别名 日期/时间格式Hexo 使用 Moment.js 来解析和显示时间. 参数 描述 默认值 date_format 日期格式 YYYY-MM-DD time_format 时间格式 H:mm:ss 分页 参数 描述 默认值 per_page 每页显示的文章数量(0= 关闭分页功能) 10 pagination_dir 分页目录 page 扩展 参数 描述 默认值 theme 当前主题名称, 值为 false 时禁用主题 deploy 部署部分的设置 指令$ hexo SUB_CMD PARAM init [floder] : 新建一个网站. floder 为空时, 为当前文件夹. new [layout] &lt;title&gt; : 新建一篇文章, 如果没有设置`layout`的话, 默认使用 `_config.yml` 中的 `default_layout` 参数代替. 如果标签包含空格, 请使用引号. generate : 生成静态文件. 可以简写为 &quot;hexo g&quot; --d, --deploy : 文件生成后, 立即部署网站 -w, --watch : 监视文件变动. publish [layout] &lt;filename&gt; : 发表草稿 server : 启动服务器, 默认为 &quot;http://localhost:4000/&quot; -p, --port : 指定端口 -s, --static : 只使用静态文件, -l, --log : 启动日志记录, 使用覆盖记录格式. deploy : 部署网站. 可以简写为 &quot;hexo d&quot; -g, --generate : 部署之前预先生成静态文件. render &lt;file1&gt; [file2] ... : 渲染文件. -o, --output : 设置输出路径. migrate &lt;type&gt; : 从其他博客系统迁移. clean : 清除缓存文件(db.json) 和 已生成的静态文件(public). 在某些情况下(尤其是更换主题后), 如果发现对网站的更改无论如何不生效, 可能需要运行该命令. list &lt;type&gt; : 列出网站资料 version : 显示 hexo 版本 --safe : 在安全模式下运行, 不会载入插件和脚本. 当安装新插件遇到问题时, 可以尝试以安全模式重新执行. --debug : 在终端中显示调试信息, 并记录到 debug.log. --silent : 隐藏终端信息 --config custom.yml : 自定义配置文件路径, 执行后将不再使用 _config.yml --draft : 显示 source/_drafts 文件夹中的草稿万丈. --cwd /path/to/cwd : 自定义当前工作目录. 基本操作写作新建文章新建一篇文章: $ hexo new [layout] &lt;title&gt; 可以在 layout 中指定文章的布局(layout), 默认为 post, 可以通过修改 _config.yml 中的 default_layout 参数来指定默认布局. 文章布局Hexo 有三种默认布局: post, page, draft. 他们分别对应不同的路径, 用户自定义的其他布局和post相同, 都将存储在source/_posts 文件夹. 布局 路径 post source/_posts page source draft source/_drafts 如果你不希望你的文章被处理, 可以将 Front-Matter 中的 layout: 设置为 false 草稿草稿(draft) 默认不会显示在页面中, 可以在执行时加上 --draft 参数, 或是把 render_drafts 参数设置为 true 来预览草稿. draft 为草稿布局, 保存与 source/_drafts 目录, 可以通过 publish 命令将草稿移动到source/_posts 文件夹, publish 与 new 使用方式十分类似. $ hexo publish [layout] &lt;title&gt; 文件名称Hexo 默认以标题作为文件名称, 可以编辑 new_post_name 参数来改变默认的文件名称.如 :year-:month-:day-:title.md. 变量 描述 :title 标题(小写, 空格将被替换为短杠) :year 建立年份, 如 2016 :mouth 建立月份, 前导有零, 如 04 :i_mouth 建立月份, 前导无零, 如 4 :day 建立的日期, 前导有零, 如 07 :i_day 建立的日期, 前导无零, 如 7 模板(scaffold)使用方法在新建文章时, Hexo 会根据 scaffolds 文件夹内向对应的文件来建立新文件. 如: # hexo 在 scaffolds 文件夹中寻找 photo.md , # 并根据其内容建立文章. $ hexo new photo &quot;My Gallery&quot; 模板中的可用变量 变量 描述 layout 布局 title 标题 date 文件建立日期 Front-matter使用格式及预定义参数Front-matter 是文件最上方以 --- 分割的区域, 用于指定个别文件的变量. title: Hello World date: 2013/7/12 20:46:25 --- 预定义参数列表如下: 参数 描述 默认值 layout 布局 title 标题 date 建立日期 文件建立日期 updated 更新日期 文件跟新日期 comments 开启文章评论功能 true tags 标签(不适用于分页) categories 分类(不适用于分页) permalink 覆盖文章网址 分类和标签只有文章支持分类和标签, 可以在 Front-matter 中设置. 分类: 分类有顺序性和层次性, 如 Foo,Bar 不等于 Bar, Foo 标签: 标签没有顺序和层次 示例: categories: - Diary tags: - PS3 - Games WordPress 支持对一篇文章设置多个分类, 而且这些分类可以是同级的, 也可以是父子分类. 但 Hexo 不支持指定多个同级分类. 如下的分类, Life 将成为 Diary 的子分类. categories - Diary - Life JSON Front-matter可以使用 JSON 来编写 Front-matter, 只需将 --- 替换为 ;;; 即可: &quot;title&quot;: &quot;Hello World&quot; &quot;date&quot;: &quot;2013/7/12 20:46:25&quot; ;;; 标签插件(Tag Plugins)标签插件和 Front-matter 中的标签不同, 标签插件是用于在文章中快速插入特定内容的插件 引用块在文章中插入引言, 可包含作者, 来源 和 标题. 格式 {% blockquote [author[, source]] [link] [source_link_title] %} CONTENT {% endblockquote %} 示例 # 引用网络上的文章 {% blockquote Seth Godin http://sethgodin.typepad.com/seths_blog/2009/07/welcome-to-island-marketing.html Welcome to Island Marketing %} Every interaction is both precious and an opportunity to delight. {% endblockquote %} # 引用书上的句子 {% blockquote David Levithan, Wide Awake %} Do not just seek happiness for yourself. Seek happiness for all. Through kindness. Through mercy. {% endblockquote %} 代码块在文章中插入代码. 格式 {% codeblock [title] [lang:language] [url] [link text] %} CODE_SNIPPET {% endcodeblock %} 示例 # 附加说明和网址 {% codeblock _.compact http://underscorejs.org/#compact Underscore.js %} _.compact([0, 1, false, 2, '', 3]); => [1, 2, 3] {% endcodeblock %} # 指定语言 {% codeblock lang:objc %} [rectangle setX: 10 y: 10 width: 20 height: 20]; {% endcodeblock %} 反引号代码块使用三个反引号类包裹的代码块: ``` [language] [title] [url] [link text] code snippet ``` Pull Quote{% pullquote [class] %} content {% endpullquote %} jsFiddle{% jsfiddle shorttag [tabs] [skin] [width] [height] %} Gist{% gist gist_id [filename] %} 3iframe{% iframe url [width] [height] %} Image{% img [class names] /path/to/image [width] [height] [title text [alt text]] %} Link在文章中插入链接, 并自动给外部链接添加 target=&quot;_blank&quot; {% link text url [external] [title] %} Include Code插入source文件夹内的代码文件. {% include_code [title] [lang:language] path/to/file %} Youtube插入 Youtube 视频. {% youtube video_id %} Vimeo插入 vimeo 视频 {% vimeo video_id %} 引用文章引用其他文章的链接. {% post_path slug %} {% post_link slug [title] %} 引用资源引用文章的资源 {% asset_path slug %} {% asset_img slug [title] %} {% asset_link slug [title] %} Raw如果希望在文章中插入 Swig 标签, 可以尝试使用 Raw 标签, 以免发生解析异常. {% raw %} content {% endraw %} 资源文件夹资源Asset代表 source 文件夹中除了文章以外的所有文件, 如图片,CSS,JS文件等. 如在 source/images 文件夹中的图片, 可以使用类似于![](/images/NAME.jpg) 方法访问他们. 文章资源文件夹更加组织化的管理资源, 可以通过修改 config.yml 文件中的 post_asset_folder 选项设为 true 来打开. post_asset_folder: true 打开资源文件管理功能之后, Hexo 将会在每一次通过 hexo new [layout] &lt;title&gt; 命令创建新文章时自动创建一个文件夹. 这个资源文件夹间会有与这个 markdown 文件一样的名字. 将所有与该文章有关的资源放在这个关联文件夹中之后, 可以通过相对路径来引用这些资源, 这样就得到了一个更简单而且方便的得多的工作流. 相对路径引用的标签插件通过常规的 markdown 语法和相对路径来引用图片和其他资源可能会导致他们在存档页和主页上显示不正常. 可以使用如下方式引用资源, 解决这个问题: {% asset_path slug %} {% asset_img slug [title] %} {% asset_link slug [title] %} 如, 当打开文章资源文件夹功能后, 资源文件夹中有一个 example.jpg 图片, 正确的引用该图片的方式是使用如下的标签插件, 而不是 markdown, 该图片将会同时出现在文章和主页及归档页中: {% asset_img example.jpg This is an example image %} 数据文件有时, 可能需要在主题中使用某些资料, 而这些资料并不在文章内, 并且是需要重复使用的, 那么可以使用 Hexo 3.0 新增的 数据文件功能, 此功能会载入 source/_data 内的 YAML 或 JSON 文件, 以方便在网站中复用这些文件. # source/_date/menu.yml Home: / Gallery: /gallery/ Archives: /archives/ # 在模板中引用这些资料: &lt;% for (var link in site.data.menu) { %&gt; &lt;a href=&quot;&lt;%= site.data.menu[link] %&gt;&quot;&gt; &lt;%= link %&gt; &lt;/a&gt; &lt;% } %&gt; # 渲染结果 &lt;a href=&quot;/&quot;&gt; Home &lt;/a&gt; &lt;a href=&quot;/gallery/&quot;&gt; Gallery &lt;/a&gt; &lt;a href=&quot;/archives&quot;&gt; Archives &lt;/a&gt; 服务器hexo-serverHexo 3.0 把服务器独立成了个别模块, 必须先安装 hexo-server 才能使用. # 安装 $ npm install hexo-server --save # 启动服务器, 默认 http://localhost:4000 $ hexo server [-p PORT] [-i IP_ADDRESS] [-s] -s : 静态模式, 服务器只处理 public 文件夹内的文件, 而不会处理文件变动, 在执行时, 应该先自行执行 hexo generate, 常用于生产环境. -i IP_ADDRESS : 指定IP地址, 默认为 0.0.0.0 . -p PORT : 指定监听端口. PowPow 是 Mac 系统上的零配置 Rack 服务器, 他也可以作为一个简单易用的静态文件服务器来使用. # 安装 $ curl get.pow.cx | sh # 设置: 在 ~/.pow 文件夹建立链接(symlink) $ cd ~/.pow $ ln -s /path/to/myapp # 网站将在 http://myapp.dev 下运行, 网址根据链接名称而定. 生成器生成文件:$ hexo generate [--watch] --watch : 监视文件变动并立即重新生成静态文件. 在生成时对比文件的 SHA1 , 只有变动的文件才会写入. 完成后部署如下两个命令功能相同, 让 Hexo 在生成完毕后自动部署网站. $ hexo generate --deploy $ hexo g -d # 上述命令的简写 $ hexo deploy --generate $ hexo d -g # 上述命令的简写 部署部署步骤 $ vim _config.yml deploy: type: git $ hexo deploy git 部署# 安装 hexo-deployer-git $ npm install hexo-deployer-git --save # 修改 _config.yml deploy: type: git repo: &lt;REPOSITORY URL&gt; branch: &lt;GIT BRANCH&gt; message: &lt;自定义提交信息&gt; # 默认为 Site updated: {{ now('YYYY-MM-DD HH:mm:ss') }} # 部署 $ hexo deploy Heroku 部署# 安装 hexo-deployer-heroku $ npm install hexo-deployer-heroku --save # 修改 _config.yml deploy: type: heroku repo: &lt;REPOSITORY URL&gt; message: &lt;自定义提交信息&gt; # 默认为 Site updated: {{ now('YYYY-MM-DD HH:mm:ss') }} # 部署 $ hexo deploy 自定义永久链接(Permalink)https://hexo.io/zh-cn/docs/permalinks.html 主题修改主题 在 themes 文件夹内, 创建一个任意名称的文件夹, 修改 _config.yml 内的 theme 设定, 即可切换主体题 主题目录结构. ├── _config.yml ├── languages ├── layout ├── scripts └── source _config.yml主体的配置文件, 修改时会自动更新, 无需重启服务器. languages语言文件夹, 参见国际化 layout布局文件夹, 用于存放主题的模板文件, 决定网站内容的呈现方式. Hexo 内建 Swig 模板引擎, 可以另外安装插件来获得 EJS, Haml, Jade 支持, Hexo 根据模板文件的扩展名来决定所使用的模板引擎. scripts脚本文件夹, 在启动时, Hexo 会自定载入此文件夹内的 JavaScript 文件. source资源文件夹, 除了模板以外的 Asset, 如 CSS , JavaScript 文件等, 都应该放在这个文件夹中. 文件或文件夹前缀为 _ (下划线) 或 隐藏的文件会被忽略. 如果文件可以被渲染的话, 会经过解析然后存储到 public 文件夹, 否则会直接拷贝到 public 文件夹. 模板https://hexo.io/zh-cn/docs/templates.html 变量https://hexo.io/zh-cn/docs/variables.html 辅助函数https://hexo.io/zh-cn/docs/helpers.html#toc 国际化(i18n)https://hexo.io/zh-cn/docs/internationalization.html 插件https://hexo.io/zh-cn/docs/plugins.html nexThttp://theme-next.iissnan.com/getting-started.html#third-party-services 参考文档hexo-theme-nexthexo-wiki hexo 文档 - 中文hexo 文档 - 英文nexT 主题配置文档]]></content>
      <categories>
        <category>Tools</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>nexT</tag>
      </tags>
  </entry>
</search>
